{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrective RAG Demo\n",
    "\n",
    "This demo shows how you can use LlamaCloud and [Tavily AI](https://tavily.com/) to build a [Corrective RAG](https://arxiv.org/abs/2401.15884) workflow. The workflow uses the indexed documents on Llamacloud as a primary tool, but falls back to web search using Tavily AI if the information presented in the query cannot be found on LlamaCloud.\n",
    "\n",
    "\n",
    "A brief understanding of the paper:  \n",
    "Corrective Retrieval Augmented Generation (CRAG) is a method designed to enhance the robustness of language model generation by evaluating and augmenting the relevance of retrieved documents through a an evaluator and large-scale web searches, ensuring more accurate and reliable information is used in generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Follow [these instructions](https://docs.cloud.llamaindex.ai/llamacloud/getting_started/quick_start) on how to set up your index. For this example, we will upload a paper about Llama2 onto LlamaCloud. On the configure data source step, download [this PDF paper](https://arxiv.org/pdf/2307.09288) and upload it into your index.\n",
    "\n",
    "After deploying your index, follow [these instructions](https://docs.cloud.llamaindex.ai/llamacloud/getting_started/api_key) on getting an API key. Once you are done with this, configure `nest_asyncio` and your enviornment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-indices-managed-llama-cloud llama-index-tools-tavily-research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<Your OpenAI API Key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Workflow\n",
    "\n",
    "Corrective RAG consists of the following steps:\n",
    "1. Ingestion of data — Loads the data into an index and setting up Tavily AI. The ingestion step will be run by itself, taking in a start event and returning a stop event.\n",
    "2. Retrieval - Retrives the most relevant nodes based on the query.\n",
    "3. Relevance evaluation - Uses an LLM to determine whether the retrieved nodes are relevant to the query given the content of the nodes.\n",
    "4. Relevance extraction - Extracts the nodes which the LLM determined to be relevant.\n",
    "5. Query transformation and Tavily search - If a node is irrelevant, then uses an LLM to transform the query to tailor towards a web search. Uses Tavily to search the web for a relevant answer based on the query.\n",
    "6. Response generation - Builds a summary index given the text from the relevant nodes and the Tavily search and uses this index to get a result given the original query.\n",
    "\n",
    "The following events are needed:\n",
    "1. `RetrieveEvent` - Event containing information about the retrieved nodes.\n",
    "2. `RelevanceEvalEvent` - Event containing a list of the results of the relevance evaluation.\n",
    "3. `TextExtractEvent` - Event containing the concatenated string of relevant text from relevant nodes.\n",
    "4. `QueryEvent` - Event containing both the relevant text and search text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from llama_index.core.schema import  NodeWithScore\n",
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    ")\n",
    "\n",
    "class RetrieveEvent(Event):\n",
    "    \"\"\"Retrieve event (gets retrieved nodes).\"\"\"\n",
    "\n",
    "    retrieved_nodes: List[NodeWithScore]\n",
    "\n",
    "\n",
    "class RelevanceEvalEvent(Event):\n",
    "    \"\"\"Relevance evaluation event (gets results of relevance evaluation).\"\"\"\n",
    "\n",
    "    relevant_results: List[str]\n",
    "\n",
    "\n",
    "class TextExtractEvent(Event):\n",
    "    \"\"\"Text extract event. Extracts relevant text and concatenates.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    \"\"\"Query event. Queries given relevant text and search text.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "    search_text: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    "    Workflow,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "DEFAULT_RELEVANCY_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
    "\n",
    "    Retrieved Document:\n",
    "    -------------------\n",
    "    {context_str}\n",
    "\n",
    "    User Question:\n",
    "    --------------\n",
    "    {query_str}\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the document contains keywords or topics related to the user's question.\n",
    "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
    "\n",
    "    Decision:\n",
    "    - Assign a binary score to indicate the document's relevance.\n",
    "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
    "\n",
    "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
    ")\n",
    "\n",
    "DEFAULT_TRANSFORM_QUERY_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
    "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
    "    Original Query:\n",
    "    \\n ------- \\n\n",
    "    {query_str}\n",
    "    \\n ------- \\n\n",
    "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
    "    Respond with the optimized query only:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class CorrectiveRAGWorkflow(Workflow):\n",
    "    @step(pass_context=True)\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> Optional[StopEvent]:\n",
    "        \"\"\"Ingest step (for ingesting docs and initializing index).\"\"\"\n",
    "        tavily_ai_apikey: Optional[str] = ev.get(\"tavily_ai_apikey\")\n",
    "        index: Optional[LlamaCloudIndex] = ev.get(\"index\")\n",
    "\n",
    "        if any(i is None for i in [tavily_ai_apikey, index]):\n",
    "            return None\n",
    "\n",
    "        llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "        ctx.data[\"llm\"] = llm\n",
    "        ctx.data[\"index\"] = index\n",
    "        ctx.data[\"tavily_tool\"] = TavilyToolSpec(api_key=tavily_ai_apikey)\n",
    "\n",
    "        return StopEvent()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def retrieve(self, ctx: Context, ev: StartEvent) -> Optional[RetrieveEvent]:\n",
    "        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
    "        query_str = ev.get(\"query_str\")\n",
    "        retriever_kwargs = ev.get(\"retriever_kwargs\", {})\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        retriever: BaseRetriever = ctx.data[\"index\"].as_retriever(**retriever_kwargs)\n",
    "        result = retriever.retrieve(query_str)\n",
    "        ctx.data[\"retrieved_nodes\"] = result\n",
    "        ctx.data[\"query_str\"] = query_str\n",
    "        return RetrieveEvent(retrieved_nodes=result)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def eval_relevance(\n",
    "        self, ctx: Context, ev: RetrieveEvent\n",
    "    ) -> RelevanceEvalEvent:\n",
    "        \"\"\"Evaluate relevancy of retrieved documents with the query.\"\"\"\n",
    "        retrieved_nodes = ev.retrieved_nodes\n",
    "        query_str = ctx.data[\"query_str\"]\n",
    "        llm: LLM = ctx.data[\"llm\"]\n",
    "\n",
    "        relevancy_results = []\n",
    "        for node in retrieved_nodes:\n",
    "            prompt = DEFAULT_RELEVANCY_PROMPT_TEMPLATE.format(context_str=node.text, query_str=query_str)\n",
    "            relevancy = llm.complete(prompt)\n",
    "            relevancy_results.append(relevancy.text.lower().strip())\n",
    "\n",
    "        ctx.data[\"relevancy_results\"] = relevancy_results\n",
    "        return RelevanceEvalEvent(relevant_results=relevancy_results)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def extract_relevant_texts(\n",
    "        self, ctx: Context, ev: RelevanceEvalEvent\n",
    "    ) -> TextExtractEvent:\n",
    "        \"\"\"Extract relevant texts from retrieved documents.\"\"\"\n",
    "        retrieved_nodes = ctx.data[\"retrieved_nodes\"]\n",
    "        relevancy_results = ev.relevant_results\n",
    "\n",
    "        relevant_texts = [\n",
    "            retrieved_nodes[i].text\n",
    "            for i, result in enumerate(relevancy_results)\n",
    "            if result == \"yes\"\n",
    "        ]\n",
    "\n",
    "        result = \"\\n\".join(relevant_texts)\n",
    "        return TextExtractEvent(relevant_text=result)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def transform_query_pipeline(\n",
    "        self, ctx: Context, ev: TextExtractEvent\n",
    "    ) -> QueryEvent:\n",
    "        \"\"\"Search the transformed query with Tavily API.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        relevancy_results = ctx.data[\"relevancy_results\"]\n",
    "        query_str = ctx.data[\"query_str\"]\n",
    "        llm: LLM = ctx.data[\"llm\"]\n",
    "\n",
    "        # If any document is found irrelevant, transform the query string for better search results.\n",
    "        if \"no\" in relevancy_results:\n",
    "            prompt = DEFAULT_TRANSFORM_QUERY_TEMPLATE.format(query_str=query_str)\n",
    "            result = llm.complete(prompt)\n",
    "            transformed_query_str = result.text\n",
    "\n",
    "            # Conduct a search with the transformed query string and collect the results.\n",
    "            search_results = ctx.data[\"tavily_tool\"].search(\n",
    "                transformed_query_str, max_results=5\n",
    "            )\n",
    "            search_text = \"\\n\".join([result.text for result in search_results])\n",
    "        else:\n",
    "            search_text = \"\"\n",
    "\n",
    "        return QueryEvent(relevant_text=relevant_text, search_text=search_text)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        \"\"\"Get result with relevant text.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        search_text = ev.search_text\n",
    "        query_str = ctx.data[\"query_str\"]\n",
    "\n",
    "        documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
    "        index = SummaryIndex.from_documents(documents)\n",
    "        query_engine = index.as_query_engine()\n",
    "        result = query_engine.query(query_str)\n",
    "        return StopEvent(result=result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LlamaCloudIndex\n",
    "\n",
    "Create a `LlamaCloudIndex` which retrieves information from the index you have on LlamaCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "    name=\"<Your index name>\",\n",
    "    project_name=\"<Your project name>\",\n",
    "    api_key=\"llx-...\",\n",
    "    organization_id=\"<Your organization ID>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://docs.cloud.llamaindex.ai/organizations) for a tutorial on how to use organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the workflow ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step ingest\n",
      "Step ingest produced event StopEvent\n",
      "Running step retrieve\n",
      "Step retrieve produced no event\n"
     ]
    }
   ],
   "source": [
    "workflow = CorrectiveRAGWorkflow(verbose=True, timeout=60)\n",
    "await workflow.run(index=index, tavily_ai_apikey=\"<Your Tavily AI API Key>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step ingest\n",
      "Step ingest produced no event\n",
      "Running step retrieve\n",
      "Step retrieve produced event RetrieveEvent\n",
      "Running step eval_relevance\n",
      "Step eval_relevance produced event RelevanceEvalEvent\n",
      "Running step extract_relevant_texts\n",
      "Step extract_relevant_texts produced event TextExtractEvent\n",
      "Running step transform_query_pipeline\n",
      "Step transform_query_pipeline produced event QueryEvent\n",
      "Running step query_result\n",
      "Step query_result produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Llama 2 was pretrained on Meta's Research Super Cluster and production clusters using NVIDIA A100 GPUs. The pretraining process involved utilizing custom training libraries and third-party cloud compute for fine-tuning, annotation, and evaluation. The pretraining data consisted of 2 trillion tokens from publicly available sources, with a cutoff date of September 2022. The carbon footprint of the pretraining process amounted to 539 tCO2eq, which was fully offset by Meta's sustainability program."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "result = await workflow.run(query_str=\"How was Llama2 pretrained?\") # this was in the given paper\n",
    "display(Markdown(str(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step ingest\n",
      "Step ingest produced no event\n",
      "Running step retrieve\n",
      "Step retrieve produced event RetrieveEvent\n",
      "Running step eval_relevance\n",
      "Step eval_relevance produced event RelevanceEvalEvent\n",
      "Running step extract_relevant_texts\n",
      "Step extract_relevant_texts produced event TextExtractEvent\n",
      "Running step transform_query_pipeline\n",
      "Step transform_query_pipeline produced event QueryEvent\n",
      "Running step query_result\n",
      "Step query_result produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The airline flight UA 1 flies from San Francisco, United States to Singapore."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await workflow.run(query_str=\"Where does the airline flight UA 1 fly?\") # this info is not in the paper\n",
    "display(Markdown(str(result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
