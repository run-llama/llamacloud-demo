{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b2c37d-3b5a-47aa-95b9-d28e0bc83f77",
   "metadata": {},
   "source": [
    "# Auto-Retrieval with LlamaCloud\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/auto_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "**Auto-retrieval** is an advanced RAG technique that uses an LLM to dynamically infer the metadata filter parameters along with the semantic query before initiating vector database retrieval, in comparison to naive RAG which directly sends the user query to the vector db retrieval interface (e.g. dense vector search). It can both be thought of as a form of query expansion/rewriting if you come from the retrieval world, as well as a specific form of function calling.\n",
    "\n",
    "![](auto_retrieval_img.png)\n",
    "\n",
    "LlamaCloud helps you easily define chunk and document-level retrieval interfaces on top of any documents. In this guide we show you how to build an auto-retrieval pipeline on top of LlamaCloud retrievers over a research document corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f707a-c7b5-473f-b4a6-881e2245e82d",
   "metadata": {},
   "source": [
    "## Setup LlamaCloud \n",
    "\n",
    "Install core packages and download relevant files. Upload these documents to LlamaCloud, and then define a chunk and document-level retriever interface over these documents.\n",
    "\n",
    "For more information on chunk-level and document-level retrieval, check out our interface [here](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa458bc-bc8d-46fe-9a57-021dd8d9e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-core\n",
    "!pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79821400-caaf-42f1-99d8-74c184c19e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: uncomment more papers if you want to do research over a larger subset of docs\n",
    "\n",
    "urls = [\n",
    "    # \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    # \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    # \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    # \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    # \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    # \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    # \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    # \"https://openreview.net/pdf?id=TpD2aG1h0D\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    # \"metagpt.pdf\",\n",
    "    # \"longlora.pdf\",\n",
    "    # \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    # \"zipformer.pdf\",\n",
    "    # \"values.pdf\",\n",
    "    # \"finetune_fair_diffusion.pdf\",\n",
    "    # \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    # \"vr_mcl.pdf\",\n",
    "]\n",
    "\n",
    "data_dir = \"iclr_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80137d15-f22b-47eb-adce-ac295ced7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"{data_dir}\"\n",
    "for url, paper in zip(urls, papers):\n",
    "    !wget \"{url}\" -O \"{data_dir}/{paper}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315d639-52a0-4888-8779-653083ec3768",
   "metadata": {},
   "source": [
    "#### Load Documents into LlamaCloud\n",
    "\n",
    "Create a new index in LlamaCloud and drag and drop these downloaded PDFs into the data source.\n",
    "\n",
    "For best results, in the Transformation Configuration click on the \"Manual\" tab, and set page-level segmentation configuration and \"None\" for additional chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579938a-54b5-4eb2-b97e-22d96382eded",
   "metadata": {},
   "source": [
    "#### Setup LlamaCloud Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4208977f-896f-4993-a3a2-1be83c92baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "import os\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=\"research_papers_page\",\n",
    "  project_name=\"llamacloud_demo\",\n",
    "  api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f14ff-45b1-41f4-84e2-a6e5d6637809",
   "metadata": {},
   "source": [
    "## Setup Auto-Retrieval\n",
    "\n",
    "Now we setup an **auto-retrieval** function over our LlamaCloud retrievers. At a high-level our auto-retrieval function uses a function-calling LLM to infer the metadata filters for a user query - this leads to more precise and relevant retrieval results beyond just using a raw semantic query.\n",
    "\n",
    "This section shows you how to build it from scratch, also includes some advanced few-shot example selection to increase reliability.\n",
    "1. Define a custom prompt to generate metadata filters\n",
    "2. Given a user query, first do chunk-level retrieval to dynamically retrieve the metadata of the retrieved chunks.\n",
    "3. Inject the metadata as few-shot examples in the auto-retrieval prompt. The goal is to show the LLM what existing, relevant examples of metadata values already look like, so that the LLM can infer correct metadata filters.\n",
    "\n",
    "A lot of the code below is lifted from our **VectorIndexAutoRetriever** module, which provides an out of the box way to do auto-retrieval against a vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "652cb067-da39-42cb-a303-faa346f72e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0a564cb-bfdb-48a5-9d67-10390c3a6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.vector_stores.types import VectorStoreInfo, VectorStoreQuerySpec, MetadataInfo, MetadataFilters\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import Response\n",
    "\n",
    "import json\n",
    "\n",
    "SYS_PROMPT = \"\"\"\\\n",
    "Your goal is to structure the user's query to match the request schema provided below.\n",
    "You MUST call the tool in order to generate the query spec.\n",
    "\n",
    "<< Structured Request Schema >>\n",
    "When responding use a markdown code snippet with a JSON object formatted in the \\\n",
    "following schema:\n",
    "\n",
    "{schema_str}\n",
    "\n",
    "The query string should contain only text that is expected to match the contents of \\\n",
    "documents. Any conditions in the filter should not be mentioned in the query as well.\n",
    "\n",
    "Make sure that filters only refer to attributes that exist in the data source.\n",
    "Make sure that filters take into account the descriptions of attributes.\n",
    "Make sure that filters are only used as needed. If there are no filters that should be \\\n",
    "applied return [] for the filter value.\\\n",
    "\n",
    "If the user's query explicitly mentions number of documents to retrieve, set top_k to \\\n",
    "that number, otherwise do not set top_k.\n",
    "\n",
    "The schema of the metadata filters in the vector db table is listed below, along with some example metadata dictionaries from relevant rows.\n",
    "The user will send the input query string.\n",
    "\n",
    "Data Source:\n",
    "```json\n",
    "{info_str}\n",
    "```\n",
    "\n",
    "Example metadata from relevant chunks:\n",
    "{example_rows}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "example_rows_retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    rerank_top_n=4\n",
    ")\n",
    "\n",
    "def get_example_rows_fn(**kwargs):\n",
    "    \"\"\"Retrieve relevant few-shot examples.\"\"\"\n",
    "    query_str = kwargs[\"query_str\"]\n",
    "    nodes = example_rows_retriever.retrieve(query_str)\n",
    "    # get the metadata, join them\n",
    "    metadata_list = [n.metadata for n in nodes]\n",
    "\n",
    "    return \"\\n\".join([json.dumps(m) for m in metadata_list])\n",
    "        \n",
    "    \n",
    "\n",
    "# TODO: define function mapping for `example_rows`.\n",
    "chat_prompt_tmpl = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        (\"user\", \"{query_str}\"),\n",
    "    ],\n",
    "    function_mappings={\n",
    "        \"example_rows\": get_example_rows_fn\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "## NOTE: this is a dataclass that contains information about the metadata\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"contains content from various research papers\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"file_name\",\n",
    "            type=\"str\",\n",
    "            description=\"Name of the source paper\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "def auto_retriever_rag(query: str, retrieve_doc: bool = False, files_top_k: int = 1, rerank_top_n: int = 5) -> Response:\n",
    "    \"\"\"Synthesizes an answer to your question by feeding in an entire relevant document as context.\"\"\"\n",
    "    print(f\"> User query string: {query}\")\n",
    "    # Use structured predict to infer the metadata filters and query string.\n",
    "    query_spec = llm.structured_predict(\n",
    "        VectorStoreQuerySpec,\n",
    "        chat_prompt_tmpl,\n",
    "        info_str=vector_store_info.model_dump_json(indent=4),\n",
    "        schema_str=json.dumps(VectorStoreQuerySpec.model_json_schema()),\n",
    "        query_str=query\n",
    "    )\n",
    "    # build retriever and query engine\n",
    "    filters = MetadataFilters(filters=query_spec.filters) if len(query_spec.filters) > 0 else None\n",
    "    print(f\"> Inferred query string: {query_spec.query}\")\n",
    "    if filters:\n",
    "        print(f\"> Inferred filters: {filters.json()}\")\n",
    "\n",
    "    # define retriever based on whether chunk or document-level is specified\n",
    "    if retrieve_doc:\n",
    "        retriever = index.as_retriever(\n",
    "            retrieval_mode=\"files_via_content\",\n",
    "            # retrieval_mode=\"files_via_metadata\",\n",
    "            files_top_k=files_top_k,\n",
    "            filters=filters\n",
    "        )\n",
    "    else:\n",
    "        retriever = index.as_retriever(\n",
    "            retrieval_mode=\"chunks\",\n",
    "            rerank_top_n=rerank_top_n,\n",
    "            filters=filters\n",
    "        )\n",
    "    \n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, \n",
    "        llm=llm,\n",
    "        response_mode=\"tree_summarize\"\n",
    "    )\n",
    "    # run query\n",
    "    return query_engine.query(query_spec.query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67303e6-ec65-499b-85bb-8189d220b466",
   "metadata": {},
   "source": [
    "### Try out Auto-Retrieval\n",
    "\n",
    "Let's try running our auto-retriever on some sample queries. We try out both the chunk-level and document-level retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7533424-ae1f-4f04-9d31-f4a41b2b2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "auto_doc_rag = partial(auto_retriever_rag, retrieve_doc=True)\n",
    "auto_chunk_rag = partial(auto_retriever_rag, retrieve_doc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e664037b-3d89-481e-8e0e-2c1d47ffc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: ELI5 the objective function in Metra\n",
      "> Inferred query string: objective function in Metra\n",
      "> Inferred filters: {\"filters\":[{\"key\":\"file_name\",\"value\":\"metra.pdf\",\"operator\":\"==\"}],\"condition\":\"and\"}\n",
      "The objective function in METRA involves maximizing the expected inner product of the difference in state representations and a skill vector, subject to a Lipschitz constraint under the temporal distance metric. This is expressed as maximizing the expected value of \\((\\phi(s') - \\phi(s))^\\top z\\), where \\(\\phi\\) is a representation function, \\(s\\) and \\(s'\\) are states, and \\(z\\) is a skill vector. The constraint ensures that the difference in representations is bounded by the temporal distance between states, promoting the discovery of diverse behaviors that cover the latent space effectively.\n"
     ]
    }
   ],
   "source": [
    "response = auto_chunk_rag(\"ELI5 the objective function in Metra\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad5758fb-63dc-4815-a0b5-54224f784230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: How was SWE-Bench constructed? Tell me all the stages that went into it.\n",
      "> Inferred query string: SWE-Bench construction stages\n",
      "> Inferred filters: {\"filters\":[{\"key\":\"file_name\",\"value\":\"swebench.pdf\",\"operator\":\"==\"}],\"condition\":\"and\"}\n",
      "The construction of SWE-bench involves a three-stage pipeline:\n",
      "\n",
      "1. **Repo Selection and Data Scraping**: This stage involves collecting pull requests (PRs) from 12 popular open-source Python repositories on GitHub, resulting in approximately 90,000 PRs. The focus is on popular repositories due to their better maintenance, clear contributor guidelines, and comprehensive test coverage.\n",
      "\n",
      "2. **Attribute-based Filtering**: In this stage, candidate tasks are created by selecting merged PRs that resolve a GitHub issue and make changes to the test files of the repository. This indicates that the user likely contributed tests to verify the resolution of the issue.\n",
      "\n",
      "3. **Execution-based Filtering**: For each candidate task, the PR’s test content is applied, and the associated test results are logged before and after the PR’s other content is applied. Task instances without at least one test changing from fail to pass are filtered out, as well as instances resulting in installation or runtime errors.\n",
      "\n",
      "Through these stages, the original 90,000 PRs are filtered down to 2,294 task instances that comprise SWE-bench.\n"
     ]
    }
   ],
   "source": [
    "response = auto_chunk_rag(\"How was SWE-Bench constructed? Tell me all the stages that went into it.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8bd8cdb-44e8-4cad-9d3c-8207e336f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: Give me a summary of the SWE-bench paper\n",
      "> Inferred query string: summary of the SWE-bench paper\n",
      "> Inferred filters: {\"filters\":[{\"key\":\"file_name\",\"value\":\"swebench.pdf\",\"operator\":\"==\"}],\"condition\":\"and\"}\n",
      "The SWE-bench paper introduces a new benchmark designed to evaluate the capabilities of language models (LMs) in real-world software engineering tasks. This benchmark, called SWE-bench, consists of 2,294 software engineering problems derived from GitHub issues and corresponding pull requests across 12 popular Python repositories. The task for the language models is to generate a pull request that resolves a given issue and passes the associated tests. SWE-bench challenges models to handle complex reasoning, long contexts, and cross-file code editing, which are typical in real-world software development but not commonly addressed in existing benchmarks.\n",
      "\n",
      "The paper highlights that current state-of-the-art models, including proprietary ones like Claude 2, struggle with these tasks, solving only a small percentage of them. The authors also introduce SWE-Llama, a fine-tuned version of the CodeLlama model, which shows competitive performance in some settings. SWE-bench is designed to be continually updatable, allowing for the inclusion of new task instances over time. The benchmark aims to push the development of more practical, intelligent, and autonomous language models for software engineering applications.\n"
     ]
    }
   ],
   "source": [
    "response = auto_doc_rag(\"Give me a summary of the SWE-bench paper\") \n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c624a69b-637f-4c3d-b888-7f1525856d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: Give me a summary of the Self-RAG paper\n",
      "> Inferred query string: summary of the Self-RAG paper\n",
      "> Inferred filters: {\"filters\":[{\"key\":\"file_name\",\"value\":\"selfrag.pdf\",\"operator\":\"==\"}],\"condition\":\"and\"}\n",
      "The Self-RAG paper introduces a framework called Self-Reflective Retrieval-Augmented Generation (SELF-RAG) designed to enhance the quality and factuality of large language models (LLMs). SELF-RAG improves upon traditional Retrieval-Augmented Generation (RAG) by incorporating a self-reflection mechanism that allows the model to retrieve relevant information on-demand and critique its own outputs. This is achieved through the use of reflection tokens, which guide the model in deciding when to retrieve information and how to evaluate the relevance and support of the retrieved passages. The framework enables the model to adapt its behavior to different tasks, improving factual accuracy and citation precision. Experiments demonstrate that SELF-RAG outperforms state-of-the-art LLMs and retrieval-augmented models across various tasks, including open-domain question answering and long-form generation. The approach allows for customizable inference-time behavior, balancing retrieval frequency and model creativity based on task requirements.\n"
     ]
    }
   ],
   "source": [
    "response = auto_doc_rag(\"Give me a summary of the Self-RAG paper\") \n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7107d-a4f3-4ed3-9671-d27973d0a3fe",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've learned the basics of auto-retrieval, you can choose to build a standalone RAG pipeline powered by this, or choose to plug this in as part of a broader agentic system. For instance, you can plug in both chunk and doc-level auto-retriever pipelines as tools for an agent to interact with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
