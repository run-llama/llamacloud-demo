{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b2c37d-3b5a-47aa-95b9-d28e0bc83f77",
   "metadata": {},
   "source": [
    "# Auto-Retrieval with LlamaCloud\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/auto_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "**Auto-retrieval** is an advanced RAG technique that uses an LLM to dynamically infer the metadata filter parameters along with the semantic query before initiating vector database retrieval, in comparison to naive RAG which directly sends the user query to the vector db retrieval interface (e.g. dense vector search). It can both be thought of as a form of query expansion/rewriting if you come from the retrieval world, as well as a specific form of function calling.\n",
    "\n",
    "LlamaCloud helps you easily define chunk and document-level retrieval interfaces on top of any documents. In this guide we show you how to build an auto-retrieval pipeline on top of LlamaCloud retrievers over a research document corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f707a-c7b5-473f-b4a6-881e2245e82d",
   "metadata": {},
   "source": [
    "## Setup LlamaCloud \n",
    "\n",
    "Install core packages and download relevant files. Upload these documents to LlamaCloud, and then define a chunk and document-level retriever interface over these documents.\n",
    "\n",
    "For more information on chunk-level and document-level retrieval, check out our interface [here](https://github.com/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa458bc-bc8d-46fe-9a57-021dd8d9e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-core\n",
    "!pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79821400-caaf-42f1-99d8-74c184c19e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: uncomment more papers if you want to do research over a larger subset of docs\n",
    "\n",
    "urls = [\n",
    "    # \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    # \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    # \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    # \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    # \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    # \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    # \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    # \"https://openreview.net/pdf?id=TpD2aG1h0D\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    # \"metagpt.pdf\",\n",
    "    # \"longlora.pdf\",\n",
    "    # \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    # \"zipformer.pdf\",\n",
    "    # \"values.pdf\",\n",
    "    # \"finetune_fair_diffusion.pdf\",\n",
    "    # \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    # \"vr_mcl.pdf\",\n",
    "]\n",
    "\n",
    "data_dir = \"iclr_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80137d15-f22b-47eb-adce-ac295ced7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"{data_dir}\"\n",
    "for url, paper in zip(urls, papers):\n",
    "    !wget \"{url}\" -O \"{data_dir}/{paper}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315d639-52a0-4888-8779-653083ec3768",
   "metadata": {},
   "source": [
    "#### Load Documents into LlamaCloud\n",
    "\n",
    "Create a new index in LlamaCloud and drag and drop these downloaded PDFs into the data source.\n",
    "\n",
    "For best results, in the Transformation Configuration click on the \"Manual\" tab, and set page-level segmentation configuration and \"None\" for additional chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579938a-54b5-4eb2-b97e-22d96382eded",
   "metadata": {},
   "source": [
    "#### Setup LlamaCloud Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4208977f-896f-4993-a3a2-1be83c92baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "import os\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=\"research_papers_page\",\n",
    "  project_name=\"llamacloud_demo\",\n",
    "  api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be7871-5190-437f-aa87-f3f73f2e49c4",
   "metadata": {},
   "source": [
    "#### Define LlamaCloud File/Chunk Retriever over Documents\n",
    "\n",
    "The document-level retriever returns documents at the level of entire files, and the chunk-level retriever returns specific chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e67dc0-11f9-4186-a29a-40a2c14d9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retriever = index.as_retriever(\n",
    "    retrieval_mode=\"files_via_content\",\n",
    "    # retrieval_mode=\"files_via_metadata\",\n",
    "    files_top_k=1\n",
    ")\n",
    "\n",
    "chunk_retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    rerank_top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f14ff-45b1-41f4-84e2-a6e5d6637809",
   "metadata": {},
   "source": [
    "## Setup Auto-Retrieval\n",
    "\n",
    "Now we setup an **auto-retrieval** function over our LlamaCloud retrievers. At a high-level our auto-retrieval function uses a function-calling LLM to infer the metadata filters for a user query - this leads to more precise and relevant retrieval results beyond just using a raw semantic query.\n",
    "\n",
    "This section shows you how to build it from scratch, also includes some advanced few-shot example selection to increase reliability.\n",
    "1. Define a custom prompt to generate metadata filters\n",
    "2. Given a user query, first do chunk-level retrieval to dynamically retrieve the metadata of the retrieved chunks.\n",
    "3. Inject the metadata as few-shot examples in the auto-retrieval prompt. The goal is to show the LLM what existing, relevant examples of metadata values already look like, so that the LLM can infer correct metadata filters.\n",
    "\n",
    "A lot of the code below is lifted from our **VectorIndexAutoRetriever** module, which provides an out of the box way to do auto-retrieval against a vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "652cb067-da39-42cb-a303-faa346f72e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0a564cb-bfdb-48a5-9d67-10390c3a6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.vector_stores.types import VectorStoreInfo, VectorStoreQuerySpec, MetadataInfo, MetadataFilters\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import Response\n",
    "\n",
    "import json\n",
    "\n",
    "SYS_PROMPT = \"\"\"\\\n",
    "Your goal is to structure the user's query to match the request schema provided below.\n",
    "You MUST call the tool in order to generate the query spec.\n",
    "\n",
    "<< Structured Request Schema >>\n",
    "When responding use a markdown code snippet with a JSON object formatted in the \\\n",
    "following schema:\n",
    "\n",
    "{schema_str}\n",
    "\n",
    "The query string should contain only text that is expected to match the contents of \\\n",
    "documents. Any conditions in the filter should not be mentioned in the query as well.\n",
    "\n",
    "Make sure that filters only refer to attributes that exist in the data source.\n",
    "Make sure that filters take into account the descriptions of attributes.\n",
    "Make sure that filters are only used as needed. If there are no filters that should be \\\n",
    "applied return [] for the filter value.\\\n",
    "\n",
    "If the user's query explicitly mentions number of documents to retrieve, set top_k to \\\n",
    "that number, otherwise do not set top_k.\n",
    "\n",
    "The schema of the metadata filters in the vector db table is listed below, along with some example metadata dictionaries from relevant rows.\n",
    "The user will send the input query string.\n",
    "\n",
    "Data Source:\n",
    "```json\n",
    "{info_str}\n",
    "```\n",
    "\n",
    "Example metadata from relevant chunks:\n",
    "{example_rows}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "example_rows_retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    rerank_top_n=4\n",
    ")\n",
    "\n",
    "def get_example_rows_fn(**kwargs):\n",
    "    \"\"\"Retrieve relevant few-shot examples.\"\"\"\n",
    "    query_str = kwargs[\"query_str\"]\n",
    "    nodes = example_rows_retriever.retrieve(query_str)\n",
    "    # get the metadata, join them\n",
    "    metadata_list = [n.metadata for n in nodes]\n",
    "\n",
    "    return \"\\n\".join([json.dumps(m) for m in metadata_list])\n",
    "        \n",
    "    \n",
    "\n",
    "# TODO: define function mapping for `example_rows`.\n",
    "chat_prompt_tmpl = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        (\"user\", \"{query_str}\"),\n",
    "    ],\n",
    "    function_mappings={\n",
    "        \"example_rows\": get_example_rows_fn\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "## NOTE: this is a dataclass that contains information about the metadata\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"contains content from various research papers\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"file_name\",\n",
    "            type=\"str\",\n",
    "            description=\"Name of the source paper\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "def auto_retriever_rag(query: str, retriever: BaseRetriever) -> Response:\n",
    "    \"\"\"Synthesizes an answer to your question by feeding in an entire relevant document as context.\"\"\"\n",
    "    print(f\"> User query string: {query}\")\n",
    "    # Use structured predict to infer the metadata filters and query string.\n",
    "    query_spec = llm.structured_predict(\n",
    "        VectorStoreQuerySpec,\n",
    "        chat_prompt_tmpl,\n",
    "        info_str=vector_store_info.json(indent=4),\n",
    "        schema_str=VectorStoreQuerySpec.schema_json(indent=4),\n",
    "        query_str=query\n",
    "    )\n",
    "    # build retriever and query engine\n",
    "    filters = MetadataFilters(filters=query_spec.filters) if len(query_spec.filters) > 0 else None\n",
    "    print(f\"> Inferred query string: {query_spec.query}\")\n",
    "    if filters:\n",
    "        print(f\"> Inferred filters: {filters.json()}\")\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, \n",
    "        llm=llm,\n",
    "        response_mode=\"tree_summarize\"\n",
    "    )\n",
    "    # run query\n",
    "    return query_engine.query(query_spec.query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67303e6-ec65-499b-85bb-8189d220b466",
   "metadata": {},
   "source": [
    "### Try out Auto-Retrieval\n",
    "\n",
    "Let's try running our auto-retriever on some sample queries. We try out both the chunk-level and document-level retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7533424-ae1f-4f04-9d31-f4a41b2b2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "auto_doc_rag = partial(auto_retriever_rag, retriever=doc_retriever)\n",
    "auto_chunk_rag = partial(auto_retriever_rag, retriever=chunk_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e664037b-3d89-481e-8e0e-2c1d47ffc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: ELI5 the objective function in Metra\n",
      "> Inferred query string: objective function in Metra\n",
      "The objective function in METRA involves maximizing the Wasserstein dependency measure (WDM) using a tractable approach. This is achieved by jointly training a 1-Lipschitz-constrained score function \\( f(s, z) \\) and a skill policy \\( \\pi(a|s, z) \\) with the reward function being an empirical estimate of the WDM. The simplified objective is expressed as:\n",
      "\n",
      "\\[ IW(S; Z) \\approx \\sup_{\\|\\varphi\\|_L \\le 1} \\mathbb{E}_{p(s, z)}[\\varphi(s)^\\top \\psi(z)] - \\mathbb{E}_{p(s)}[\\varphi(s)]^\\top \\mathbb{E}_{p(z)}[\\psi(z)] \\]\n",
      "\n",
      "where \\( \\varphi(s) \\) and \\( \\psi(z) \\) are parameterizations with independent 1-Lipschitz constraints. The full objective also incorporates the temporal distance between states as a distance metric, which is crucial for learning a compact set of useful behaviors.\n"
     ]
    }
   ],
   "source": [
    "response = auto_chunk_rag(\"ELI5 the objective function in Metra\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad5758fb-63dc-4815-a0b5-54224f784230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: How was SWE-Bench constructed? Tell me all the stages that went into it.\n",
      "> Inferred query string: SWE-Bench construction stages\n",
      "The construction of SWE-Bench involves a three-stage pipeline:\n",
      "\n",
      "1. **Repo Selection and Data Scraping**: Pull requests (PRs) are collected from 12 popular open-source Python repositories on GitHub, resulting in approximately 90,000 PRs. These repositories are chosen for their popularity, better maintenance, clear contributor guidelines, and extensive test coverage.\n",
      "\n",
      "2. **Attribute-Based Filtering**: Candidate tasks are created by selecting merged PRs that resolve a GitHub issue and make changes to the test files of the repository. This indicates that the user likely contributed tests to check whether the issue has been resolved.\n",
      "\n",
      "3. **Execution-Based Filtering**: For each candidate task, the PR’s test content is applied, and the associated test results are logged before and after the PR’s other content is applied. Tasks are filtered out if they do not have at least one test where its status changes from fail to pass or if they result in installation or runtime errors.\n",
      "\n",
      "Through these stages, the original 90,000 PRs are filtered down to 2,294 task instances that comprise SWE-Bench.\n"
     ]
    }
   ],
   "source": [
    "response = auto_chunk_rag(\"How was SWE-Bench constructed? Tell me all the stages that went into it.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8bd8cdb-44e8-4cad-9d3c-8207e336f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: Give me a summary of the SWE-bench paper\n",
      "> Inferred query string: summary of the SWE-bench paper\n",
      "> Inferred filters: {\"filters\": [{\"key\": \"file_name\", \"value\": \"swebench.pdf\", \"operator\": \"==\"}], \"condition\": \"and\"}\n",
      "The construction of SWE-Bench involves a three-stage pipeline:\n",
      "\n",
      "1. **Repo Selection and Data Scraping**: Pull requests (PRs) are collected from 12 popular open-source Python repositories on GitHub, resulting in approximately 90,000 PRs. These repositories are chosen for their popularity, better maintenance, clear contributor guidelines, and extensive test coverage.\n",
      "\n",
      "2. **Attribute-Based Filtering**: Candidate tasks are created by selecting merged PRs that resolve a GitHub issue and make changes to the test files of the repository. This indicates that the user likely contributed tests to check whether the issue has been resolved.\n",
      "\n",
      "3. **Execution-Based Filtering**: For each candidate task, the PR’s test content is applied, and the associated test results are logged before and after the PR’s other content is applied. Tasks are filtered out if they do not have at least one test where its status changes from fail to pass or if they result in installation or runtime errors.\n",
      "\n",
      "Through these stages, the original 90,000 PRs are filtered down to 2,294 task instances that comprise SWE-Bench.\n"
     ]
    }
   ],
   "source": [
    "auto_doc_rag(\"Give me a summary of the SWE-bench paper\") \n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c624a69b-637f-4c3d-b888-7f1525856d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User query string: Give me a summary of the Self-RAG paper\n",
      "> Inferred query string: summary of the Self-RAG paper\n",
      "> Inferred filters: {\"filters\": [{\"key\": \"file_name\", \"value\": \"selfrag.pdf\", \"operator\": \"==\"}], \"condition\": \"and\"}\n",
      "The construction of SWE-Bench involves a three-stage pipeline:\n",
      "\n",
      "1. **Repo Selection and Data Scraping**: Pull requests (PRs) are collected from 12 popular open-source Python repositories on GitHub, resulting in approximately 90,000 PRs. These repositories are chosen for their popularity, better maintenance, clear contributor guidelines, and extensive test coverage.\n",
      "\n",
      "2. **Attribute-Based Filtering**: Candidate tasks are created by selecting merged PRs that resolve a GitHub issue and make changes to the test files of the repository. This indicates that the user likely contributed tests to check whether the issue has been resolved.\n",
      "\n",
      "3. **Execution-Based Filtering**: For each candidate task, the PR’s test content is applied, and the associated test results are logged before and after the PR’s other content is applied. Tasks are filtered out if they do not have at least one test where its status changes from fail to pass or if they result in installation or runtime errors.\n",
      "\n",
      "Through these stages, the original 90,000 PRs are filtered down to 2,294 task instances that comprise SWE-Bench.\n"
     ]
    }
   ],
   "source": [
    "auto_doc_rag(\"Give me a summary of the Self-RAG paper\") \n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7107d-a4f3-4ed3-9671-d27973d0a3fe",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've learned the basics of auto-retrieval, you can choose to build a standalone RAG pipeline powered by this, or choose to plug this in as part of a broader agentic system. For instance, you can plug in both chunk and doc-level auto-retriever pipelines as tools for an agent to interact with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
