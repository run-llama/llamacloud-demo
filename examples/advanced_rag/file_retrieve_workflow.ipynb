{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ⚠️ Important Notice\n\nThis notebook (and repository) is deprecated.\n\nFor the latest python examples, please refer to the `llama-cloud-services` repository examples: \nhttps://github.com/run-llama/llama_cloud_services/tree/main/examples\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# File-level and Chunk-Level Retrieval with LlamaCloud and Workflows\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/file_retrieve_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "In this notebook we will show you how to perform file-level and chunk-level retrieval with LlamaCloud using a custom router query engine and a custom agent built with [Workflows](https://docs.llamaindex.ai/en/latest/module_guides/workflow/).\n",
        "\n",
        "![](file_retrieve_workflow_img.png)\n",
        "\n",
        "File-level retrieval is useful for handling user questions that require the entire document context to properly answer the question. Since only doing file-level retrieval can be slow + expensive, we also show you how to build an agent that can dynamically decide whether to do file-level or chunk-level retrieval! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install LlamaIndex, apply nest_asyncio, and set up your OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama-index llama-index-indices-managed-llama-cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<Your OpenAI API Key>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Optional] Setup Observability\n",
        "\n",
        "We setup an integration with LlamaTrace (integration with Arize).\n",
        "\n",
        "If you haven't already done so, make sure to create an account here: https://llamatrace.com/login. Then create an API key and put it in the `PHOENIX_API_KEY` variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U llama-index-callbacks-arize-phoenix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup Arize Phoenix for logging/observability\n",
        "import llama_index.core\n",
        "import os\n",
        "\n",
        "PHOENIX_API_KEY = \"<phoenix_api_key>\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
        "llama_index.core.set_global_handler(\n",
        "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Documents into LlamaCloud\n",
        "\n",
        "The first order of business is to download the 5 Apple and Tesla 10Ks and upload them into LlamaCloud.\n",
        "\n",
        "You can easily do this by creating a pipeline and uploading docs via the \"Files\" mode.\n",
        "\n",
        "After this is done, proceed to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "# download Apple \n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf\" -O data/apple_2023.pdf\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2022/q4/_10-K-2022-(As-Filed).pdf\" -O data/apple_2022.pdf\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2021/q4/_10-K-2021-(As-Filed).pdf\" -O data/apple_2021.pdf\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2020/ar/_10-K-2020-(As-Filed).pdf\" -O data/apple_2020.pdf\n",
        "!wget \"https://www.dropbox.com/scl/fi/i6vk884ggtq382mu3whfz/apple_2019_10k.pdf?rlkey=eudxh3muxh7kop43ov4bgaj5i&dl=1\" -O data/apple_2019.pdf\n",
        "\n",
        "# download Tesla\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf\" -O data/tesla_2023.pdf\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000095017023001409/tsla-20221231-gen.pdf\" -O data/tesla_2022.pdf\n",
        "!wget \"https://www.dropbox.com/scl/fi/ptk83fmye7lqr7pz9r6dm/tesla_2021_10k.pdf?rlkey=24kxixeajbw9nru1sd6tg3bye&dl=1\" -O data/tesla_2021.pdf\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459021004599/tsla-10k_20201231-gen.pdf\" -O data/tesla_2020.pdf\n",
        "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459020004475/tsla-10k_20191231-gen_0.pdf\" -O data/tesla_2019.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Classes\n",
        "\n",
        "We define the `Answer` model, which is a model that stores whether to pick chunk-level retrieval or document-level retrieval, along with a reason for that choice. We will let the LLM choose given a query string, and we will ask the LLM to produce a JSON output that can be parsed by a Pydantic model.\n",
        "\n",
        "We will define the `RouterOutputParser` helper class, which parses the output from the LLM into a list of `Answer` models, which is then put into the `Answers` model that contains a list of `Answer`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from llama_index.core.bridge.pydantic import BaseModel\n",
        "from typing import List\n",
        "from llama_index.core.types import BaseOutputParser\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# tells LLM to select choices given a list\n",
        "ROUTER_PROMPT = PromptTemplate(\n",
        "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
        "    \" {num_choices}), where each item in the list corresponds to a\"\n",
        "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n",
        "    \" only the choices above and not prior knowledge, return the top choices\"\n",
        "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n",
        "    \" most relevant to the question: '{query_str}'\\n\"\n",
        ")\n",
        "\n",
        "# tells LLM to format list of choices in a certain way\n",
        "FORMAT_STR = \"\"\"The output should be formatted as a JSON instance that conforms to \n",
        "the JSON schema below. \n",
        "\n",
        "Here is the output schema:\n",
        "{\n",
        "  \"type\": \"array\",\n",
        "  \"items\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"choice\": {\n",
        "        \"type\": \"integer\"\n",
        "      },\n",
        "      \"reason\": {\n",
        "        \"type\": \"string\"\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\n",
        "      \"choice\",\n",
        "      \"reason\"\n",
        "    ],\n",
        "    \"additionalProperties\": false\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "class Answer(BaseModel):\n",
        "    \"\"\"Answer model.\"\"\"\n",
        "\n",
        "    choice: int\n",
        "    reason: str\n",
        "\n",
        "\n",
        "class Answers(BaseModel):\n",
        "    \"\"\"List of answers model.\"\"\"\n",
        "\n",
        "    answers: List[Answer]\n",
        "\n",
        "class RouterOutputParser(BaseOutputParser):\n",
        "    \"\"\"Custom output parser.\"\"\"\n",
        "\n",
        "    def _escape_curly_braces(self, input_string: str):\n",
        "        \"\"\"Escape the brackets in the format string so contents are not treated as variables.\"\"\"\n",
        "\n",
        "        return input_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
        "\n",
        "    def _marshal_output_to_json(self, output: str):\n",
        "        \"\"\"Find JSON string within response.\"\"\"\n",
        "\n",
        "        output = output.strip()\n",
        "        left = output.find(\"[\")\n",
        "        right = output.find(\"]\")\n",
        "        output = output[left : right + 1]\n",
        "        return output\n",
        "\n",
        "    def parse(self, output: str) -> Answers:\n",
        "        \"\"\"Parse string\"\"\"\n",
        "\n",
        "        json_output = self._marshal_output_to_json(output)\n",
        "        json_dicts = json.loads(json_output)\n",
        "        answers = [Answer.parse_obj(json_dict) for json_dict in json_dicts]\n",
        "        return Answers(answers=answers)\n",
        "    \n",
        "    def format(self, query: str) -> str:\n",
        "        return query + \"\\n\\n\" + self._escape_curly_braces(FORMAT_STR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Router Query Workflow\n",
        "\n",
        "In the code snippet below, we define a router query workflow. This workflow requires 2 events: a `ChooseQueryEngineEvent`, which chooses the document-level or chunk-retrieval query engine, and `SynthesizeAnswersEvent`, which contains the results from the query engines and synthesizes a final response.\n",
        "\n",
        "The workflow consists of the following steps:\n",
        "1. Choosing the query engine(s) by passing the prompt and output parser defined above into an LLM. Both query engines can be chosen if the LLM thinks both query engines (defined in `choose_query_engine()`).\n",
        "2. Queries the engines chosen by the LLM in the previous step (defined in `query_each_engine`).\n",
        "3. Synthesizes a final response given the results from the queries above (defined in `synthesize_response()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional, Any\n",
        "\n",
        "from llama_index.core.query_engine import (\n",
        "    BaseQueryEngine,\n",
        "    RetrieverQueryEngine,\n",
        ")\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.llms import LLM\n",
        "from llama_index.core.response_synthesizers import TreeSummarize\n",
        "from llama_index.core.workflow import (\n",
        "    Workflow,\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    step,\n",
        ")\n",
        "\n",
        "class ChooseQueryEngineEvent(Event):\n",
        "    \"\"\"Query engine event.\"\"\"\n",
        "\n",
        "    answers: Answers\n",
        "    query_str: str\n",
        "\n",
        "class SynthesizeAnswersEvent(Event):\n",
        "    \"\"\"Synthesize answers event.\"\"\"\n",
        "\n",
        "    responses: List[Any]\n",
        "    query_str: str\n",
        "\n",
        "\n",
        "class RouterQueryWorkflow(Workflow):\n",
        "    \"\"\"Router query workflow.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_engines: List[BaseQueryEngine],\n",
        "        choice_descriptions: List[str],\n",
        "        router_prompt: PromptTemplate,\n",
        "        timeout: Optional[float] = 10.0,\n",
        "        disable_validation: bool = False,\n",
        "        verbose: bool = False,\n",
        "        llm: Optional[LLM] = None,\n",
        "        summarizer: Optional[TreeSummarize] = None,\n",
        "    ):\n",
        "        \"\"\"Constructor\"\"\"\n",
        "\n",
        "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
        "\n",
        "        self.query_engines: List[BaseQueryEngine] = query_engines\n",
        "        self.choice_descriptions: List[str] = choice_descriptions\n",
        "        self.router_prompt: PromptTemplate = router_prompt\n",
        "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "        self.summarizer: TreeSummarize = summarizer or TreeSummarize()\n",
        "\n",
        "    def _get_choice_str(self, choices):\n",
        "        \"\"\"String of choices to feed into LLM.\"\"\"\n",
        "\n",
        "        choices_str = \"\\n\\n\".join([f\"{idx+1}. {c}\" for idx, c in enumerate(choices)])\n",
        "        return choices_str\n",
        "    \n",
        "    async def _query(self, query_str: str, choice_idx: int):\n",
        "        \"\"\"Query using query engine\"\"\"\n",
        "\n",
        "        query_engine = self.query_engines[choice_idx]\n",
        "        return await query_engine.aquery(query_str)\n",
        "\n",
        "    \n",
        "    @step()\n",
        "    async def choose_query_engine(self, ev: StartEvent) -> ChooseQueryEngineEvent:\n",
        "        \"\"\"Choose query engine.\"\"\"\n",
        "\n",
        "        # get query str\n",
        "        query_str = ev.get(\"query_str\")\n",
        "        if query_str is None:\n",
        "            raise ValueError(\"'query_str' is required.\")\n",
        "        \n",
        "        # partially format prompt with number of choices and max outputs\n",
        "        router_prompt1 = self.router_prompt.partial_format(\n",
        "            num_choices=len(self.choice_descriptions),\n",
        "            max_outputs=len(self.choice_descriptions),\n",
        "        )\n",
        "        \n",
        "        \n",
        "        # get choices selected by LLM\n",
        "        choices_str = self._get_choice_str(self.choice_descriptions)\n",
        "        output = llm.structured_predict(\n",
        "            Answers,\n",
        "            router_prompt1,\n",
        "            context_list=choices_str, \n",
        "            query_str=query_str\n",
        "        )\n",
        "\n",
        "        if self._verbose:\n",
        "            print(f\"Selected choice(s):\")\n",
        "            for answer in output.answers:\n",
        "                print(f\"Choice: {answer.choice}, Reason: {answer.reason}\")\n",
        "        \n",
        "        return ChooseQueryEngineEvent(answers=output, query_str=query_str)\n",
        "            \n",
        "    @step()\n",
        "    async def query_each_engine(self, ev: ChooseQueryEngineEvent) -> SynthesizeAnswersEvent:\n",
        "        \"\"\"Query each engine.\"\"\"\n",
        "\n",
        "        query_str = ev.query_str\n",
        "        answers = ev.answers\n",
        "\n",
        "        # query using corresponding query engine given in Answers list\n",
        "        responses = []\n",
        "\n",
        "        for answer in answers.answers:\n",
        "            choice_idx = answer.choice - 1\n",
        "            response = await self._query(query_str, choice_idx)\n",
        "            responses.append(response)\n",
        "        \n",
        "        return SynthesizeAnswersEvent(responses=responses, query_str=query_str)\n",
        "    \n",
        "    @step()\n",
        "    async def synthesize_response(self, ev: SynthesizeAnswersEvent) -> StopEvent:\n",
        "        \"\"\"Synthesizes response.\"\"\"\n",
        "\n",
        "        responses = ev.responses\n",
        "        query_str = ev.query_str\n",
        "\n",
        "        # get result of responses\n",
        "        if len(responses) == 1:\n",
        "            return StopEvent(result=responses[0])\n",
        "        else:\n",
        "            response_strs = [str(r) for r in responses]\n",
        "            result_response = self.summarizer.get_response(query_str, response_strs)\n",
        "            return StopEvent(result=result_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define LlamaCloud Retriever over documents\n",
        "\n",
        "We'll define an instance of `LLamaCloudIndex`, which will allow us to access the indexed docs stored on LlamaCloud. We define two separate retrievers for this index: a file-level retriever and a chunk-level retriever. We create two query engines from these retrievers.\n",
        "\n",
        "After this, we give a description for what each retriever does to allow the LLM to know which one to pick. We'll define our router workflow based on the two query engines and descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
        "\n",
        "index = LlamaCloudIndex(\n",
        "    name=\"<index_name>\", \n",
        "    project_name=\"<project_name>\",\n",
        "    organization_id=\"<organization_id>\",\n",
        "    # api_key=\"<Your API Key>\"\n",
        ")\n",
        "\n",
        "llm = OpenAI(\"gpt-4o\")\n",
        "\n",
        "doc_retriever = index.as_retriever(retrieval_mode=\"files_via_content\", files_top_k=1)\n",
        "query_engine_doc = RetrieverQueryEngine.from_args(\n",
        "    doc_retriever, llm=llm, response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "chunk_retriever = index.as_retriever(retrieval_mode=\"chunks\", rerank_top_n=10)\n",
        "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
        "    chunk_retriever, llm=llm, response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "DOC_METADATA_EXTRA_STR = \"\"\"\\\n",
        "Each document represents a complete 10K report for a given year (e.g. Apple in 2019).\n",
        "Here's an example of relevant documents:\n",
        "1. apple_2019.pdf\n",
        "2. tesla_2020.pdf\n",
        "\"\"\"\n",
        "\n",
        "TOOL_DOC_DESC = f\"\"\"\\\n",
        "Synthesizes an answer to your question by feeding in an entire relevant document as context. Best used for higher-level summarization options.\n",
        "Do NOT use if answer can be found in a specific chunk of a given document. Use the chunk_query_engine instead for that purpose.\n",
        "\n",
        "Below we give details on the format of each document:\n",
        "{DOC_METADATA_EXTRA_STR}\n",
        "\"\"\"\n",
        "\n",
        "TOOL_CHUNK_DESC = f\"\"\"\\\n",
        "Synthesizes an answer to your question by feeding in a relevant chunk as context. Best used for questions that are more pointed in nature.\n",
        "Do NOT use if the question asks seems to require a general summary of any given document. Use the doc_query_engine instead for that purpose.\n",
        "\n",
        "Below we give details on the format of each document:\n",
        "{DOC_METADATA_EXTRA_STR}\n",
        "\"\"\"\n",
        "\n",
        "router_query_workflow = RouterQueryWorkflow(\n",
        "    query_engines=[query_engine_doc, query_engine_chunk],\n",
        "    choice_descriptions=[TOOL_DOC_DESC, TOOL_CHUNK_DESC],\n",
        "    verbose=True,\n",
        "    llm=llm,\n",
        "    router_prompt=ROUTER_PROMPT,\n",
        "    timeout=60\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining our router query workflow, we'll create a query engine wrapper around this workflow, and we'll define a query engine tool around this wrapper to pass into an agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating an Agent Around the Query Engine\n",
        "\n",
        "We'll create a workflow that acts as an agent around the router query engine. In this workflow, we need four events:\n",
        "1. `GatherToolsEvent`: Gets all tools that need to be called (which is determined by the LLM).\n",
        "2. `ToolCallEvent`: An individual tool call. Multiple of these events will be triggered at the same time.\n",
        "3. `ToolCallEventResult`: Gets result from a tool call.\n",
        "4. `GatherEvent`: Returned from dispatcher that triggers the `ToolCallEvent`.\n",
        "\n",
        "This workflow consists of the following steps:\n",
        "1. `chat()`: Appends the message to the chat history. This chat history is fed into the LLM, along with the given tools, and the LLM determines which tools to call. This returns a `GatherToolsEvent`.\n",
        "2. `dispatch_calls()`: Triggers a `ToolCallEvent` for each tool call given in the `GatherToolsEvent` using `send_event()`. Returns a `GatherEvent` with the number of tool calls.\n",
        "3. `call_tool()`: Calls an individual tool. This step will run multiple times if there is more than one tool call. This step calls the tool and appends the result as a chat message to the chat history. It returns a `ToolCallEventResult` with the result of the tool call.\n",
        "4. `gather()`: Gathers the results from all tool calls using `collect_events()`. Waits for all tool calls to finish, then feeds chat history (following all tool calls) into the LLM. Returns the response from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "from llama_index.core.tools import BaseTool\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.llms.llm import ToolSelection\n",
        "from llama_index.core.workflow import Context, Workflow\n",
        "from llama_index.core.base.response.schema import Response\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "class InputEvent(Event):\n",
        "    \"\"\"Input event.\"\"\"\n",
        "\n",
        "class GatherToolsEvent(Event):\n",
        "    \"\"\"Gather Tools Event\"\"\"\n",
        "\n",
        "    tool_calls: Any\n",
        "\n",
        "class ToolCallEvent(Event):\n",
        "    \"\"\"Tool Call event\"\"\"\n",
        "\n",
        "    tool_call: ToolSelection\n",
        "\n",
        "class ToolCallEventResult(Event):\n",
        "    \"\"\"Tool call event result.\"\"\"\n",
        "\n",
        "    msg: ChatMessage\n",
        "\n",
        "\n",
        "class RouterOutputAgentWorkflow(Workflow):\n",
        "    \"\"\"Custom router output agent workflow.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        rag_workflow: Workflow,\n",
        "        timeout: Optional[float] = 10.0,\n",
        "        disable_validation: bool = False,\n",
        "        verbose: bool = False,\n",
        "        llm: Optional[LLM] = None,\n",
        "        chat_history: Optional[List[ChatMessage]] = None,\n",
        "    ):\n",
        "        \"\"\"Constructor.\"\"\"\n",
        "\n",
        "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
        "\n",
        "        self.rag_workflow = rag_workflow\n",
        "\n",
        "        def query_workflow(query_str: str) -> Response:\n",
        "            \"\"\"Queries 10k reports for a given year.\"\"\"\n",
        "            return self.rag_workflow.run(query_str=query_str)\n",
        "        \n",
        "        self.rag_workflow_tool = FunctionTool.from_defaults(query_workflow)\n",
        "        \n",
        "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "        self.chat_history: List[ChatMessage] = chat_history or []\n",
        "    \n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Resets Chat History\"\"\"\n",
        "\n",
        "        self.chat_history = []\n",
        "\n",
        "    @step()\n",
        "    async def prepare_chat(self, ev: StartEvent) -> InputEvent:\n",
        "        message = ev.get(\"message\")\n",
        "        if message is None:\n",
        "            raise ValueError(\"'message' field is required.\")\n",
        "        \n",
        "        # add msg to chat history\n",
        "        chat_history = self.chat_history\n",
        "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
        "        return InputEvent()\n",
        "\n",
        "    @step()\n",
        "    async def chat(self, ev: InputEvent) -> GatherToolsEvent | StopEvent:\n",
        "        \"\"\"Appends msg to chat history, then gets tool calls.\"\"\"\n",
        "\n",
        "        # Put msg into LLM with tools included\n",
        "        chat_res = await self.llm.achat_with_tools(\n",
        "            [self.rag_workflow_tool],\n",
        "            chat_history=self.chat_history,\n",
        "            verbose=self._verbose,\n",
        "            allow_parallel_tool_calls=True\n",
        "        )\n",
        "        tool_calls = self.llm.get_tool_calls_from_response(chat_res, error_on_no_tool_call=False)\n",
        "        \n",
        "        ai_message = chat_res.message\n",
        "        self.chat_history.append(ai_message)\n",
        "        if self._verbose:\n",
        "            print(f\"Chat message: {ai_message.content}\")\n",
        "\n",
        "        # no tool calls, return chat message.\n",
        "        if not tool_calls:\n",
        "            return StopEvent(result=ai_message.content)\n",
        "\n",
        "        return GatherToolsEvent(tool_calls=tool_calls)\n",
        "\n",
        "    @step(pass_context=True)\n",
        "    async def dispatch_calls(self, ctx: Context, ev: GatherToolsEvent) -> ToolCallEvent:\n",
        "        \"\"\"Dispatches calls.\"\"\"\n",
        "\n",
        "        tool_calls = ev.tool_calls\n",
        "        await ctx.set(\"num_tool_calls\", len(tool_calls))\n",
        "\n",
        "        # trigger tool call events\n",
        "        for tool_call in tool_calls:\n",
        "            ctx.send_event(ToolCallEvent(tool_call=tool_call))\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    @step()\n",
        "    async def call_tool(self, ev: ToolCallEvent) -> ToolCallEventResult:\n",
        "        \"\"\"Calls tool.\"\"\"\n",
        "\n",
        "        tool_call = ev.tool_call\n",
        "\n",
        "        # get tool ID and function call\n",
        "        id_ = tool_call.tool_id\n",
        "\n",
        "        if self._verbose:\n",
        "            print(f\"Calling function {tool_call.tool_name} with msg {tool_call.tool_kwargs}\")\n",
        "\n",
        "        # directly run workflow, don't call tools\n",
        "        output = await self.rag_workflow.run(**tool_call.tool_kwargs)\n",
        "        msg = ChatMessage(\n",
        "            name=tool_call.tool_name,\n",
        "            content=str(output),\n",
        "            role=\"tool\",\n",
        "            additional_kwargs={\n",
        "                \"tool_call_id\": id_,\n",
        "                \"name\": tool_call.tool_name\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return ToolCallEventResult(msg=msg)\n",
        "    \n",
        "    @step(pass_context=True)\n",
        "    async def gather(self, ctx: Context, ev: ToolCallEventResult) -> StopEvent | None:\n",
        "        \"\"\"Gathers tool calls.\"\"\"\n",
        "        # wait for all tool call events to finish.\n",
        "        tool_events = ctx.collect_events(ev, [ToolCallEventResult] * await ctx.get(\"num_tool_calls\"))\n",
        "        if not tool_events:\n",
        "            return None\n",
        "        \n",
        "        for tool_event in tool_events:\n",
        "            # append tool call chat messages to history\n",
        "            self.chat_history.append(tool_event.msg)\n",
        "        \n",
        "        # # after all tool calls finish, pass input event back, restart agent loop\n",
        "        return InputEvent()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates an instance of the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = RouterOutputAgentWorkflow(router_query_workflow, verbose=True, timeout=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'NoneType'>\n",
            "<class '__main__.ToolCallEventResult'>\n",
            "<class '__main__.GatherToolsEvent'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "<class '__main__.ToolCallEvent'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "<class '__main__.InputEvent'>\n",
            "workflow_all_flows.html\n"
          ]
        }
      ],
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(RouterOutputAgentWorkflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Apple revenue 2021'}\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla revenue 2021'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question 'Apple revenue 2021' is pointed in nature, asking for specific information about Apple's revenue in 2021. Therefore, using a relevant chunk as context is more appropriate than synthesizing an answer from an entire document.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 2, Reason: The question 'Tesla revenue 2021' is specific and pointed in nature, asking for a particular piece of information. Therefore, using a relevant chunk as context is more appropriate than synthesizing an answer from an entire document.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced no event\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: In 2021, Apple's revenue was $365.8 billion, while Tesla's revenue was $53.82 billion.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In 2021, Apple's revenue was $365.8 billion, while Tesla's revenue was $53.82 billion."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "response = await agent.run(message=\"Tell me the revenue for Apple and Tesla in 2021.\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Apple tailwinds 2021'}\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla tailwinds 2021'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 1, Reason: The question 'Apple tailwinds 2021' seems to require a general summary of the entire document for the year 2021. Since the question is about tailwinds, which could be a broad topic covering various aspects of Apple's business, using the entire document as context would be more appropriate.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 1, Reason: The question 'Tesla tailwinds 2021' seems to require a general summary of the entire document for the year 2021, which aligns with the purpose of choice 1. This choice is best used for higher-level summarization options and synthesizes an answer by feeding in an entire relevant document as context.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced no event\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: In 2021, Apple experienced several tailwinds that contributed to its growth:\n",
            "\n",
            "- Significant increase in net sales across all product and service categories, driven by strong demand for new iPhone models, Mac computers, iPads, and wearables.\n",
            "- Substantial growth in the services segment, particularly in advertising, the App Store, and cloud services.\n",
            "- Favorable foreign currency movements and improved financial leverage.\n",
            "- Expansion of its share repurchase program and increased dividends, reflecting strong financial health and confidence in future growth.\n",
            "\n",
            "For Tesla, the tailwinds in 2021 included:\n",
            "\n",
            "- Significant increase in vehicle production and deliveries, supported by ramping up production at Gigafactory Shanghai and the Fremont Factory.\n",
            "- Spike in demand in the automotive industry and ongoing electrification of the sector.\n",
            "- Increasing environmental awareness driving demand for electric vehicles.\n",
            "- Efforts to reduce costs and increase affordability through localized procurement and manufacturing, particularly in China.\n",
            "- Advancements in vehicle performance and functionality, including Full Self-Driving (FSD) capabilities.\n",
            "- Growth in the energy generation and storage segment, with increased deployments of Megapack, Powerwall, and Solar Roof products.\n",
            "\n",
            "These factors contributed to Tesla's strong financial performance, with total revenues increasing by 71% compared to the previous year.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In 2021, Apple experienced several tailwinds that contributed to its growth:\n",
              "\n",
              "- Significant increase in net sales across all product and service categories, driven by strong demand for new iPhone models, Mac computers, iPads, and wearables.\n",
              "- Substantial growth in the services segment, particularly in advertising, the App Store, and cloud services.\n",
              "- Favorable foreign currency movements and improved financial leverage.\n",
              "- Expansion of its share repurchase program and increased dividends, reflecting strong financial health and confidence in future growth.\n",
              "\n",
              "For Tesla, the tailwinds in 2021 included:\n",
              "\n",
              "- Significant increase in vehicle production and deliveries, supported by ramping up production at Gigafactory Shanghai and the Fremont Factory.\n",
              "- Spike in demand in the automotive industry and ongoing electrification of the sector.\n",
              "- Increasing environmental awareness driving demand for electric vehicles.\n",
              "- Efforts to reduce costs and increase affordability through localized procurement and manufacturing, particularly in China.\n",
              "- Advancements in vehicle performance and functionality, including Full Self-Driving (FSD) capabilities.\n",
              "- Growth in the energy generation and storage segment, with increased deployments of Megapack, Powerwall, and Solar Roof products.\n",
              "\n",
              "These factors contributed to Tesla's strong financial performance, with total revenues increasing by 71% compared to the previous year."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await agent.run(message=\"Tell me the tailwinds for Apple and Tesla in 2021.\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Apple performance 2019'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 1, Reason: The question 'Apple performance 2019' requires a general summary of the entire document, which aligns with the purpose of choice 1. It is best used for higher-level summarization options and synthesizes an answer by feeding in an entire relevant document as context.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: In 2019, Apple's overall performance was mixed. The company experienced a 2% decrease in total net sales compared to 2018, primarily due to lower iPhone sales. However, there was growth in other areas:\n",
            "\n",
            "- Increased sales in Wearables, Home and Accessories, and Services across all geographic segments.\n",
            "- Growth in Mac and iPad sales.\n",
            "- Significant growth in Services revenue.\n",
            "\n",
            "Apple also focused on shareholder returns, repurchasing $67.1 billion of its common stock and paying $14.1 billion in dividends. Geographically, the Americas segment saw an increase in net sales, while Europe, Greater China, and Japan experienced declines. Overall, 2019 was marked by a shift in product sales dynamics and continued investment in shareholder returns.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In 2019, Apple's overall performance was mixed. The company experienced a 2% decrease in total net sales compared to 2018, primarily due to lower iPhone sales. However, there was growth in other areas:\n",
              "\n",
              "- Increased sales in Wearables, Home and Accessories, and Services across all geographic segments.\n",
              "- Growth in Mac and iPad sales.\n",
              "- Significant growth in Services revenue.\n",
              "\n",
              "Apple also focused on shareholder returns, repurchasing $67.1 billion of its common stock and paying $14.1 billion in dividends. Geographically, the Americas segment saw an increase in net sales, while Europe, Greater China, and Japan experienced declines. Overall, 2019 was marked by a shift in product sales dynamics and continued investment in shareholder returns."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await agent.run(message=\"How was apple doing generally in 2019?\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Advanced] Setup Auto-Retrieval for Files\n",
        "\n",
        "We make our file-level retrieval more sophisticated by allowing the LLM to infer a set of metadata filters, based on some relevant example documents. This allows document-level retrieval to be more precise, since it allows the LLM to narrow down search results via metadata filters and not just top-k.\n",
        "\n",
        "We do some advanced things to make this happen\n",
        "- Define a custom prompt to generate metadata filters\n",
        "- Dynamically include few-shot examples of metadata as context to infer the set of metadata filters. These initial few-shot examples of metadata are obtained through vector search.\n",
        "\n",
        "We prompt the LLM to generate a prompt, a list of filters, and optionally a top-k value. We will define another workflow that is subclassed from the `RouterQueryWorkflow`. In this workflow, we will replace the `_query()` method defined in `RouterQueryWorkflow`.\n",
        "\n",
        "In this `_query()` method, we will check if the choice is the document-level retrieval. If it is, then we'll create a new query engine with certain LLM-generated filters applied. We'll return the response from this query engine.\n",
        "\n",
        "A lot of the code below is lifted from our **VectorIndexAutoRetriever** module, which provides an out of the box way to do auto-retrieval against a vector index.\n",
        "\n",
        "Since we are adding some customizations like adding few-shot examples, we re-use prompt pieces and implement auto-retrieval from scratch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.prompts import ChatPromptTemplate\n",
        "from llama_index.core.vector_stores.types import (\n",
        "    VectorStoreInfo,\n",
        "    VectorStoreQuerySpec,\n",
        "    MetadataInfo,\n",
        "    MetadataFilters,\n",
        ")\n",
        "\n",
        "SYS_PROMPT = \"\"\"\\\n",
        "Your goal is to structure the user's query to match the request schema provided below.\n",
        "\n",
        "<< Structured Request Schema >>\n",
        "When responding use a markdown code snippet with a JSON object formatted in the \\\n",
        "following schema:\n",
        "\n",
        "{schema_str}\n",
        "\n",
        "The query string should contain only text that is expected to match the contents of \\\n",
        "documents. Any conditions in the filter should not be mentioned in the query as well.\n",
        "\n",
        "Make sure that filters only refer to attributes that exist in the data source.\n",
        "Make sure that filters take into account the descriptions of attributes.\n",
        "Make sure that filters are only used as needed. If there are no filters that should be \\\n",
        "applied return [] for the filter value.\\\n",
        "\n",
        "If the user's query explicitly mentions number of documents to retrieve, set top_k to \\\n",
        "that number, otherwise do not set top_k.\n",
        "\n",
        "The schema of the metadata filters in the vector db table is listed below, along with some example metadata dictionaries from relevant rows.\n",
        "The user will send the input query string.\n",
        "\n",
        "Data Source:\n",
        "```json\n",
        "{info_str}\n",
        "```\n",
        "\n",
        "Example metadata from relevant chunks:\n",
        "{example_rows}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class AutoRetrievalRouterQueryWorkflow(RouterQueryWorkflow):\n",
        "    \"\"\"Router query engine with auto retrieval.\"\"\"\n",
        "\n",
        "    async def _get_auto_retriever_query_engine(\n",
        "        self, query: str, verbose: bool = False\n",
        "    ) -> RetrieverQueryEngine:\n",
        "        \"\"\"Gets auto doc retriever query engine\"\"\"\n",
        "\n",
        "        # retriever that retrieves example rows\n",
        "        example_rows_retriever = index.as_retriever(\n",
        "            retrieval_mode=\"chunks\", rerank_top_n=4\n",
        "        )\n",
        "\n",
        "        def get_example_rows_fn(**kwargs):\n",
        "            \"\"\"Retrieve relevant few-shot examples of metadata.\"\"\"\n",
        "\n",
        "            query_str = kwargs[\"query_str\"]\n",
        "            nodes = example_rows_retriever.retrieve(query_str)\n",
        "            # get the metadata, join them\n",
        "            metadata_list = [n.metadata for n in nodes]\n",
        "\n",
        "            return \"\\n\".join([json.dumps(m) for m in metadata_list])\n",
        "\n",
        "        # define chat prompt template to feed into LLM\n",
        "        chat_prompt_tmpl = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\"system\", SYS_PROMPT),\n",
        "                (\"user\", \"{query_str}\"),\n",
        "            ],\n",
        "            function_mappings={\"example_rows\": get_example_rows_fn},\n",
        "        )\n",
        "\n",
        "        # information about vector store - used to generate json schema in prompt template\n",
        "        vector_store_info = VectorStoreInfo(\n",
        "            content_info=\"document chunks around Apple and Tesla 10K documents\",\n",
        "            metadata_info=[\n",
        "                MetadataInfo(\n",
        "                    name=\"file_name\",\n",
        "                    type=\"str\",\n",
        "                    description=\"Name of the source file\",\n",
        "                ),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        query_spec: VectorStoreQuerySpec = await llm.astructured_predict(\n",
        "            VectorStoreQuerySpec,\n",
        "            chat_prompt_tmpl,\n",
        "            info_str=vector_store_info.model_dump_json(),\n",
        "            schema_str=json.dumps(VectorStoreQuerySpec.model_json_schema()),\n",
        "            query_str=query,\n",
        "        )\n",
        "\n",
        "        # build retriever and query engine\n",
        "        filters = (\n",
        "            MetadataFilters(filters=query_spec.filters)\n",
        "            if len(query_spec.filters) > 0\n",
        "            else None\n",
        "        )\n",
        "        if verbose:\n",
        "            print(f\"> Using query str: {query_spec.query}\")\n",
        "\n",
        "        if filters and verbose:\n",
        "            print(f\"> Using filters{filters.json()}\")\n",
        "\n",
        "        retriever = index.as_retriever(\n",
        "            retrieval_mode=\"files_via_content\", files_top_k=1, filters=filters\n",
        "        )\n",
        "\n",
        "        query_engine = RetrieverQueryEngine.from_args(\n",
        "            retriever, llm=self.llm, response_mode=\"tree_summarize\"\n",
        "        )\n",
        "\n",
        "        return query_engine\n",
        "\n",
        "    async def _query(self, query_str: str, choice_idx: int):\n",
        "        \"\"\"Query with auto retriever\"\"\"\n",
        "\n",
        "        if choice_idx == 0:\n",
        "            query_engine = await self._get_auto_retriever_query_engine(\n",
        "                query_str, self._verbose\n",
        "            )\n",
        "        else:\n",
        "            query_engine = self.query_engines[choice_idx]\n",
        "        return await query_engine.aquery(query_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the auto retrieval query workflow, then wrap it around a RouterQueryEngine, then create a tool around that engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# auto retrieval query engine\n",
        "auto_retrieval_query_workflow = AutoRetrievalRouterQueryWorkflow(\n",
        "    query_engines=[query_engine_doc, query_engine_chunk],\n",
        "    choice_descriptions=[TOOL_DOC_DESC, TOOL_CHUNK_DESC],\n",
        "    verbose=True,\n",
        "    llm=llm,\n",
        "    router_prompt=ROUTER_PROMPT,\n",
        "    timeout=120\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create an agent using auto retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agent\n",
        "agent_router_output = RouterOutputAgentWorkflow(auto_retrieval_query_workflow, verbose=True, timeout=120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step prepare_chat\n",
            "Step prepare_chat produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: None\n",
            "Step chat produced event GatherToolsEvent\n",
            "Running step dispatch_calls\n",
            "Step dispatch_calls produced no event\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla 2021 performance'}\n",
            "Running step call_tool\n",
            "Calling function query_workflow with msg {'query_str': 'Tesla 2022 performance'}\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 1, Reason: The question 'Tesla 2021 performance' seems to require a general summary of Tesla's performance for the year 2021. Since the question is asking for a higher-level summarization of Tesla's performance, it is best to use the option that synthesizes an answer by feeding in an entire relevant document as context. This aligns with the description of choice 1, which is suitable for higher-level summarization options.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "Running step choose_query_engine\n",
            "Selected choice(s):\n",
            "Choice: 1, Reason: The question 'Tesla 2022 performance' seems to require a general summary of the entire document for Tesla in 2022. Since the document represents a complete 10K report for a given year, using the entire document as context would provide a higher-level summarization of Tesla's performance in 2022.\n",
            "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
            "Running step query_each_engine\n",
            "> Using query str: Tesla 2021 performance\n",
            "> Using filters{\"filters\":[{\"key\":\"file_name\",\"value\":\"tesla_2021.pdf\",\"operator\":\"==\"}],\"condition\":\"and\"}\n",
            "> Using query str: Tesla 2022 performance\n",
            "> Using filters{\"filters\":[{\"key\":\"file_name\",\"value\":\"tesla_2022.pdf\",\"operator\":\"==\"}],\"condition\":\"and\"}\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced no event\n",
            "Step query_each_engine produced event SynthesizeAnswersEvent\n",
            "Running step synthesize_response\n",
            "Step synthesize_response produced event StopEvent\n",
            "Step call_tool produced event ToolCallEventResult\n",
            "Running step gather\n",
            "Step gather produced event InputEvent\n",
            "Running step chat\n",
            "Chat message: In 2021, Tesla had a strong year with significant growth and achievements:\n",
            "\n",
            "- **Production and Deliveries**: Tesla produced 930,422 vehicles and delivered 936,222 vehicles.\n",
            "- **Financial Performance**: The company recognized total revenues of $53.82 billion, a 71% increase compared to the previous year, and reported a net income of $5.52 billion, a favorable change of $4.80 billion from the prior year.\n",
            "- **Cash and Investments**: Tesla ended the year with $17.58 billion in cash and cash equivalents.\n",
            "- **Energy Products**: They deployed 3.99 GWh of energy storage products and 345 megawatts of solar energy systems.\n",
            "- **Focus Areas**: Tesla focused on increasing vehicle production and capacity, improving battery technologies, enhancing Full Self-Driving (FSD) capabilities, and expanding global infrastructure.\n",
            "\n",
            "In 2022, Tesla continued its growth trajectory despite challenges:\n",
            "\n",
            "- **Production and Deliveries**: Tesla produced 1,369,611 consumer vehicles and delivered 1,313,851 vehicles, overcoming supply chain and logistics challenges.\n",
            "- **Financial Performance**: Total revenues reached $81.46 billion, marking an increase of $27.64 billion from the previous year, and the net income was $12.56 billion, a favorable change of $7.04 billion compared to the prior year.\n",
            "- **Cash and Investments**: The company ended the year with $22.19 billion in cash and cash equivalents and investments, an increase of $4.48 billion from the end of 2021.\n",
            "- **Energy Products**: They deployed 6.5 GWh of energy storage products and 348 megawatts of solar energy systems.\n",
            "- **Focus Areas**: Tesla focused on increasing production and delivery capabilities, improving battery technologies, enhancing Full Self-Driving capabilities, increasing vehicle affordability and efficiency, bringing new products to market, and expanding global infrastructure.\n",
            "\n",
            "Overall, Tesla showed strong performance and growth in both years, with significant increases in production, deliveries, and financial metrics.\n",
            "Step chat produced event StopEvent\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In 2021, Tesla had a strong year with significant growth and achievements:\n",
              "\n",
              "- **Production and Deliveries**: Tesla produced 930,422 vehicles and delivered 936,222 vehicles.\n",
              "- **Financial Performance**: The company recognized total revenues of $53.82 billion, a 71% increase compared to the previous year, and reported a net income of $5.52 billion, a favorable change of $4.80 billion from the prior year.\n",
              "- **Cash and Investments**: Tesla ended the year with $17.58 billion in cash and cash equivalents.\n",
              "- **Energy Products**: They deployed 3.99 GWh of energy storage products and 345 megawatts of solar energy systems.\n",
              "- **Focus Areas**: Tesla focused on increasing vehicle production and capacity, improving battery technologies, enhancing Full Self-Driving (FSD) capabilities, and expanding global infrastructure.\n",
              "\n",
              "In 2022, Tesla continued its growth trajectory despite challenges:\n",
              "\n",
              "- **Production and Deliveries**: Tesla produced 1,369,611 consumer vehicles and delivered 1,313,851 vehicles, overcoming supply chain and logistics challenges.\n",
              "- **Financial Performance**: Total revenues reached $81.46 billion, marking an increase of $27.64 billion from the previous year, and the net income was $12.56 billion, a favorable change of $7.04 billion compared to the prior year.\n",
              "- **Cash and Investments**: The company ended the year with $22.19 billion in cash and cash equivalents and investments, an increase of $4.48 billion from the end of 2021.\n",
              "- **Energy Products**: They deployed 6.5 GWh of energy storage products and 348 megawatts of solar energy systems.\n",
              "- **Focus Areas**: Tesla focused on increasing production and delivery capabilities, improving battery technologies, enhancing Full Self-Driving capabilities, increasing vehicle affordability and efficiency, bringing new products to market, and expanding global infrastructure.\n",
              "\n",
              "Overall, Tesla showed strong performance and growth in both years, with significant increases in production, deliveries, and financial metrics."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await agent_router_output.run(message=\"How was Tesla doing generally in 2021 and 2022?\")\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llamacloud-demo",
      "language": "python",
      "name": "llamacloud-demo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}