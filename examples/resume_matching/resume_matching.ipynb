{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ⚠️ Important Notice\n\nThis notebook (and repository) is deprecated.\n\nFor the latest python examples, please refer to the `llama-cloud-services` repository examples: \nhttps://github.com/run-llama/llama_cloud_services/tree/main/examples\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86XWvjxeZV8I"
      },
      "source": [
        "# LLM-Native Resume Matching Solution\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/resume_matching/resume_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook demonstrates the implementation of an LLM-native resume matching solution that transforms traditional resume screening into an AI-powered, conversational experience. This aims to streamline the recruitment process by automating candidate matching and providing natural language interaction for recruiters.\n",
        "\n",
        "## Use Case Overview\n",
        "- **Problem**: Traditional resume screening relies heavily on manual filter selection and explicit matching criteria, making it inefficient and time-consuming for recruiters.\n",
        "- **Solution**: An LLM-native approach that uses generative AI to:\n",
        "  - Extract structured information from resumes automatically\n",
        "  - Enable natural language queries for candidate search\n",
        "  - Provide matching between job descriptions and candidates\n",
        "  - Offer detailed analysis of why candidates match specific roles\n",
        "\n",
        "## Implementation Steps\n",
        "1. **Data Processing**\n",
        "   - Parse PDF resumes using LlamaParse\n",
        "   - Extract structured metadata (skills, education, domain) using LLMs\n",
        "   - Store processed documents in LlamaCloud for efficient retrieval\n",
        "\n",
        "2. **Index Creation**\n",
        "   - Create a Pipeline/ Index using LlamaCloud\n",
        "   - Configure embedding and transformation settings\n",
        "   - Upload processed documents with metadata\n",
        "\n",
        "3. **Query Processing**\n",
        "   - Support two types of queries:\n",
        "     - Natural language queries from recruiters (e.g., \"Find Java developers from US universities\")\n",
        "     - Job description-based matching\n",
        "   - Extract relevant metadata filters from queries using LLMs\n",
        "   - Retrieve matching candidates based on metadata and semantic search\n",
        "\n",
        "4. **Candidate Analysis**\n",
        "   - Generate detailed analysis of why candidates match job requirements\n",
        "   - Compare candidate qualifications against job criteria\n",
        "   - Provide insights into strengths and potential gaps.\n",
        "\n",
        "\n",
        "**NOTE**: For this demonstration, I have used a sample dataset consisting of 30 resumes (10 each from Information Technology, Sales, and Finance domains) sourced from the [Kaggle Resume Dataset](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset). This smaller dataset allows for easier experimentation and clearer demonstration of the concept.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqSD3115Uoep"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Here we install `llama-index`, `llama-index-indices-managed-llama-cloud`, `llama-parse` and `llama-cloud`. \n",
        "\n",
        "These packages are tools for building, parsing, and managing LLM applications on LlamaIndex's cloud platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tbgPfxqILy5e",
        "outputId": "a28f1b7e-1ce5-45b5-c75e-080d15b70238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.23-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-parse\n",
            "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud\n",
            "  Downloading llama_cloud-0.1.5-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.23 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.3.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "INFO: pip is looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-indices-managed-llama-cloud\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from llama-parse) (8.1.7)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cloud) (0.27.2)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud) (2.9.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->llama-cloud) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud) (0.14.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.54.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10.10)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (11.0.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-readers-llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud) (2.23.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (2.6)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.7.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->llama-cloud) (1.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.2.0)\n",
            "Downloading llama_index-0.11.23-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl (11 kB)\n",
            "Downloading llama_parse-0.5.14-py3-none-any.whl (13 kB)\n",
            "Downloading llama_cloud-0.1.5-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.11.23-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.3.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, tenacity, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.5 llama-index-0.11.23 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.23 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.6.0 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.3.0 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.14 marshmallow-3.23.1 mypy-extensions-1.0.0 pypdf-5.1.0 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U llama-index llama-index-indices-managed-llama-cloud llama-parse llama-cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr0EAvWqMCx5"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abREdlc9Us-5"
      },
      "source": [
        "## Setup API Keys\n",
        "\n",
        "We will utilize `gpt-4o-mini` from OpenAI's LLM and our LlamaCloud, an enterprise platform designed for building LLM applications.\n",
        "\n",
        "Here, we will set up the `OPENAI_API_KEY` and `LLAMA_CLOUD_API_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG3WZeDYMF8h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR OPENAI API KEY>\" # Get your API key from https://platform.openai.com/account/api-keys\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"<YOUR LLAMA CLOUD API KEY>\" # Get your API key from https://cloud.llamaindex.ai/api-key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeVZhD86UvXc"
      },
      "source": [
        "## Setup LLM\n",
        "\n",
        "We will initialize `gpt-4o-mini` OpenAI LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI-rcTFWMHTk"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model='gpt-4o-mini')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Files\n",
        "\n",
        "We will download sampled data from [Kaggle Resume Dataset](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset) and `job_description.pdf`.\n",
        "\n",
        "`sampled_dataset` - contains 10 each from Information Technology, Sales, and Finance domains.\n",
        "\n",
        "`job_description.pdf` - This is the job description file we will use to retrieve candidate profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-19 19:59:40--  https://www.dropbox.com/scl/fo/v1mn1rxqz2ifqtx009owh/APHC7xPTQ7BiRZv0BKZ7cag?rlkey=rh09o73172vzifjqlsmw4fhmo&st=v220giff&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6031:18::a27d:5112, 162.125.81.18\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6031:18::a27d:5112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc12a6b60dc633d4a25a40da5fd0.dl.dropboxusercontent.com/zip_download_get/CCFRPlgNx6gNHc00HuNV-PYr0K9CFSmYyikQMpy3kkMXsfIoryxQntvNhR0CaAHZgfp2Yp4Y4VOBjuJ-aFEjIrDzAP1CbFlX9ZPMwLAKsuIf2A# [following]\n",
            "--2024-11-19 19:59:42--  https://uc12a6b60dc633d4a25a40da5fd0.dl.dropboxusercontent.com/zip_download_get/CCFRPlgNx6gNHc00HuNV-PYr0K9CFSmYyikQMpy3kkMXsfIoryxQntvNhR0CaAHZgfp2Yp4Y4VOBjuJ-aFEjIrDzAP1CbFlX9ZPMwLAKsuIf2A\n",
            "Resolving uc12a6b60dc633d4a25a40da5fd0.dl.dropboxusercontent.com (uc12a6b60dc633d4a25a40da5fd0.dl.dropboxusercontent.com)... 2620:100:6031:15::a27d:510f, 162.125.81.15\n",
            "Connecting to uc12a6b60dc633d4a25a40da5fd0.dl.dropboxusercontent.com (uc12a6b60dc633d4a25a40da5fd0.dl.dropboxusercontent.com)|2620:100:6031:15::a27d:510f|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782926 (765K) [application/zip]\n",
            "Saving to: ‘sampled_data.zip’\n",
            "\n",
            "sampled_data.zip    100%[===================>] 764.58K  2.02MB/s    in 0.4s    \n",
            "\n",
            "2024-11-19 19:59:43 (2.02 MB/s) - ‘sampled_data.zip’ saved [782926/782926]\n",
            "\n",
            "Archive:  sampled_data.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            "   creating: ./sampled_data/IT/\n",
            "   creating: ./sampled_data/SALES/\n",
            "   creating: ./sampled_data/FINANCE/\n",
            " extracting: ./sampled_data/IT/52246737.pdf  \n",
            " extracting: ./sampled_data/IT/16899268.pdf  \n",
            " extracting: ./sampled_data/IT/17641670.pdf  \n",
            " extracting: ./sampled_data/IT/23527321.pdf  \n",
            " extracting: ./sampled_data/IT/25959103.pdf  \n",
            " extracting: ./sampled_data/IT/52618188.pdf  \n",
            " extracting: ./sampled_data/IT/18159866.pdf  \n",
            " extracting: ./sampled_data/IT/27536013.pdf  \n",
            " extracting: ./sampled_data/IT/38753827.pdf  \n",
            " extracting: ./sampled_data/IT/57002858.pdf  \n",
            " extracting: ./sampled_data/SALES/12696104.pdf  \n",
            " extracting: ./sampled_data/SALES/17509935.pdf  \n",
            " extracting: ./sampled_data/SALES/17704246.pdf  \n",
            " extracting: ./sampled_data/SALES/19473948.pdf  \n",
            " extracting: ./sampled_data/SALES/30608780.pdf  \n",
            " extracting: ./sampled_data/SALES/55097118.pdf  \n",
            " extracting: ./sampled_data/SALES/25315791.pdf  \n",
            " extracting: ./sampled_data/SALES/28198029.pdf  \n",
            " extracting: ./sampled_data/SALES/31199035.pdf  \n",
            " extracting: ./sampled_data/SALES/33236701.pdf  \n",
            " extracting: ./sampled_data/FINANCE/38907798.pdf  \n",
            " extracting: ./sampled_data/FINANCE/12071138.pdf  \n",
            " extracting: ./sampled_data/FINANCE/14408510.pdf  \n",
            " extracting: ./sampled_data/FINANCE/15891494.pdf  \n",
            " extracting: ./sampled_data/FINANCE/24967652.pdf  \n",
            " extracting: ./sampled_data/FINANCE/25101183.pdf  \n",
            " extracting: ./sampled_data/FINANCE/26961846.pdf  \n",
            " extracting: ./sampled_data/FINANCE/28398216.pdf  \n",
            " extracting: ./sampled_data/FINANCE/84373843.pdf  \n",
            " extracting: ./sampled_data/FINANCE/19540089.pdf  \n",
            "--2024-11-19 19:59:43--  https://www.dropbox.com/scl/fi/b1djiczj6vy8s6h4isvmr/job_description.pdf?rlkey=drpkd2exj8edkuw1f0evhvqfx&st=2i2wb801&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6031:18::a27d:5112, 162.125.81.18\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6031:18::a27d:5112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com/cd/0/inline/Ceqv4x5NQv0ToPYfwBKfOBmq4nETcYQwun0Mh9Ys-hvE-oKZnLMM18dLQvpAb44OrwSGlgWllQ8lEmXj4V1fgTJxZAAQ4hpb-nfpliXiVjO-Pwbg50aZ-M6fIdGh5jITr8jO-7LaNxyeJ6VRXlqgpsAr/file?dl=1# [following]\n",
            "--2024-11-19 19:59:45--  https://uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com/cd/0/inline/Ceqv4x5NQv0ToPYfwBKfOBmq4nETcYQwun0Mh9Ys-hvE-oKZnLMM18dLQvpAb44OrwSGlgWllQ8lEmXj4V1fgTJxZAAQ4hpb-nfpliXiVjO-Pwbg50aZ-M6fIdGh5jITr8jO-7LaNxyeJ6VRXlqgpsAr/file?dl=1\n",
            "Resolving uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com (uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com)... 2620:100:6031:15::a27d:510f, 162.125.81.15\n",
            "Connecting to uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com (uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com)|2620:100:6031:15::a27d:510f|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CerGZ9rmjfYhAnZyapyxgni1ITCJh4jxciJjsQ0Yqxl2Cmr0BObRM8EkvUFsoSsYkgiuxKD8l-wwwygM0mFvt-9mErj-conp_ngWJN1B7OLUl7KqdpELCpMN7D2X7D7LTWe4ghEuYxlm3rc73JlQ0VwkLp7tH--0LTZjDt01S6mNQdyrkRobp111ykC0COlH5lZWxfkENZgOH9YeNzWgNCsAnvYq9l1zodjVYAtstoVLGPd4HLv37X3W9WWFBQAXdY_SANZNvC2q6AkF0sHCSrldpyhNhKw7o5YsszwuWmcY6qhonBUNd0hTmhh1snCP-nLY9uHV8jIYE7N1Pnap1b6nZmxBzuIJx1M-zRA-b5D5PwiKaBkC5uxH4x-lZd_mRLo/file?dl=1 [following]\n",
            "--2024-11-19 19:59:46--  https://uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com/cd/0/inline2/CerGZ9rmjfYhAnZyapyxgni1ITCJh4jxciJjsQ0Yqxl2Cmr0BObRM8EkvUFsoSsYkgiuxKD8l-wwwygM0mFvt-9mErj-conp_ngWJN1B7OLUl7KqdpELCpMN7D2X7D7LTWe4ghEuYxlm3rc73JlQ0VwkLp7tH--0LTZjDt01S6mNQdyrkRobp111ykC0COlH5lZWxfkENZgOH9YeNzWgNCsAnvYq9l1zodjVYAtstoVLGPd4HLv37X3W9WWFBQAXdY_SANZNvC2q6AkF0sHCSrldpyhNhKw7o5YsszwuWmcY6qhonBUNd0hTmhh1snCP-nLY9uHV8jIYE7N1Pnap1b6nZmxBzuIJx1M-zRA-b5D5PwiKaBkC5uxH4x-lZd_mRLo/file?dl=1\n",
            "Reusing existing connection to [uc5606711e2b9fa90c9074c83e83.dl.dropboxusercontent.com]:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20862 (20K) [application/binary]\n",
            "Saving to: ‘job_description.pdf’\n",
            "\n",
            "job_description.pdf 100%[===================>]  20.37K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-19 19:59:47 (140 KB/s) - ‘job_description.pdf’ saved [20862/20862]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the sampled data\n",
        "!wget --content-disposition \"https://www.dropbox.com/scl/fo/v1mn1rxqz2ifqtx009owh/APHC7xPTQ7BiRZv0BKZ7cag?rlkey=rh09o73172vzifjqlsmw4fhmo&st=v220giff&dl=1\"\n",
        "\n",
        "# make a directory to store the data\n",
        "!mkdir -p \"./sampled_data\"\n",
        "\n",
        "# unzip the data\n",
        "!unzip sampled_data.zip -d \"./sampled_data\"\n",
        "\n",
        "# Download the job description file\n",
        "!wget -O job_description.pdf \"https://www.dropbox.com/scl/fi/b1djiczj6vy8s6h4isvmr/job_description.pdf?rlkey=drpkd2exj8edkuw1f0evhvqfx&st=2i2wb801&dl=1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHwuhLUBUxkj"
      },
      "source": [
        "## Utils\n",
        "\n",
        "Here we define some functions for further processing.\n",
        "\n",
        "1. `parse_files`: Processes PDF files using LlamaParse and converts them to markdown format with updated metadata\n",
        "\n",
        "2. `list_pdf_files`: Recursively finds all PDF files in a directory and its subdirectories\n",
        "\n",
        "3. `Metadata`: Pydantic model to structure resume metadata including domain, skills, and educational country information.\n",
        "\n",
        "4. `create_llamacloud_pipeline`: Creates or updates a LlamaCloud pipeline with specified configurations.\n",
        "\n",
        "5. `get_metadata`: Extracts structured metadata from resume text using an LLM.\n",
        "\n",
        "6. `get_document_upload`: Prepares a document for cloud upload by combining text and extracted metadata.\n",
        "\n",
        "7. `upload_documents`: Batch uploads documents to LlamaCloud pipeline with parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-pM24SBMNse"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "from pathlib import Path\n",
        "from llama_index.core import Document\n",
        "from llama_cloud.types import CloudDocumentCreate\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from llama_cloud.client import LlamaCloud\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.async_utils import run_jobs\n",
        "\n",
        "def parse_files(pdf_files):\n",
        "    \"\"\"Function to parse the pdf files using LlamaParse in markdown format\"\"\"\n",
        "\n",
        "    parser = LlamaParse(\n",
        "        result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
        "        num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    for index, pdf_file in enumerate(pdf_files):\n",
        "        print(f\"Processing file {index + 1}/{len(pdf_files)}: {pdf_file}\")\n",
        "        docs = parser.load_data(pdf_file)\n",
        "        # Updating metadata with filepath\n",
        "        for doc in docs:\n",
        "          doc.metadata.update({'filepath': pdf_file})\n",
        "        documents.append(docs)\n",
        "\n",
        "    return documents\n",
        "\n",
        "def list_pdf_files(directory):\n",
        "    # List all .pdf files recursively using pathlib\n",
        "    # rglob ('recursive glob') searches through all subdirectories\n",
        "    pdf_files = [str(file) for file in Path(directory).rglob('*.pdf')]\n",
        "    return pdf_files\n",
        "\n",
        "class Metadata(BaseModel):\n",
        "    \"\"\"\n",
        "    A data model representing key professional and educational metadata extracted from a resume.\n",
        "    This class captures essential candidate information including technical/professional skills\n",
        "    and the geographical distribution of their educational background.\n",
        "\n",
        "    Attributes:\n",
        "        skills (List[str]): Technical and professional competencies of the candidate\n",
        "        country (List[str]): Countries where the candidate pursued formal education\n",
        "\n",
        "    Example:\n",
        "        {\n",
        "            \"skills\": [\"Python\", \"Machine Learning\", \"SQL\", \"Project Management\"],\n",
        "            \"country\": [\"United States\", \"India\"],\n",
        "            \"domain\": \"Information Technology\"\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    domain: str = Field(...,\n",
        "                        description=\"The domain of the candidate can be one of SALES/ IT/ FINANCE\"\n",
        "                                    \"Returns an empty string if no domain is identified.\")\n",
        "\n",
        "    skills: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of technical, professional, and soft skills extracted from the resume. \"\n",
        "                   \"and domain expertise. Returns an empty list if no skills are identified.\"\n",
        "    )\n",
        "\n",
        "    country: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"List of countries where the candidate completed their formal education, Only extract the country.\"\n",
        "                   \"Returns an empty list if countries are not specified.\"\n",
        "    )\n",
        "\n",
        "def create_llamacloud_pipeline(pipeline_name, embedding_config, transform_config, data_sink_id=None):\n",
        "    \"\"\"Function to create a pipeline in llamacloud\"\"\"\n",
        "\n",
        "    client = LlamaCloud(token=os.environ[\"LLAMA_CLOUD_API_KEY\"])\n",
        "\n",
        "    pipeline = {\n",
        "        'name': pipeline_name,\n",
        "        'transform_config': transform_config,\n",
        "        'embedding_config': embedding_config,\n",
        "        'data_sink_id': data_sink_id\n",
        "    }\n",
        "\n",
        "    pipeline = client.pipelines.upsert_pipeline(request=pipeline)\n",
        "\n",
        "    return client, pipeline\n",
        "\n",
        "async def get_metadata(text):\n",
        "    \"\"\"Function to get the metadata from the given resume of the candidate\"\"\"\n",
        "    prompt_template = PromptTemplate(\"\"\"Generate skills, and country of the education for the given candidate resume.\n",
        "\n",
        "    Resume of the candidate:\n",
        "\n",
        "    {text}\"\"\")\n",
        "\n",
        "    metadata = await llm.astructured_predict(\n",
        "        Metadata,\n",
        "        prompt_template,\n",
        "        text=text,\n",
        "    )\n",
        "\n",
        "    return metadata\n",
        "\n",
        "async def get_document_upload(documents, llm):\n",
        "    full_text = \"\\n\\n\".join([doc.text for doc in documents])\n",
        "\n",
        "    # Get the file path of the resume\n",
        "    file_path = documents[0].metadata['filepath']\n",
        "\n",
        "    # Extract metadata from the resume\n",
        "    extracted_metadata = await get_metadata(full_text)\n",
        "\n",
        "    skills = list(set(getattr(extracted_metadata, 'skills', [])))\n",
        "    country = list(set(getattr(extracted_metadata, 'country', [])))\n",
        "    domain = getattr(extracted_metadata, 'domain', '')\n",
        "\n",
        "    global_skills.extend(skills)\n",
        "    global_countries.extend(country)\n",
        "    global_domains.append(domain)\n",
        "\n",
        "    return CloudDocumentCreate(\n",
        "                text=full_text,\n",
        "                metadata={\n",
        "                    'skills': skills,\n",
        "                    'country': country,\n",
        "                    'domain': domain,\n",
        "                    'file_path': file_path\n",
        "                }\n",
        "            )\n",
        "\n",
        "async def upload_documents(client, pipeline, documents):\n",
        "    \"\"\"Function to upload the documents to the cloud\"\"\"\n",
        "\n",
        "    # Upload the documents to the cloud\n",
        "    extract_jobs = []\n",
        "    for doc in documents:\n",
        "        extract_jobs.append(get_document_upload(doc, llm))\n",
        "\n",
        "    documents_upload_objs = await run_jobs(extract_jobs, workers=4)\n",
        "\n",
        "    _ = client.pipelines.create_batch_pipeline_documents(pipeline.id, request=documents_upload_objs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSaUjbxuVrbI"
      },
      "source": [
        "## Parse the files\n",
        "\n",
        "Here, we get a list of files from the `sampled_data` directory and parse them using `LlamaParse`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQsRIXOkMZKf",
        "outputId": "a8419c5e-25b4-45fc-ecd5-19d7e73b06a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1/30: sampled_data/SALES/31199035.pdf\n",
            "Started parsing the file under job_id cc9ca080-7579-44e7-b099-acafd625858a\n",
            "Processing file 2/30: sampled_data/SALES/17509935.pdf\n",
            "Started parsing the file under job_id e6f6b9a3-226c-48e7-b7e9-0e172c5c0a12\n",
            "Processing file 3/30: sampled_data/SALES/12696104.pdf\n",
            "Started parsing the file under job_id a557e4db-90b8-4e0f-84bd-c596a125c43b\n",
            "Processing file 4/30: sampled_data/SALES/28198029.pdf\n",
            "Started parsing the file under job_id d36574e9-b757-42e3-9ed2-f3578d49cf5e\n",
            "Processing file 5/30: sampled_data/SALES/33236701.pdf\n",
            "Started parsing the file under job_id 8758ad69-ce5c-4953-8faa-8d6f377605a7\n",
            "Processing file 6/30: sampled_data/SALES/30608780.pdf\n",
            "Started parsing the file under job_id 21354951-ba5b-4202-8eac-b8790e0e584f\n",
            ".Processing file 7/30: sampled_data/SALES/19473948.pdf\n",
            "Started parsing the file under job_id 56660d5c-faea-4fa5-8b2b-f97c92d8d128\n",
            "Processing file 8/30: sampled_data/SALES/55097118.pdf\n",
            "Started parsing the file under job_id 5057955c-8e7c-40df-97db-53bcee4a5424\n",
            "Processing file 9/30: sampled_data/SALES/17704246.pdf\n",
            "Started parsing the file under job_id 9ac7d873-b7ac-4df5-bcef-f9963578f12b\n",
            "Processing file 10/30: sampled_data/SALES/25315791.pdf\n",
            "Started parsing the file under job_id 9d80cd1b-abec-48f6-adc4-14d728de5e31\n",
            "Processing file 11/30: sampled_data/IT/38753827.pdf\n",
            "Started parsing the file under job_id a07ad73c-4cb3-4275-84e9-d3121bc7f309\n",
            "Processing file 12/30: sampled_data/IT/27536013.pdf\n",
            "Started parsing the file under job_id cfda640d-0db9-4b59-8a57-7b5a5590d661\n",
            "Processing file 13/30: sampled_data/IT/17641670.pdf\n",
            "Started parsing the file under job_id 453ed329-e965-4077-9100-dd8acb48d81b\n",
            "Processing file 14/30: sampled_data/IT/52618188.pdf\n",
            "Started parsing the file under job_id 71b21832-5a57-4bff-806f-c1e9aa22aa5f\n",
            "Processing file 15/30: sampled_data/IT/23527321.pdf\n",
            "Started parsing the file under job_id c176028d-4a57-48f9-b564-44612f204e45\n",
            "Processing file 16/30: sampled_data/IT/16899268.pdf\n",
            "Started parsing the file under job_id ba68b0c4-6cfd-495c-ac54-abcb4362006c\n",
            "Processing file 17/30: sampled_data/IT/52246737.pdf\n",
            "Started parsing the file under job_id 6ba28943-59e9-48fb-ac03-2a2c2a51ebaf\n",
            "Processing file 18/30: sampled_data/IT/18159866.pdf\n",
            "Started parsing the file under job_id ddea8d61-950c-4979-8e60-b80963bada3c\n",
            "Processing file 19/30: sampled_data/IT/25959103.pdf\n",
            "Started parsing the file under job_id c4b41ed4-48bd-49b2-b0c7-b200f7d2610a\n",
            "Processing file 20/30: sampled_data/IT/57002858.pdf\n",
            "Started parsing the file under job_id 3804f986-33da-47c6-9bf3-b5ac44897c02\n",
            "Processing file 21/30: sampled_data/FINANCE/12071138.pdf\n",
            "Started parsing the file under job_id 65f840f3-eaf7-43e3-8be6-6eaa272c3afa\n",
            "Processing file 22/30: sampled_data/FINANCE/25101183.pdf\n",
            "Started parsing the file under job_id b54cbb60-2683-4d02-a186-5a6184b717d4\n",
            "Processing file 23/30: sampled_data/FINANCE/28398216.pdf\n",
            "Started parsing the file under job_id 7eeab9da-da5d-40b1-85b6-d3b8ba6d3d44\n",
            "Processing file 24/30: sampled_data/FINANCE/38907798.pdf\n",
            "Started parsing the file under job_id 94358957-059a-47a3-b29c-2a01a1d0cc1b\n",
            "Processing file 25/30: sampled_data/FINANCE/19540089.pdf\n",
            "Started parsing the file under job_id fbae8274-f4d8-4025-93b9-085a42f5227a\n",
            "Processing file 26/30: sampled_data/FINANCE/26961846.pdf\n",
            "Started parsing the file under job_id 1a98c14b-1ee9-4246-8961-6802db2e5c37\n",
            "Processing file 27/30: sampled_data/FINANCE/14408510.pdf\n",
            "Started parsing the file under job_id 7b9ea375-41f6-4715-ba63-eadfdff38b75\n",
            "Processing file 28/30: sampled_data/FINANCE/24967652.pdf\n",
            "Started parsing the file under job_id a0b87e0c-a01b-47d1-97fa-ebc1d050def6\n",
            "Processing file 29/30: sampled_data/FINANCE/15891494.pdf\n",
            "Started parsing the file under job_id cf755430-11d3-461e-b328-75e1d000f6ec\n",
            "Processing file 30/30: sampled_data/FINANCE/84373843.pdf\n",
            "Started parsing the file under job_id e726caf8-24a6-46b2-b440-f7595b17cfad\n"
          ]
        }
      ],
      "source": [
        "directory = './sampled_data/'\n",
        "pdf_files = list_pdf_files(directory)\n",
        "\n",
        "documents = parse_files(pdf_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_gtZk0ZVtrS"
      },
      "source": [
        "## Let's keep a track of skills, countries and domains.\n",
        "\n",
        "We will track `skills`, `countries`, and `domains` in each parsed resume.\n",
        "\n",
        "Here, we will initialize lists for `global_skills`, `global_countries`, and `global_domains` to monitor these attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvdY7FLeMbL9"
      },
      "outputs": [],
      "source": [
        "global_skills = []\n",
        "global_countries = []\n",
        "global_domains = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYzBr6tsWVoI"
      },
      "source": [
        "## Create LlamaCloud Pipeline/ Index\n",
        "\n",
        "Here, we define `embedding_config` and `transform_config` to set the `OPENAI_EMBEDDING`, `chunk_size`, and `chunk_overlap` parameters needed for creating an index on `LlamaCloud`.\n",
        "\n",
        "We will then create a pipeline/index on `LlamaCloud` under the name `resume_matching`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLh_E7AyMdsq"
      },
      "outputs": [],
      "source": [
        "# Embedding config\n",
        "embedding_config = {\n",
        "    'type': 'OPENAI_EMBEDDING',\n",
        "    'component': {\n",
        "        'api_key': os.environ[\"OPENAI_API_KEY\"], # editable\n",
        "        'model_name': 'text-embedding-ada-002' # editable\n",
        "    }\n",
        "}\n",
        "\n",
        "# Transformation auto config\n",
        "transform_config = {\n",
        "    'mode': 'auto',\n",
        "    'config': {\n",
        "        'chunk_size': 1024,\n",
        "        'chunk_overlap': 20\n",
        "    }\n",
        "}\n",
        "\n",
        "client, pipeline = create_llamacloud_pipeline('resume_matching', embedding_config, transform_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTNqPyNZWYuC"
      },
      "source": [
        "## Upload Documents\n",
        "\n",
        "Once the index/pipeline is created, we will upload all the parsed resumes (documents) using the `upload_documents` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpHcng41MfS4"
      },
      "outputs": [],
      "source": [
        "await upload_documents(client, pipeline, documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efO7c5IHWa4S"
      },
      "source": [
        "## Connect to LlamaCloud Index\n",
        "\n",
        "Here, we connect to the `resume_matching` index that was created on `LlamaCloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AVm31UbMgnZ"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
        "\n",
        "index = LlamaCloudIndex(\n",
        "  name=\"resume_matching\",\n",
        "  project_name=\"Default\",\n",
        "  organization_id=\"YOUR ORGANIZATION ID\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An5NXm0mXBql"
      },
      "source": [
        "## Utils for Candidate retrieval.\n",
        "\n",
        "Once the index is created, we need to retrieve candidate profiles based on HR queries. Here, we will define some functions for this purpose.\n",
        "\n",
        "1. `get_query_metadata`: Extracts structured metadata from user queries by matching against existing global metadata\n",
        "\n",
        "2. `candidates_retriever_from_query`: Retrieves relevant candidate profiles based on user query using metadata filters\n",
        "\n",
        "3. `get_candidates_file_paths`: Extracts unique file paths from retrieved candidate metadata\n",
        "\n",
        "4. `candidates_retriever_from_jd`: Retrieves matching candidate profiles based on job description using metadata filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idSIbIS_MiDC"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.vector_stores import (\n",
        "    MetadataFilter,\n",
        "    MetadataFilters,\n",
        "    FilterOperator,\n",
        "    FilterCondition\n",
        ")\n",
        "async def get_query_metadata(text):\n",
        "    \"\"\"Function to get the metadata from the given user query\"\"\"\n",
        "    prompt_template = PromptTemplate(\"\"\"Generate skills, and country of the education for the given user query.\n",
        "\n",
        "    Extracted metadata should be from the following items:\n",
        "\n",
        "    skills: {global_skills}\n",
        "    countries: {global_countries}\n",
        "    domains: {global_domains}\n",
        "    user query:\n",
        "\n",
        "    {text}\"\"\")\n",
        "\n",
        "    extracted_metadata = await llm.astructured_predict(\n",
        "        Metadata,\n",
        "        prompt_template,\n",
        "        text=text,\n",
        "        global_skills=global_skills,\n",
        "        global_countries=global_countries,\n",
        "        global_domains=global_domains\n",
        "    )\n",
        "\n",
        "    return extracted_metadata\n",
        "\n",
        "async def candidates_retriever_from_query(query: str):\n",
        "    \"\"\"Synthesizes an answer to your question by feeding in an entire relevant document as context.\"\"\"\n",
        "    print(f\"> User query string: {query}\")\n",
        "    # Use structured predict to infer the metadata filters and query string.\n",
        "    metadata_info = await get_query_metadata(query)\n",
        "    filters = MetadataFilters(\n",
        "    filters=[\n",
        "        MetadataFilter(key=\"domain\", operator=FilterOperator.EQ, value=metadata_info.domain),\n",
        "        MetadataFilter(key=\"country\", operator=FilterOperator.IN, value=metadata_info.country),\n",
        "        MetadataFilter(key=\"skills\", operator=FilterOperator.IN, value=metadata_info.skills)\n",
        "    ],\n",
        "    condition=FilterCondition.OR\n",
        ")\n",
        "    print(f\"> Inferred filters: {filters.json()}\")\n",
        "    retriever = index.as_retriever(\n",
        "    retrieval_mode=\"chunks\",\n",
        "    metadata_filters=filters,\n",
        "    )\n",
        "    # run query\n",
        "    return retriever.retrieve(query)\n",
        "\n",
        "def get_candidates_file_paths(candidates):\n",
        "\n",
        "  file_paths = []\n",
        "  for candidate in candidates:\n",
        "    file_paths.append(candidate.metadata['file_path'])\n",
        "\n",
        "  return list(set(file_paths))\n",
        "\n",
        "async def candidates_retriever_from_jd(job_description: str):\n",
        "    # Use structured predict to infer the metadata filters and query string.\n",
        "    metadata_info = await get_metadata(job_description)\n",
        "    filters = MetadataFilters(\n",
        "    filters=[\n",
        "        MetadataFilter(key=\"domain\", operator=FilterOperator.EQ, value=metadata_info.domain),\n",
        "        MetadataFilter(key=\"country\", operator=FilterOperator.IN, value=metadata_info.country),\n",
        "        MetadataFilter(key=\"skills\", operator=FilterOperator.IN, value=metadata_info.skills)\n",
        "    ],\n",
        "    condition=FilterCondition.OR\n",
        ")\n",
        "    print(f\"> Inferred filters: {filters.json()}\")\n",
        "    retriever = index.as_retriever(\n",
        "    retrieval_mode=\"chunks\",\n",
        "    metadata_filters=filters,\n",
        "    )\n",
        "    # run query\n",
        "    return retriever.retrieve(job_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0LGRz-MXRtl"
      },
      "source": [
        "## Retrieve based on HR query\n",
        "\n",
        "Let's test the process based on a usual sample HR query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkMluwweMj7O",
        "outputId": "4648f0e3-37f5-47ba-e096-d2e02a4977d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> User query string: I want someone who studied in US, Java developer, and worked in IT\n",
            "> Inferred filters: {\"filters\":[{\"key\":\"domain\",\"value\":\"IT\",\"operator\":\"==\"},{\"key\":\"country\",\"value\":[\"USA\",\"United States\",\"Philippines\",\"China\",\"Netherlands\",\"Sierra Leone\"],\"operator\":\"in\"},{\"key\":\"skills\",\"value\":[\"Java\",\"Troubleshooting\",\"Problem Solving\",\"Communication Skills\",\"Team Collaboration\",\"Project Management\",\"Database Management\",\"Data Analysis\",\"Technical Assistance\",\"IT Management\",\"Cloud computing\",\"Business Intelligence\",\"Systems Architecture\",\"SQL\",\"Microsoft Office\",\"ERP\",\"Business Process Design\",\"Data Warehouse\",\"Project Management\",\"User Relations/User Training\",\"Business Analysis\",\"Disaster recovery\",\"IT Strategy\",\"Networking\",\"Information Security\",\"Technical Trainer\",\"Change Management\",\"Risk Management\",\"Process Improvement\",\"Team Leadership\",\"Client-focused\",\"Results-oriented\",\"Strategic Planning\",\"Budgeting/Cost control\",\"Financial Analysis\",\"Quality Assurance\",\"Sales expertise\",\"Customer Service\",\"Active Listening\",\"Problem Solving\",\"Time Management\",\"Leadership\",\"Training and Development\",\"Documentation\",\"Researching\",\"Analytical reasoning\",\"Excellent communication\",\"Fast Learner\",\"Self-Motivated\",\"Organizational Skills\",\"Team Oriented\",\"Computer proficient\",\"Problem Solver\",\"Billing online system\",\"Customer-oriented\",\"Financial statement analysis\"],\"operator\":\"in\"}],\"condition\":\"or\"}\n"
          ]
        }
      ],
      "source": [
        "query = \"I want someone who studied in US, Java developer, and worked in IT\"\n",
        "nodes = await candidates_retriever_from_query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the retrieved candidates resumes file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idd3UoL4MlYp",
        "outputId": "852844b2-11be-4fca-854a-5e43dd1d6d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sampled_data/IT/16899268.pdf', 'sampled_data/IT/27536013.pdf']\n"
          ]
        }
      ],
      "source": [
        "print(get_candidates_file_paths(nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfBDzbhNXV09"
      },
      "source": [
        "## Retrieve candidate based on JD (Job Description)\n",
        "\n",
        "Here we retrieve candidates based on Job Description.\n",
        "\n",
        "We parse the job description pdf and use it to retrieve the relevant candidates for the job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K3-TQjHXY3i"
      },
      "source": [
        "### Parse Job Description (JD)\n",
        "\n",
        "Here, we parse the sample job_description.pdf that we have downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVr2ZZHFMmzq",
        "outputId": "e0958b08-faa2-4a24-e0b6-875d1d85c2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1/1: ./job_description.pdf\n",
            "Started parsing the file under job_id 0df3043a-74dd-40e8-a83b-c93416160d0d\n"
          ]
        }
      ],
      "source": [
        "job_description_file_path = './job_description.pdf'\n",
        "\n",
        "job_description_document = parse_files([job_description_file_path])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlKmaJY6MoYG"
      },
      "outputs": [],
      "source": [
        "job_description = \"\\n\\n\".join([doc.text for doc in job_description_document[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dogo3GkNMpkd",
        "outputId": "c8a50dc9-c824-4c61-90f5-1dfcafb5d3c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Senior Information Technology Manager\n",
            "\n",
            "# About the Role\n",
            "\n",
            "We are seeking an experienced Information Technology Manager to lead our technology initiatives and drive digital transformation across the organization. The ideal candidate will combine strong technical expertise with business acumen and leadership skills.\n",
            "\n",
            "# Key Responsibilities\n",
            "\n",
            "- Lead and manage a cross-functional IT team in developing and implementing technology solutions\n",
            "- Oversee the planning, implementation, and maintenance of enterprise IT systems and infrastructure\n",
            "- Drive strategic technology initiatives aligned with business objectives\n",
            "- Manage vendor relationships and technology partnerships\n",
            "- Ensure system security, data integrity, and business continuity\n",
            "- Develop and maintain IT policies, procedures, and best practices\n",
            "- Budget planning and resource allocation for IT projects\n",
            "- Provide technical leadership in evaluating and implementing new technologies\n",
            "- Collaborate with stakeholders to identify technology needs and solutions\n",
            "\n",
            "# Required Qualifications\n",
            "\n",
            "- Bachelor's degree in Computer Science, Information Technology, or related field\n",
            "- 8+ years of progressive IT management experience\n",
            "- Strong experience with enterprise systems and infrastructure management\n",
            "- Proven track record of successful project management and delivery\n",
            "- Experience with IT security, compliance, and risk management\n",
            "- Strong knowledge of current technology trends and solutions\n",
            "\n",
            "# Technical Skills\n",
            "\n",
            "- Enterprise Resource Planning (ERP) systems\n",
            "- Network infrastructure and security\n",
            "- Cloud computing platforms and services\n",
            "\n",
            "# IT Service Management Frameworks\n",
            "\n",
            "- Database management systems\n",
            "- System integration and architecture\n",
            "- Virtualization technologies\n",
            "- Disaster recovery and business continuity\n",
            "\n",
            "# Leadership Skills\n",
            "\n",
            "- Team management and development\n",
            "- Strategic planning and execution\n",
            "- Strong communication and presentation skills\n",
            "- Problem-solving and analytical thinking\n",
            "- Change management\n",
            "- Budget management\n",
            "- Stakeholder management\n",
            "- Cross-functional collaboration\n",
            "\n",
            "# Preferred Qualifications\n",
            "\n",
            "- Master's degree in related field\n",
            "- Professional certifications (PMP, ITIL, etc.)\n",
            "- Experience with digital transformation initiatives\n",
            "- Knowledge of agile methodologies\n",
            "- Experience in similar industry\n",
            "\n",
            "# What We Offer\n",
            "\n",
            "- Competitive salary and benefits package\n",
            "- Professional development opportunities\n",
            "- Collaborative work environment\n",
            "- Opportunity to drive technological innovation\n",
            "- Work-life balance\n",
            "- Career advancement potential\n",
            "\n",
            "The ideal candidate will be a results-driven leader who can balance technical expertise with business strategy, while effectively managing teams and stakeholders across the organization.\n"
          ]
        }
      ],
      "source": [
        "print(job_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaQq0v4xXdHg"
      },
      "source": [
        "### Retrieve candidates\n",
        "\n",
        "Here we retrieve candidates based on the job description text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR1ZHslDMtZJ",
        "outputId": "f9f1aafe-247e-4b35-a346-a6992e28b2d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Inferred filters: {\"filters\":[{\"key\":\"domain\",\"value\":\"IT\",\"operator\":\"==\"},{\"key\":\"country\",\"value\":[\"United States\"],\"operator\":\"in\"},{\"key\":\"skills\",\"value\":[\"Enterprise Resource Planning (ERP) systems\",\"Network infrastructure and security\",\"Cloud computing platforms and services\",\"Database management systems\",\"System integration and architecture\",\"Virtualization technologies\",\"Disaster recovery and business continuity\",\"Team management and development\",\"Strategic planning and execution\",\"Strong communication and presentation skills\",\"Problem-solving and analytical thinking\",\"Change management\",\"Budget management\",\"Stakeholder management\",\"Cross-functional collaboration\"],\"operator\":\"in\"}],\"condition\":\"or\"}\n"
          ]
        }
      ],
      "source": [
        "candidates_based_on_jd = await candidates_retriever_from_jd(job_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_hdQbXCMvSw"
      },
      "outputs": [],
      "source": [
        "candidates_file_paths = get_candidates_file_paths(candidates_based_on_jd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5DFzje1P1-C",
        "outputId": "8df11b4b-0cc7-46be-fdb0-a21020c6b898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sampled_data/IT/18159866.pdf', 'sampled_data/FINANCE/25101183.pdf', 'sampled_data/IT/27536013.pdf']\n"
          ]
        }
      ],
      "source": [
        "print(candidates_file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBJaAwAlXiKL"
      },
      "source": [
        "## Analyse candidate resume based on retrieval\n",
        "\n",
        "Once we have the relevant candidate resumes, we need to analyze why, how, and which candidates are suitable for the job description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7YmWH3uXm7p"
      },
      "source": [
        "### Parse the candidate resumes\n",
        "\n",
        "Here, we parse the candidate resumes retrieved based on the job description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LxRhGjQP4ml",
        "outputId": "f200ba2f-5a34-4f9b-9d9d-d661d0cb5fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1/3: sampled_data/IT/18159866.pdf\n",
            "Started parsing the file under job_id 50c1cf22-b692-4521-b40c-62f6a31e1215\n",
            "Processing file 2/3: sampled_data/FINANCE/25101183.pdf\n",
            "Started parsing the file under job_id 85992196-4c31-474c-8423-3ead6fe5835f\n",
            "Processing file 3/3: sampled_data/IT/27536013.pdf\n",
            "Started parsing the file under job_id e4263df3-4611-453c-834d-3c0eecf522be\n"
          ]
        }
      ],
      "source": [
        "candidates_resumes = parse_files(candidates_file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YAcJ2b0P9g1"
      },
      "outputs": [],
      "source": [
        "candidates_resumes_text = \"\\n\\n\".join([doc.text for docs in candidates_resumes for doc in docs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BX27yyvXpeP"
      },
      "source": [
        "### Analyses\n",
        "\n",
        "Let's analyze the candidate resumes against the job description by processing them through the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of6qb4S_Qb4J"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"Based on the following job description, please share the analysis of why specific candidates are suitable for the job.\n",
        "\n",
        "        Job Description:\n",
        "        {job_description}\n",
        "\n",
        "        Candidates:\n",
        "        {candidates_resumes_text}\n",
        "        \"\"\"\n",
        "\n",
        "analyses = llm.complete(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyGtOSpSQx_x",
        "outputId": "d73e4559-2db4-404e-958c-978064977bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the job description for the Senior Information Technology Manager position and the profiles of the candidates provided, here is an analysis of why specific candidates may be suitable for the job:\n",
            "\n",
            "### Candidate 1: Senior Vice President of Global Information Technology\n",
            "\n",
            "**Strengths:**\n",
            "1. **Extensive Experience:** With over 20 years in IT management, including a current role as Senior Vice President, this candidate has significant experience leading large teams and managing complex IT environments, which aligns well with the requirement for 8+ years of progressive IT management experience.\n",
            "   \n",
            "2. **Project Management Expertise:** The candidate has a proven track record of managing cross-functional teams on large implementations and development projects, which is crucial for overseeing the planning, implementation, and maintenance of enterprise IT systems.\n",
            "\n",
            "3. **Strategic Planning and Execution:** Their experience in strategic planning and change implementation demonstrates the ability to drive strategic technology initiatives aligned with business objectives.\n",
            "\n",
            "4. **Risk Management and Security:** The candidate has conducted periodic risk assessments and developed disaster recovery plans, which is essential for ensuring system security, data integrity, and business continuity.\n",
            "\n",
            "5. **Cost Savings and Efficiency:** The candidate has a history of delivering projects on time and within budget, realizing significant improvements in processing efficiency, which is a key responsibility in the job description.\n",
            "\n",
            "**Suitability:** This candidate is highly suitable due to their extensive leadership experience, strategic planning capabilities, and proven success in managing large-scale IT projects and teams.\n",
            "\n",
            "---\n",
            "\n",
            "### Candidate 2: Program Manager / PMO Director\n",
            "\n",
            "**Strengths:**\n",
            "1. **Program Management Experience:** This candidate has a strong background in program management and has led multi-functional technology teams, which is essential for managing a cross-functional IT team.\n",
            "\n",
            "2. **Financial Acumen:** Their experience in managing budgets, financial analysis, and cost savings initiatives aligns well with the job's requirement for budget planning and resource allocation for IT projects.\n",
            "\n",
            "3. **Change Management:** The candidate has a proven ability to turn around underperforming programs and lead successful change initiatives, which is important for driving digital transformation across the organization.\n",
            "\n",
            "4. **Stakeholder Management:** They have demonstrated skills in stakeholder management and collaboration, which are critical for identifying technology needs and solutions in collaboration with various stakeholders.\n",
            "\n",
            "5. **Certifications:** The candidate holds relevant certifications such as PMP and ITIL, which are preferred qualifications for the role.\n",
            "\n",
            "**Suitability:** This candidate is suitable due to their strong program management skills, financial expertise, and experience in leading change initiatives, making them a good fit for the strategic and operational aspects of the role.\n",
            "\n",
            "---\n",
            "\n",
            "### Candidate 3: Experienced Information Technology Manager\n",
            "\n",
            "**Strengths:**\n",
            "1. **Diverse IT Management Experience:** With over 10 years of experience in various management areas, this candidate has a solid foundation in IT management, which meets the job's experience requirement.\n",
            "\n",
            "2. **Project Management Skills:** The candidate has demonstrated proficiency in project management, managing resources, and personnel, which is essential for leading and managing an IT team.\n",
            "\n",
            "3. **Business Intelligence and Reporting:** Their experience in redesigning BI programs and implementing effective systems indicates a strong understanding of technology solutions that can drive business objectives.\n",
            "\n",
            "4. **User Relations and Training:** The candidate has experience in user relations and training, which is important for collaborating with stakeholders to identify technology needs and solutions.\n",
            "\n",
            "5. **Cost Savings Initiatives:** They have successfully implemented projects that resulted in significant cost savings, aligning with the job's focus on budget management and resource allocation.\n",
            "\n",
            "**Suitability:** This candidate is suitable due to their solid IT management experience, project management skills, and ability to implement effective technology solutions that align with business needs.\n",
            "\n",
            "---\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "Each candidate brings a unique set of skills and experiences that align with the requirements of the Senior Information Technology Manager position. Candidate 1 stands out for their extensive leadership experience and strategic planning capabilities. Candidate 2 offers strong program management and financial acumen, while Candidate 3 provides a solid foundation in IT management and project execution. Depending on the specific needs and culture of the organization, any of these candidates could be a strong fit for the role.\n"
          ]
        }
      ],
      "source": [
        "print(analyses)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llamaindex",
      "language": "python",
      "name": "llamaindex"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}