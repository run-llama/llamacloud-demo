{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1348d3-4c0e-450f-8faf-19503f61b7b2",
   "metadata": {},
   "source": [
    "# LlamaCloud Client SDK: Integrating LlamaHub Loaders\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/client_sdk/llamahub_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This tutorial shows you how to use LlamaHub loaders to insert documents with LlamaCloud, with the help of the lower-level LlamaCloud Client SDK.\n",
    "\n",
    "In this example, we use the [Firecrawl web page reader](https://www.firecrawl.dev/) in order to load a web page document, and feed it into our LlamaCloud pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83a35ec-8e6c-475c-827c-20f46c4a3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47019e7-5bf8-49ee-8ab1-1f1ae1ae55e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-cloud\n",
    "!pip install llama-index-readers-web firecrawl-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57082f55-66e0-44e1-8072-2450405c21d1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we setup our environment variables, client, and load data using the Firecrawl loader available on LlamaHub.\n",
    "\n",
    "The Firecrawl loader is available as part of our `llama-index-readers-web` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a907cb-a727-4c12-86c9-ca2c55d73a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_BASE_URL\"] = \"https://api.cloud.llamaindex.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddd884dc-8da5-498d-a186-d08d30e8478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"<LLAMA_CLOUD_API_KEY>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2235076d-3032-41ee-b936-f8a0081ec5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRECRAWL_API_KEY = \"<FIRECRAWL_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f00a3-cace-4975-bc73-13280d0f5d32",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6590c2eb-4a0d-4a5c-8dd9-092ed43d2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import FireCrawlWebReader\n",
    "\n",
    "# using firecrawl to crawl a website\n",
    "firecrawl_reader = FireCrawlWebReader(\n",
    "    api_key=FIRECRAWL_API_KEY,\n",
    "    mode=\"scrape\",  # Choose between \"crawl\" and \"scrape\" for single page scraping\n",
    "    # params={\"additional\": \"parameters\"},  # Optional additional parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b59927cc-c43b-4c40-9840-586b3fce39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from a single page URL\n",
    "documents = firecrawl_reader.load_data(url=\"https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdb3e5-902b-4d79-930b-df2063c3a39b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(documents[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91a633-34c7-4c5b-b6cb-de37e80ace68",
   "metadata": {},
   "source": [
    "#### Setup Index\n",
    "\n",
    "Please setup an empty index. You can either do this through the UI or [programmatically](https://docs.cloud.llamaindex.ai/llamacloud/guides/framework_integration).\n",
    "\n",
    "After you've done so, make sure to note down the pipeline_id, pipeline_name, project_id, and project_name in the variables below. You'll need these later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04e80c0d-d58d-4fce-aa78-9247b9840738",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_id = \"<pipeline_id>\"\n",
    "pipeline_name = \"<pipeline_name>\"\n",
    "project_id = \"<project_id>\"\n",
    "project_name = \"<project_name>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afeb53",
   "metadata": {},
   "source": [
    "#### Setup LlamaCloud Client SDK and Framework Client\n",
    "\n",
    "Here we define both the client (giving us access to low-level client operations) as well as the `LlamaCloudIndex` defined through the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4c36489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud.client import LlamaCloud\n",
    "\n",
    "client = LlamaCloud(\n",
    "    token=os.environ[\"LLAMA_CLOUD_API_KEY\"],\n",
    "    base_url=os.environ[\"LLAMA_CLOUD_BASE_URL\"]\n",
    ")\n",
    "\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "import os\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=pipeline_name, \n",
    "  project_name=project_name,\n",
    "  api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e45c5-4255-4dfa-ae3b-f33aac640df5",
   "metadata": {},
   "source": [
    "## Inserting Documents\n",
    "\n",
    "Now let's create the custom Document objects. We assume that your pipeline has been created in the last section. Copy the pipeline and project ids into the box below.\n",
    "\n",
    "We insert one document containing the parsed document text, and another document as a toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7171d-eb65-479b-8bab-a38e7f5ef221",
   "metadata": {},
   "source": [
    "#### Inserting Document Objects through the Client SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adfde1cf-1941-4bf6-ad0b-903ebd298ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_documents = [d.to_cloud_document() for d in documents]\n",
    "upserted_docs = client.pipelines.upsert_batch_pipeline_documents(pipeline_id, request=cloud_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0e171-8834-4a46-aee5-4d7551edac92",
   "metadata": {},
   "source": [
    "#### Inserting Document Objects through the Framework Integration\n",
    "\n",
    "You can also do `index.insert` to directly upload document objects using the types defined by the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "346dde6d-3c6f-4425-8be4-a3d06baca325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the llamaparsed document is already in the right representation\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "for doc in documents:\n",
    "    index.insert(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62a2da-f1a8-486d-8c62-775844369142",
   "metadata": {},
   "source": [
    "#### Validating the Documents\n",
    "\n",
    "After the documents have been inserted, we can validate that they exist in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd0cfcd8-2e77-4e75-ba5c-7cd629706aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "pipeline_docs = client.pipelines.list_pipeline_documents(pipeline_id)\n",
    "\n",
    "print(len(pipeline_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a951e23-8608-4082-913b-d7567de845c1",
   "metadata": {},
   "source": [
    "#### Deleting the Documents\n",
    "\n",
    "If you want to reset, you can use the client SDK to delete pipeline documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1082187-6218-4d7b-9244-982ba309f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_docs = client.pipelines.list_pipeline_documents(pipeline_id)\n",
    "for doc in pipeline_docs:\n",
    "    client.pipelines.delete_pipeline_document(pipeline_id, doc.id)\n",
    "client.pipelines.sync_pipeline(pipeline_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5cf563-d119-40da-977d-9557e41328c7",
   "metadata": {},
   "source": [
    "## Test RAG\n",
    "\n",
    "Let's create a sample RAG pipeline through the Python framework, through our `LlamaCloudIndex` (you can also run our lower-level search API through `client.pipelines.run_search`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23ccb658-6620-49f6-a363-5f608733b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intern test for evaluating generations involves assessing whether an average college student in the relevant major could successfully complete the task given the same input and context as the language model. If the student could succeed, the task is considered feasible for the model. If not, the context may need to be enriched or the task simplified. If the task is too complex even after improvements, it may be beyond the capabilities of contemporary language models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "retriever = index.as_retriever(rerank_top_n=5)\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever,\n",
    "    llm=llm\n",
    ")\n",
    "response = query_engine.query(\"What is the intern test for evaluating generations?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c2f771a-f213-46fd-a23e-37695e0df0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- SOURCE NODE 0 --------\n",
      "Also consider checks to ensure that word, item, or sentence counts lie within a range. For other kinds of generation, assertions can look different. [Execution-evaluation](https://www.semanticscholar.org/paper/Execution-Based-Evaluation-for-Open-Domain-Code-Wang-Zhou/1bed34f2c23b97fd18de359cf62cd92b3ba612c3)\n",
      " is a powerful method for **evaluating** code-generation, wherein you run the generated code and determine that the state of runtime is sufficient for the user-request.\n",
      "\n",
      "As an example, if the user asks for a new function named foo; then after executing the agent’s generated code, foo should be callable! One challenge in execution-evaluation is that the agent code frequently leaves the runtime in slightly different form than the target code. It can be effective to “relax” assertions to the absolute most weak assumptions that any viable answer would satisfy.\n",
      "\n",
      "Finally, using your product as intended for customers (i.e., “dogfooding”) can provide insight into failure modes on real-worl....\n",
      "--------- SOURCE NODE 1 --------\n",
      "![](https://www.oreilly.com/radar/wp-content/uploads/sites/3/2024/05/Picture1.png)\n",
      "\n",
      "LLM-as-Judge is not a silver bullet though. There are subtle aspects of language where even the strongest models fail to evaluate reliably. In addition, we’ve found that [conventional classifiers](https://eugeneyan.com/writing/finetuning/)\n",
      " and reward models can achieve higher accuracy than LLM-as-Judge, and with lower cost and latency. For code generation, LLM-as-Judge can be weaker than more direct evaluation strategies like execution-evaluation.\n",
      "\n",
      " **The “**intern** **test**” for **evaluating** generations**\n",
      "\n",
      "We like to use the following “**intern** **test**” when **evaluating** generations: If you took the exact input to the language model, including the context, and gave it to an average college student in the relevant major as a task, could they succeed? How long would it take?\n",
      "\n",
      "If the answer is no because the LLM lacks the required knowledge, consider ways to enrich the context.\n",
      "\n",
      "If the answer is ....\n",
      "--------- SOURCE NODE 2 --------\n",
      "When a request comes in, we can check to see if a summary already exists in the cache. If so, we can return it immediately; if not, we generate, guardrail, and serve it, and then store it in the cache for future requests.\n",
      "\n",
      "For more open-ended queries, we can borrow techniques from the field of search, which also leverages caching for open-ended inputs. Features like autocomplete and spelling correction also help normalize user input and thus increase the cache hit rate.\n",
      "\n",
      " **When to fine-tune**\n",
      "\n",
      "We may have some tasks where even the most cleverly designed prompts fall short. For example, even after significant prompt engineering, our system may still be a ways from returning reliable, high-quality output. If so, then it may be necessary to finetune a model for your specific task.\n",
      "\n",
      "Successful examples include:\n",
      "\n",
      "*   [Honeycomb’s Natural Language Query Assistant](https://www.honeycomb.io/blog/introducing-query-assistant)\n",
      "    : Initially, the “programming manual” was provided in the prompt ....\n",
      "--------- SOURCE NODE 3 --------\n",
      "[Skip to main content](#maincontent)\n",
      "\n",
      "[![O'Reilly home](https://cdn.oreillystatic.com/images/sitewide-headers/oreilly_logo_mark_red_@2x.png)](https://www.oreilly.com/ \"home page\")\n",
      "\n",
      "*   *   [Sign In](https://www.oreilly.com/member/login/)\n",
      "        \n",
      "    *   [Try Now](https://oreilly.com/online-learning/try-now.html)\n",
      "        \n",
      "*   *   [](https://www.oreilly.com/online-learning/teams.html)\n",
      "        [](https://www.oreilly.com/online-learning/teams.html)\n",
      "        [Teams](https://www.oreilly.com/online-learning/teams.html)\n",
      "        *   [For business](https://www.oreilly.com/online-learning/teams.html)\n",
      "            \n",
      "        *   [For government](https://www.oreilly.com/online-learning/government.html)\n",
      "            \n",
      "        *   [For higher ed](https://www.oreilly.com/online-learning/academic.html)\n",
      "            \n",
      "    *   [](https://www.oreilly.com/online-learning/individuals.html)\n",
      "        [](https://www.oreilly.com/online-learning/individuals.html)\n",
      "        [Individuals](https://www.oreilly.com/online-lear....\n",
      "--------- SOURCE NODE 4 --------\n",
      "In the first step, given a high-level goal or prompt, the agent generates a plan. Then, the plan is executed deterministically. This allows each step to be more predictable and reliable. Benefits include:\n",
      "\n",
      "*   Generated plans can serve as few-shot samples to prompt or finetune an agent.\n",
      "*   Deterministic execution makes the system more reliable, and thus easier to **test** and debug. Furthermore, failures can be traced to the specific steps in the plan.\n",
      "*   Generated plans can be represented as directed acyclic graphs (DAGs) which are easier, relative to a static prompt, to understand and adapt to new situations.\n",
      "\n",
      "The most successful agent builders may be those with strong experience managing junior engineers because the process of generating plans is similar to how we instruct and manage juniors. We give juniors clear goals and concrete plans, instead of vague open-ended directions, and we should do the same for our agents too.\n",
      "\n",
      "In the end, the key to reliable, working agents will lik....\n"
     ]
    }
   ],
   "source": [
    "# view sources\n",
    "for idx, n in enumerate(response.source_nodes):\n",
    "    print(f\"--------- SOURCE NODE {idx} --------\")\n",
    "    print(n.get_content()[:1000] + \"....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabe94c-ae69-4c33-a11f-edc7f4015c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
