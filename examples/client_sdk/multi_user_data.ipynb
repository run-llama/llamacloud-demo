{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1348d3-4c0e-450f-8faf-19503f61b7b2",
   "metadata": {},
   "source": [
    "# LlamaCloud Client SDK: Supporting User-Level Data across Multiple Users\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/client_sdk/multi_user.ipynb\n",
    "    \" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This tutorial shows you how to use LlamaCloud to build RAG workflows that can support user-level data. For instance, you may want to build a chatbot where each user can upload their own files. Each user should only be able to ask questions over the files they've uploaded (and optionally organizational data), but not the files of other users.\n",
    "\n",
    "We show two approaches to do this: \n",
    "1. [Preferred] Create a separate index per user\n",
    "2. Create a single index, separate users by metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83a35ec-8e6c-475c-827c-20f46c4a3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47019e7-5bf8-49ee-8ab1-1f1ae1ae55e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57082f55-66e0-44e1-8072-2450405c21d1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we setup our environment variables, data, and the client SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a907cb-a727-4c12-86c9-ca2c55d73a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_BASE_URL\"] = \"https://api.cloud.llamaindex.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150bc97-7b34-4817-b65f-f909f76045d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"<LLAMA_CLOUD_API_KEY>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f00a3-cace-4975-bc73-13280d0f5d32",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "We download three ArXiv papers and pretend that each paper file corresponds to a separate user upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94260e77-c2f2-4083-8167-d8d8dea9da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"data/metagpt.pdf\",\n",
    "    \"data/longlora.pdf\",\n",
    "    \"data/selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde2f43-fb9b-49ef-8c4d-250dbc39d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, paper in zip(urls, papers):\n",
    "    !wget \"{url}\" -O \"{paper}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91a633-34c7-4c5b-b6cb-de37e80ace68",
   "metadata": {},
   "source": [
    "#### Define Project Configuration\n",
    "\n",
    "Specify your project name and project id below.\n",
    "\n",
    "The pipeline_id and pipeline_name will be programmatically created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e80c0d-d58d-4fce-aa78-9247b9840738",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<project_id>\"\n",
    "project_name = \"<project_name>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afeb53",
   "metadata": {},
   "source": [
    "#### Setup LlamaCloud Client SDK\n",
    "\n",
    "Here we define both the client (giving us access to low-level client operations).s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c36489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud.client import LlamaCloud\n",
    "\n",
    "client = LlamaCloud(\n",
    "    token=os.environ[\"LLAMA_CLOUD_API_KEY\"],\n",
    "    base_url=os.environ[\"LLAMA_CLOUD_BASE_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e45c5-4255-4dfa-ae3b-f33aac640df5",
   "metadata": {},
   "source": [
    "## Inserting Documents For Each User\n",
    "\n",
    "We use the `upload_file` capability in the SDK to upload files to LlamaCloud.\n",
    "\n",
    "We show two ways of supporting per-user data in LlamaCloud (note: they are mutually exclusive).\n",
    "1. Create a separate index for each user.\n",
    "2. Add all files to the same index, separate them by metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb953ed-b97c-4f9b-ab31-351ccc9ea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend each user corresponds to a separate paper upload\n",
    "users = [\"jerry\", \"bob\", \"alice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c63bb94-b0e0-4b40-b611-1629e83518c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload file and add file to pipeline\n",
    "files = []\n",
    "for paper in papers:\n",
    "    with open(paper, 'rb') as f:\n",
    "        file = client.files.upload_file(upload_file=f, project_id=project_id)\n",
    "        files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da3390-ecfb-4339-9b56-b776336ea2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# NOTE: specify if you want to explicitly specify a data sink\n",
    "QDRANT_API_KEY = \"<QDRANT_API_KEY>\"\n",
    "QDRANT_URL = \"<QDRANT_URL>\"\n",
    "\n",
    "def get_qdrant_sink(collection_name):\n",
    "    ds = {\n",
    "        'name': 'qdrant',\n",
    "        'sink_type': 'QDRANT', \n",
    "        'component': CloudQdrantVectorStore(\n",
    "            collection_name=collection_name,\n",
    "            url=QDRANT_URL, \n",
    "            api_key=QDRANT_API_KEY\n",
    "        )\n",
    "    }\n",
    "    data_sink = client.data_sinks.create_data_sink(request=ds)\n",
    "    return data_sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d201f9a1-a38b-4393-a67b-df990b10f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_pipeline(index_name, project_id, data_sink = None, transformations=None):\n",
    "    \"\"\"Create pipeline.\"\"\"\n",
    "    if transformations is None:\n",
    "        transformations = [\n",
    "          {\n",
    "              'configurable_transformation_type': 'SENTENCE_AWARE_NODE_PARSER',\n",
    "              'component': {\n",
    "                  'chunk_size': 1024,\n",
    "                  'chunk_overlap': 20,\n",
    "              }\n",
    "          },\n",
    "          {\n",
    "              'configurable_transformation_type': 'OPENAI_EMBEDDING',\n",
    "              'component': {\n",
    "                  'model_name': 'text-embedding-ada-002',\n",
    "                  'api_key': os.environ[\"OPENAI_API_KEY\"],\n",
    "              }\n",
    "          }\n",
    "      ]\n",
    "    pipeline_req = {\n",
    "      'name': index_name,\n",
    "      'configured_transformations': transformations,\n",
    "      'data_sink': data_sink\n",
    "    }\n",
    "    pipeline = client.pipelines.upsert_pipeline(project_id=project_id, request=pipeline_req)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9fe63-d894-4dde-8feb-bf0aecd26045",
   "metadata": {},
   "source": [
    "### Option 1: Create separate index for each user\n",
    "\n",
    "We first configure a Qdrant data sink, and then we configure our transformations. \n",
    "\n",
    "We then create a separate pipeline per user, and store it in an in-memory dict (**note**: you will likely want to persist the pipeline ids per user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67f40a96-5c9a-4c24-be7f-111fbb2a18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud.types import CloudQdrantVectorStore\n",
    "\n",
    "# configure your transformations here \n",
    "user_pipeline_dict = {}\n",
    "for user, paper in zip(users, papers):\n",
    "\n",
    "    # uncomment if you want to use a managed vector store\n",
    "    # data_sink = get_qdrant_sink(f\"collection_{user}\")\n",
    "    data_sink = None\n",
    "    pipeline = create_pipeline(f\"{user}_index\", project_id, data_sink=data_sink)\n",
    "    user_pipeline_dict[user] = pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd236d-f4d7-4e5f-8893-659e0e09604b",
   "metadata": {},
   "source": [
    "#### Attach file per pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "164cf4eb-a185-4c42-815a-9855bfba1031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file metagpt.pdf for user jerry\n",
      "Adding file longlora.pdf for user bob\n",
      "Adding file selfrag.pdf for user alice\n"
     ]
    }
   ],
   "source": [
    "for file, (user, pipeline) in zip(files, user_pipeline_dict.items()):\n",
    "    print(f\"Adding file {file.name} for user {user}\")\n",
    "    pipeline_files = client.pipelines.add_files_to_pipeline(pipeline.id, request=[{'file_id': file.id}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "748cce62-78a2-46f3-80f7-a11aae38cc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_docs = client.pipelines.list_pipeline_documents(user_pipeline_dict[\"bob\"].id)\n",
    "len(pipeline_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41b625-8148-4642-8c41-870bc18e828d",
   "metadata": {},
   "source": [
    "#### Try querying your pipelines\n",
    "\n",
    "Here we use our framework integration to perform retrieval against our pipelines/indexes.\n",
    "\n",
    "Since each user corresponds to a separate pipeline, we will need to specify the index before we get a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3db3ab0d-4115-4cc1-a623-1e54cbdb37a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper presents LongLoRA, an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with limited computational cost. It addresses the high computational expense typically associated with training LLMs on long context sizes. The approach speeds up context extension through two main strategies: using sparse local attention during fine-tuning and revisiting parameter-efficient fine-tuning regimes for context expansion. LongLoRA combines these strategies to achieve significant computation savings while maintaining performance. It demonstrates strong empirical results on various tasks with Llama2 models and extends their context sizes significantly without altering their original architectures. The paper also mentions the availability of their code, models, dataset, and demo online.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import os\n",
    "\n",
    "# change this user to whatever you prefer\n",
    "USER = \"bob\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "index = LlamaCloudIndex(\n",
    "  name=user_pipeline_dict[USER].name, \n",
    "  project_name=project_name,\n",
    "  api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    ")\n",
    "query_engine = index.as_query_engine(rerank_top_n=2, llm=llm)\n",
    "response = query_engine.query(\"Tell me about the abstract of this paper\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317bceb-7a16-47b6-8ab3-9cd6d31137d9",
   "metadata": {},
   "source": [
    "### Option 2: Add all files to the same index, separate by metadata\n",
    "\n",
    "The other option is to create a single index, and then add all files to the same index (separated by metadata).\n",
    "\n",
    "You have two options to create an index:\n",
    "1. Through the UI. Make sure to note down the pipeline id.\n",
    "2. Through the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed407d6-af0a-48b7-a917-f7066abd22e5",
   "metadata": {},
   "source": [
    "#### Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9c5c08-6c5e-4894-b940-bf8d0dc1578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 1: Through UI. Note down the pipeline_id below\n",
    "# pipeline_id = \"<pipeline_id>\"\n",
    "# pipeline_name = \"<pipeline_name>\"\n",
    "\n",
    "## Option 2: Programmatically\n",
    "data_sink = None\n",
    "# data_sink = get_qdrant_sink(f\"collection_{user}\")\n",
    "pipeline = create_pipeline(\"users_index\", project_id, data_sink=data_sink)\n",
    "pipeline_id = pipeline.id\n",
    "pipeline_name = pipeline.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eee2452-a6b4-4cff-bb2f-d5841e19a758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faaef3db-3f1a-4e9c-be79-31992b9c2fc9'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c754b-2804-4fc2-896d-897c3e2771d2",
   "metadata": {},
   "source": [
    "#### Upload each file, attach user metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57b5d4e6-1563-4002-85f1-60b8ffea382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, user in zip(files, users):\n",
    "    pipeline_files = client.pipelines.add_files_to_pipeline(pipeline_id, request=[{'file_id': file.id}])\n",
    "    # update metadata with user info\n",
    "    pipeline_files = client.pipelines.update_pipeline_file(\n",
    "        pipeline_id=pipeline_id, file_id=file.id, custom_metadata={ \"user\": user }\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52eeab1d-d37f-48b0-99d9-4ed61bdda20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_docs = client.pipelines.list_pipeline_documents(pipeline_id)\n",
    "len(pipeline_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dcd84-1d45-4f85-813a-531b1a42f7ef",
   "metadata": {},
   "source": [
    "#### Try querying your pipelines\n",
    "\n",
    "Here we use our framework integration to perform retrieval against our LlamaCloud APIs. We can query the pipeline with a set of specified metadata filters to filter for specific user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3255889a-a56e-414c-a837-b6d591be75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import os\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "index = LlamaCloudIndex(\n",
    "  name=pipeline_name, \n",
    "  project_name=project_name,\n",
    "  api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb994b33-a39e-485a-a769-b92a80bb1394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper introduces LongLoRA, an efficient fine-tuning approach designed to extend the context sizes of pre-trained large language models (LLMs) with minimal computational cost. Training LLMs with long context sizes typically requires significant computational resources. LongLoRA addresses this by using sparse local attention during fine-tuning, which reduces computation while maintaining performance similar to dense global attention. Additionally, it combines an improved LoRA with shifted sparse attention (S2-Attn) to achieve context extension efficiently. LongLoRA has demonstrated strong empirical results on various tasks using Llama2 models, extending their context sizes significantly while retaining their original architectures and compatibility with existing techniques. The paper also mentions the availability of their code, models, dataset, and demo on GitHub.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** fc5bd4bc-11c5-424f-80bd-8b1daf253551<br>**Similarity:** 0.89947516<br>**Text:** **ABSTRACT**\n",
       "\n",
       "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16× computational costs in self-attention layers as that of 2048. In this **paper**, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention (S2-Attn) effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other han...<br>**Metadata:** {'user': 'bob', 'file_size': '1168720', 'last_modified_at': '2024-07-06T18:19:04', 'file_path': 'longlora.pdf', 'file_name': 'longlora.pdf', 'pipeline_id': '2532c4c9-f216-445d-8766-e7dfa22397d4'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 011b9560-abdb-4bdc-bafb-9102f7e8d1de<br>**Similarity:** 0.85524774<br>**Text:** We set the per-device batch size as 1 and gradient accumulation steps as 8, which means that the global batch size equals 64, using 8 GPUs. We train our models for 1000 steps.\n",
       "\n",
       "Datasets\n",
       "\n",
       "We use the Redpajama (Computer, 2023) dataset for training. We evaluate the long-sequence language modeling performance of our fine-tuned models on the book corpus dataset PG19 (Rae et al., 2020) and the cleaned Arxiv Math proof-pile dataset (Azerbayev et al., 2022). We use the test split of PG19 (Rae et al., 2020), consisting of 100 documents. For the proof-pile dataset, we also use the test split of it for evaluation. We follow Position Interpolation (Chen et al., 2023) for proof-pile data processing. We evaluate perplexity by using a sliding window approach with S = 256, following (Press et al., 2022).\n",
       "\n",
       " 4.2 MAIN RESULTS\n",
       "\n",
       "Long-sequence Language Modeling.\n",
       "\n",
       "In Table 3, we report the perplexity for our models and baseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain traini...<br>**Metadata:** {'user': 'bob', 'file_size': '1168720', 'last_modified_at': '2024-07-06T18:19:04', 'file_path': 'longlora.pdf', 'file_name': 'longlora.pdf', 'pipeline_id': '2532c4c9-f216-445d-8766-e7dfa22397d4'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try filtering for a specific user\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "# specify the user you want to filter by here \n",
    "filters = MetadataFilters.from_dicts(\n",
    "    [{\"key\": \"user\", \"operator\": \"==\", \"value\": \"bob\"}]\n",
    "    # []\n",
    ")\n",
    "retriever = index.as_retriever(\n",
    "    rerank_top_n=2, \n",
    "    filters=filters\n",
    ")\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n",
    "response = query_engine.query(\"Tell me about the abstract of this paper\")\n",
    "print(str(response))\n",
    "\n",
    "for n in response.source_nodes:\n",
    "    display_source_node(n, source_length=1000, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabe94c-ae69-4c33-a11f-edc7f4015c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
