{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ⚠️ Important Notice\n\nThis notebook (and repository) is deprecated.\n\nFor the latest python examples, please refer to the `llama-cloud-services` repository examples: \nhttps://github.com/run-llama/llama_cloud_services/tree/main/examples\n\n---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaParse + LlamaCloud + AWS Bedrock Cookbook\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/10k_apple_tesla/demo_file_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "In this notebook we demonstrate demonstrate how to build a RAG application using LlamaParse, LlamaCloud, embedding models and LLMs supported on AWS Bedrock.\n",
        "\n",
        "Here are the steps involved:\n",
        "\n",
        "1. Install the packages and setup API keys. \n",
        "2. Download Apple-10K 2023 SEC filing.\n",
        "3. Parse the documents using LlamaParse.\n",
        "4. Create a pipeline/ Index on LlamaCloud.\n",
        "5. Upload the document to Index with `amazon.titan-embed-text-v1` embedding.\n",
        "6. Connect to the Index.\n",
        "7. Initiate LLM.\n",
        "8. Create `query_engine`.\n",
        "9. Query the index using `query_engine`.\n",
        "\n",
        "[LlamaCloud](https://docs.cloud.llamaindex.ai/), [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and Setup API Keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installation\n",
        "\n",
        "Here we install following packages:\n",
        "\n",
        "1. `llama-index`: Core package for OSS orchestration.\n",
        "2. `llama-index-llms-bedrock-converse`: To utilize Bedrock LLMs.\n",
        "3. `llama-index-indices-managed-llama-cloud`: For managing indices on LlamaCloud.\n",
        "4. `llama-parse`: For parsing documents efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-llms-bedrock-converse\n",
        "!pip install llama-index-indices-managed-llama-cloud\n",
        "!pip install llama-parse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup API Keys\n",
        "\n",
        "Here we setup `LLAMA_CLOUD_API_KEY` for managing the index on LlamaCloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "# API access to llama-cloud\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"<LLAMACLOUD API KEY>\" # Get your API key from https://cloud.llamaindex.ai/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download files\n",
        "\n",
        "Here we download `Apple-10K` 2023 SEC filings and use it for our demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-28 16:05:55--  https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf\n",
            "Resolving s2.q4cdn.com (s2.q4cdn.com)... 181.41.142.154\n",
            "Connecting to s2.q4cdn.com (s2.q4cdn.com)|181.41.142.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 714094 (697K) [application/pdf]\n",
            "Saving to: ‘data/apple_2023.pdf’\n",
            "\n",
            "data/apple_2023.pdf 100%[===================>] 697.36K  4.37MB/s    in 0.2s    \n",
            "\n",
            "2024-11-28 16:05:57 (4.37 MB/s) - ‘data/apple_2023.pdf’ saved [714094/714094]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download Apple \n",
        "!mkdir -p data\n",
        "!wget \"https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf\" -O data/apple_2023.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse the document.\n",
        "\n",
        "Here we use `LlamaParse` to parse the downloaded `Apple` 10K-SEC filings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started parsing the file under job_id d6c339e3-9014-4139-afce-48c1ffbaa098\n"
          ]
        }
      ],
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
        "    num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
        "    verbose=True,\n",
        "    language=\"en\",  # Optionally you can define a language, default=en\n",
        ")\n",
        "\n",
        "# sync\n",
        "documents = parser.load_data(\"data/apple_2023.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Pipeline/ Index on LlamaCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LlamaCloud Client\n",
        "\n",
        "We will connect to `LlamaCloud` client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_cloud.client import LlamaCloud\n",
        "\n",
        "client = LlamaCloud(token=os.environ[\"LLAMA_CLOUD_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create LlamaCloud Pipeline/ Index\n",
        "\n",
        "We need `embedding_config` and `transform_config` to create a pipeline.\n",
        "\n",
        "`embedding_config` - Sets up the embedding model details required to configure and create the pipeline.\n",
        "\n",
        "`transform_config` - Configures the `chunk_size` and `chunk_overlap` parameters required for the RAG application.\n",
        "\n",
        "We will use the `amazon.titan-embed-text-v1` embedding model available on AWS Bedrock. To access it, you will need the following credentials: `region_name`, `aws_access_key_id`, and `aws_secret_access_key`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformation auto config\n",
        "transform_config = {\n",
        "    'mode': 'auto',\n",
        "    'config': {\n",
        "        'chunk_size': 1024, # editable\n",
        "        'chunk_overlap': 20 # editable\n",
        "    }\n",
        "}\n",
        "\n",
        "embedding_config = {\n",
        "      'type': 'BEDROCK_EMBEDDING',\n",
        "      'component': {\n",
        "          'region_name': '<REGION NAME>',\n",
        "          'aws_access_key_id': '<AWS ACCESS KEY ID>',\n",
        "          'aws_secret_access_key': '<AWS SECRET ACCESS KEY>',\n",
        "          'model': 'amazon.titan-embed-text-v1',\n",
        "      }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = {\n",
        "  'name': 'apple_2023', # pipeline/ index name\n",
        "  'transform_config': transform_config,\n",
        "  'embedding_config': embedding_config,\n",
        "  'data_sink_id': None\n",
        "}\n",
        "\n",
        "pipeline = client.pipelines.upsert_pipeline(request=pipeline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload the Documents\n",
        "\n",
        "Here we use the parsed document and upload it to the LlamaCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_cloud.types import CloudDocumentCreate\n",
        "\n",
        "text = \"\\n\\n\".join([doc.text for doc in documents])\n",
        "\n",
        "documents = [\n",
        "CloudDocumentCreate(\n",
        "text=text,\n",
        "metadata={\"filename\": \"apple_2023.pdf\", \"file_path\": \"data/apple_2023.pdf\"},\n",
        ")\n",
        "]\n",
        "\n",
        "documents = client.pipelines.create_batch_pipeline_documents(pipeline.id, request=documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check status if its uploaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ManagedIngestionStatus.SUCCESS\n"
          ]
        }
      ],
      "source": [
        "status = client.pipelines.get_pipeline_status(pipeline.id)\n",
        "print(status.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to the Index\n",
        "\n",
        "We will connect to the LlamaCloud pipeline or index that has been created. You can get the `project_name` and `organization_id` from your LlamaCloud index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
        "\n",
        "index = LlamaCloudIndex(\n",
        "  name=\"apple_2023\", \n",
        "  project_name=\"<PROJECT NAME>\",\n",
        "  organization_id=\"<ORGANIZATION ID>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the LLM\n",
        "\n",
        "Here, we will initiate the supported LLM on AWS Bedrock LLM. You can refer to the [AWS Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) to explore the available LLMs.\n",
        "\n",
        "To access it, you will need the following credentials: `region_name`, `aws_access_key_id`, `aws_secret_access_key` and `aws_session_token`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.bedrock_converse import BedrockConverse\n",
        "\n",
        "llm = BedrockConverse(model=\"<MODEL ID>\", \n",
        "                      region_name=\"<REGION NAME>\", \n",
        "                      aws_access_key_id=\"<AWS ACCESS KEY ID>\", \n",
        "                      aws_secret_access_key=\"<AWS SECRET ACCESS KEY>\", \n",
        "                      aws_session_token=\"<AWS SESSION TOKEN>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create QueryEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Querying\n",
        "\n",
        "We will test out with a query using the created `QueryEngine`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The revenue of Apple in 2023 was $383.3 billion.\n"
          ]
        }
      ],
      "source": [
        "query = \"what is the revenue of Apple in 2023?\"\n",
        "response = query_engine.query(query)\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llamacloud",
      "language": "python",
      "name": "llamacloud"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}