{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Control demo with LlamaCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install core packages, download files. You will need to upload these documents to LlamaCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-embeddings-openai\n",
    "!pip install llama-index-question-gen-openai\n",
    "!pip install llama-index-postprocessor-flag-embedding-reranker\n",
    "!pip install git+https://github.com/FlagOpen/FlagEmbedding.git\n",
    "!pip install llama-parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some OpenAI and LlamaParse details. The OpenAI LLM is used for response synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI API for embeddings/llms\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud.client import LlamaCloud\n",
    "\n",
    "client = LlamaCloud(token=os.environ[\"LLAMA_CLOUD_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Sharepoint Programatically\n",
    "Please for more details about how to setup the permissions check our docuentation [here](https://docs.cloud.llamaindex.ai/llamacloud/integrations/data_sources/sharepoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud.types import CloudSharepointDataSource\n",
    "\n",
    "ds = {\n",
    "    'name': '<your-data-source-name>',\n",
    "    'source_type': 'MICROSOFT_SHAREPOINT', \n",
    "    'component': CloudSharepointDataSource(\n",
    "        site_name='<site_name>',\n",
    "        folder_path='<folder_path>',  # optional\n",
    "        client_id='<client_id>',\n",
    "        client_secret='<client_secret>',\n",
    "        tenant_id='<tenant_id>',\n",
    "    )\n",
    "}\n",
    "\n",
    "data_source = client.data_sources.create_data_source(request=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Transformations/Embeddings Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding config\n",
    "embedding_config = {\n",
    "    'type': 'OPENAI_EMBEDDING',\n",
    "    'component': {\n",
    "        'api_key': os.environ[\"OPENAI_API_KEY\"], # editable\n",
    "        'model_name': 'text-embedding-ada-002' # editable\n",
    "    }\n",
    "}\n",
    "\n",
    "# Transformation auto config\n",
    "transform_config = {\n",
    "    'mode': 'auto',\n",
    "    'config': {\n",
    "        'chunk_size': 1024, # editable\n",
    "        'chunk_overlap': 20 # editable\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = {\n",
    "    'name': 'test-pipeline',\n",
    "    'embedding_config': embedding_config,\n",
    "    'transform_config': transform_config,\n",
    "}\n",
    "\n",
    "pipeline = client.pipelines.upsert_pipeline(request=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Sharepoint Data Source to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = [\n",
    "  {\n",
    "    'data_source_id': data_source.id,\n",
    "    'sync_interval': 43200.0 # Optional, scheduled sync frequency in seconds. In this case, every 12 hours.\n",
    "  }\n",
    "]\n",
    "\n",
    "pipeline_data_sources = client.pipelines.add_data_sources_to_pipeline(pipeline.id, request=data_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync Pipeline\n",
    "This triggers the data-source run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(id='477e2c1b-6a31-4f95-8b8c-f86a736cef30', created_at=datetime.datetime(2025, 2, 6, 1, 10, 43, 365914, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 2, 6, 1, 10, 43, 365914, tzinfo=datetime.timezone.utc), name='test-pipeline', project_id='177261ea-1af1-4a47-9029-15ed38c0cea8', embedding_model_config_id=None, pipeline_type=<PipelineType.MANAGED: 'MANAGED'>, managed_pipeline_id=None, embedding_config=PipelineEmbeddingConfig_OpenaiEmbedding(component=OpenAiEmbedding(model_name='text-embedding-ada-002', embed_batch_size=10, num_workers=None, additional_kwargs={}, api_key='********MUwA', api_base='https://api.openai.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True, dimensions=None, class_name='OpenAIEmbedding'), type='OPENAI_EMBEDDING'), configured_transformations=[], config_hash=PipelineConfigurationHashes(embedding_config_hash='e23a92ac03041136e80e731de4f1f15b4aeb5a5f24c3c2ea4d', parsing_config_hash='e0d357e5d44a8e1364c6f6c6f38c261c676e7d1289157ee273', transform_config_hash='84032cd089f78343a74e25f97a30c5f7419a67f43d65ada2f1'), transform_config=PipelineTransformConfig_Auto(chunk_size=1024, chunk_overlap=200, mode='auto'), preset_retrieval_parameters=PresetRetrievalParams(dense_similarity_top_k=30, dense_similarity_cutoff=0.0, sparse_similarity_top_k=30, enable_reranking=True, rerank_top_n=6, alpha=0.5, search_filters=None, files_top_k=1, retrieval_mode=<RetrievalMode.CHUNKS: 'chunks'>, retrieve_image_nodes=False, class_name='base_component'), eval_parameters=EvalExecutionParams(llm_model=<SupportedLlmModelNames.GPT_4_O: 'GPT_4O'>, qa_prompt_tmpl='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), llama_parse_parameters=LlamaParseParameters(languages=[<ParserLanguages.EN: 'en'>], parsing_instruction='', disable_ocr=False, annotate_links=False, disable_reconstruction=False, disable_image_extraction=False, invalidate_cache=False, output_pdf_of_document=False, do_not_cache=False, fast_mode=False, skip_diagonal_text=False, gpt_4_o_mode=False, gpt_4_o_api_key='', do_not_unroll_columns=False, extract_layout=False, html_make_all_elements_visible=False, html_remove_navigation_elements=False, html_remove_fixed_elements=False, guess_xlsx_sheet_name=False, page_separator=None, bounding_box='', bbox_top=None, bbox_right=None, bbox_bottom=None, bbox_left=None, target_pages='', use_vendor_multimodal_model=False, vendor_multimodal_model_name='', vendor_multimodal_api_key='', page_prefix='', page_suffix='', webhook_url='', take_screenshot=False, is_formatting_instruction=True, premium_mode=False, continuous_mode=False, s_3_input_path='', input_s_3_region='', s_3_output_path_prefix='', output_s_3_region='', project_id=None, azure_openai_deployment_name=None, azure_openai_endpoint=None, azure_openai_api_version=None, azure_openai_key=None, input_url=None, http_proxy=None, auto_mode=False, auto_mode_trigger_on_regexp_in_page=None, auto_mode_trigger_on_text_in_page=None, auto_mode_trigger_on_table_in_page=False, auto_mode_trigger_on_image_in_page=False, structured_output=False, structured_output_json_schema=None, structured_output_json_schema_name=None, max_pages=None, max_pages_enforced=None, extract_charts=False, formatting_instruction=None, complemental_formatting_instruction=None, content_guideline_instruction=None, spreadsheet_extract_sub_tables=False, job_timeout_in_seconds=None, job_timeout_extra_time_per_page_in_seconds=None, strict_mode_image_extraction=False, strict_mode_image_ocr=False, strict_mode_reconstruction=False, strict_mode_buggy_font=False, ignore_document_elements_for_layout_detection=False, output_tables_as_html=False, internal_is_screenshot_job=False), data_sink=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.pipelines.sync_pipeline(pipeline.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LlamaCloud Chunk Retriever over Documents\n",
    "\n",
    "In this section we define a chunk-level LlamaCloud Retriever over these documents.\n",
    "\n",
    "The chunk-level LlamaCloud retriever is our default retriever that returns chunks via hybrid search + reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "import os\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=pipeline.name,\n",
    "  project_id=pipeline.project_id,\n",
    "  api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"],\n",
    "  organization_id=\"04db4a56-04e3-43c5-aef5-0f39f1653dc8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define chunk retriever\n",
    "\n",
    "The chunk-level retriever does vector search with a final reranked set of `rerank_top_n=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# resolver user id/group id through sharepoint/db here \n",
    "# obs: you can also filter by the groups of the user\n",
    "FILTER_BY_USER_ID = \"11\" # editable\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"allowed_siteUser_ids\", operator=FilterOperator.IN, value=[FILTER_BY_USER_ID]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chunk_retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    rerank_top_n=5,\n",
    "    filters=filters\n",
    ")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
    "    chunk_retriever, \n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Agent\n",
    "\n",
    "In this section we build an agent that takes in both file-level and chunk-level query engines as tools. It decides which query engine to call depending on the nature of this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool, ToolMetadata, QueryEngineTool\n",
    "\n",
    "\n",
    "# this variable tells the agent specific properties about your document.\n",
    "doc_metadata_extra_str = \"\"\"\\\n",
    "Each document represents a complete 10K report for a given year (e.g. Apple in 2019). \n",
    "Here's an example of relevant documents:\n",
    "1. apple_2019.pdf\n",
    "\"\"\"\n",
    "\n",
    "tool_chunk_description = f\"\"\"\\\n",
    "Synthesizes an answer to your question by feeding in a relevant chunk as context. Best used for questions that are more pointed in nature.\n",
    "Do NOT use if the question asks seems to require a general summary of any given document. Use the doc_query_engine instead for that purpose.\n",
    "\n",
    "Below we give details on the format of each document:\n",
    "{doc_metadata_extra_str}\n",
    "\"\"\"\n",
    "\n",
    "tool_chunk = QueryEngineTool(\n",
    "    query_engine=query_engine_chunk,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"chunk_query_engine\",\n",
    "        description=tool_chunk_description\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm_agent = OpenAI(model=\"gpt-4o\")\n",
    "agent = FunctionCallingAgentWorker.from_tools(\n",
    "    [tool_chunk], llm=llm_agent, verbose=True\n",
    ").as_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the revenue for Apple in 2019?\n",
      "=== Calling Function ===\n",
      "Calling function: chunk_query_engine with args: {\"input\": \"Apple revenue in 2019\"}\n",
      "=== Function Output ===\n",
      "Apple's total net sales in 2019 amounted to $260.174 billion, which represented a 2% decrease compared to 2018. The revenue was primarily driven by various product categories, including iPhone, Mac, iPad, Wearables, Home and Accessories, and Services.\n",
      "=== LLM Response ===\n",
      "Apple's total net sales in 2019 amounted to $260.174 billion, which represented a 2% decrease compared to 2018.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me the revenue for Apple in 2019?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the revenue for Apple in 2020?\n",
      "=== Calling Function ===\n",
      "Calling function: chunk_query_engine with args: {\"input\": \"Apple revenue in 2020\"}\n",
      "=== Function Output ===\n",
      "Apple's total net sales in 2020 amounted to $274.5 billion, reflecting a 6% increase compared to 2019. The revenue was primarily driven by higher sales in Services and Wearables, Home and Accessories.\n",
      "=== LLM Response ===\n",
      "Apple's total net sales in 2020 amounted to $274.5 billion, reflecting a 6% increase compared to 2019.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me the revenue for Apple in 2020?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (systra)",
   "language": "python",
   "name": "bp_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
