{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007f366e-5098-42fa-baf1-b709f5845301",
   "metadata": {},
   "source": [
    "# Ad-Hoc Experimentation with Chunking\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/experimentation/chunk_size_adhoc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "A key parameter for RAG pipelines is chunking - the chunk size affects the accuracy of your overall RAG pipeline.\n",
    "\n",
    "Unlike retrieval and query-time parameters though, chunking is a little harder to experiment with. This is because changing your chunking configuration requires reindexing your data, which can be tedious to experiment with.\n",
    "\n",
    "LlamaCloud provides easy ways for you to perform **ad-hoc** experimentation over chunking. \n",
    "1. First, validate if a given query is correct over an index with our index playground features.\n",
    "2. If not correct, you can clone an index with a click of a button, and set different chunking/ingestion parameters more broadly.as\n",
    "3. Try the same query over the playground features again and see if it leads to the right results. \n",
    "\n",
    "**NOTE**: More structured experimentation capabilities here are coming soon! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483a0e1-3344-4527-a6b5-6f6b829915a2",
   "metadata": {},
   "source": [
    "## Setup a LlamaCloud Index\n",
    "\n",
    "Download the three ICLR 2024 papers below. Then, create a new LlamaCloud Index in the UI and upload these three files through drag/drop.\n",
    "\n",
    "In the \"Transform Settings\" - make sure to select the \"Auto\" tab with a chunk size of 512. This will be our starting point - we'll analyze how well different queries perform on this index, and iterate on indexing parameters after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e40b962-b6e1-46c7-9a6b-cbe14d21ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f94da7d-b18a-4ce7-90eb-f6ef99e20a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a99a0f-4463-46a8-a847-f390ee578ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: insert your own `name`, `project_name`, and `api_key`\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=\"research_papers_512\", \n",
    "  project_name=\"llamacloud_demo\",\n",
    "  # api_key=\"llx-\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ea13c-95db-4790-b0c2-54917e20c4f6",
   "metadata": {},
   "source": [
    "## Ad-hoc Test a Question\n",
    "\n",
    "You want to test this index by sanity-checking with a question you already know the answer to. In this example, we want to understand the core features of the SWE-Bench dataset in page 3, as shown in the image below.\n",
    "\n",
    "![](chunk_size_adhoc_images/source_chunk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974d6f4e-9c74-47e7-8ef4-6fc535510471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set re-ranking top-n to 3 \n",
    "query_engine = index.as_query_engine(rerank_top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "186eb4cb-d809-434d-80fb-22610c32ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING: Tell me about the core features of SWE-bench\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the core features of SWE-bench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8ac742-0828-44c4-839f-9454af3899a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWE-bench is a benchmark designed to evaluate language models (LMs) in a realistic software engineering setting. Its core features include:\n",
      "\n",
      "1. **Real-world Software Engineering Tasks**: Each task involves a large and complex codebase along with a detailed issue description, requiring sophisticated skills and knowledge akin to those of experienced software engineers.\n",
      "\n",
      "2. **Continually Updatable**: The benchmark can be easily extended with new task instances from any Python repository on GitHub, ensuring a continual supply of fresh challenges that were not part of the models' training data.\n",
      "\n",
      "3. **Diverse Long Inputs**: Issue descriptions are typically lengthy and detailed, and the codebases contain many thousands of files. This requires models to identify the specific lines that need modification among a vast amount of context.\n",
      "\n",
      "4. **Robust Evaluation**: Each task instance includes at least one fail-to-pass test to verify the solution, with many instances having multiple such tests. Additionally, a median of 51 other tests are run to ensure that prior functionality is maintained.\n",
      "\n",
      "5. **Realistic Setting**: Unlike traditional benchmarks that involve short and contrived problems, SWE-bench uses user-submitted issues and solutions from popular GitHub repositories, providing a more realistic and challenging environment for evaluation.\n",
      "\n",
      "6. **Execution-based Evaluation**: The benchmark uses the repositoryâ€™s testing framework to evaluate the revised codebase, ensuring that the proposed solutions are practical and effective.\n",
      "\n",
      "These features collectively make SWE-bench a comprehensive and challenging benchmark for evaluating the capabilities of language models in real-world software engineering tasks.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeef466-a95e-4211-af53-532236a7ea36",
   "metadata": {},
   "source": [
    "### Analyze Results\n",
    "\n",
    "**NOTE**: Assuming we have knowledge of the ground-truth, we know this answer isn't quite complete. \n",
    "Instead of the notebook, we can also quickly validate this over the chat UI and retrieval UI in the \"Playground\" section of the index page. \n",
    "Try clicking into the LlamaCloud playground, and enter the question above in the chat UI, and look at the response and set of retrieved nodes.\n",
    "\n",
    "![](chunk_size_adhoc_images/chat_ui_test.png)\n",
    "\n",
    "Now enter the same question into the retrieval UI, which lets you not only see the chunks but also the source document for each chunk. \n",
    "\n",
    "![](chunk_size_adhoc_images/retrieval_ui_test.png)\n",
    "\n",
    "Clicking \"View in File\" on the first chunk will let you see how the source document is parsed and chunked. Since we have knowledge of the ground-truth, we can check to see if the ground-truth context is chunked in a cohesive manner - in this case we can see that the relevant section is cutoff, and Node 21 is not in the retrieved set at all. \n",
    "\n",
    "![](chunk_size_adhoc_images/view_chunks.png)\n",
    "\n",
    "You'll notice that the relevant paragraph is broken up into two chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a8b94-27b5-46bd-b066-f21d1146d02b",
   "metadata": {},
   "source": [
    "## Experimenting with Chunk Sizes\n",
    "\n",
    "You can tackle the above issue in a variety of ways. For instance, you can keep the chunk sizes fixed and only tune retrieval parameters, like top-k, hybrid search, reranking, etc. There are tradeoffs to only tuning retrieval though. Increasing top-k can lead to increased latency and cost.\n",
    "\n",
    "Chunk sizes are a little harder than retrieval parameters to experiment with, since changing it requires retriggering an index run.\n",
    "\n",
    "With LlamaCloud, we can easily create a new index with a different chunking configuration and see if the retrieved results change. \n",
    "\n",
    "First, on the Index page click the \"Copy\" button to duplicate the index, and give it a new name. \n",
    "\n",
    "Click into the new index, rename it as you wish, and then click \"Edit\" and change the chunking configuration to page-level chunking.\n",
    "1. Click into \"Manual\"\n",
    "2. Click \"Page\" segmentation in \"Segmentation Configuration\" to segment by page at the top-level\n",
    "3. In \"Chunking Configuration\", select None for the mode.\n",
    "\n",
    "![](chunk_size_adhoc_images/transform_config.png)\n",
    "\n",
    "Click \"Save\" to set the new index settings and retrigger a run of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df214f58-b010-452b-8787-3b985b32e42c",
   "metadata": {},
   "source": [
    "Let's try testing this same index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815d5f40-ae6d-4944-a243-fd99d0b05ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=\"research_papers_page\", \n",
    "  project_name=\"llamacloud_demo\",\n",
    "  # api_key=\"llx-\"\n",
    ")\n",
    "query_engine = index.as_query_engine(rerank_top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71676364-ef94-4a15-bb6d-ddc69a7821a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING: Tell me about the core features of SWE-bench\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the core features of SWE-bench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8331e3d1-b537-49ca-a57b-bcab179a4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWE-bench is a benchmark designed to evaluate language models (LMs) in realistic software engineering settings. It features several core attributes:\n",
      "\n",
      "1. **Real-world Software Engineering Tasks**: It involves large and complex codebases with detailed issue descriptions, requiring sophisticated skills and knowledge akin to those of experienced software engineers.\n",
      "\n",
      "2. **Continually Updatable**: The collection process can be applied to any Python repository on GitHub with minimal human intervention, allowing for a continual supply of new task instances.\n",
      "\n",
      "3. **Diverse Long Inputs**: Issue descriptions are typically long and detailed, and codebases contain many thousands of files, necessitating the identification of specific lines that need editing.\n",
      "\n",
      "4. **Robust Evaluation**: Each task instance includes at least one fail-to-pass test to ensure the model addresses the problem, with additional tests to check for proper maintenance of prior functionality.\n",
      "\n",
      "5. **Cross-context Code Editing**: Unlike benchmarks that constrain edit scope, SWE-bench requires generating revisions in multiple locations of a large codebase, challenging models to handle repository-scale code editing.\n",
      "\n",
      "6. **Wide Scope for Possible Solutions**: It allows for creative freedom in generating novel solutions, providing a level playing field to compare various approaches, from retrieval and long-context models to decision-making agents.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f2ccf3-a456-4c5f-a75c-db1b151e798b",
   "metadata": {},
   "source": [
    "**Result**: Turns out that page-level chunking helps you give back the main result. This is not unexpected, since page-level chunking preserves context across an entire page. \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "If you are aiming for development velocity, you can keep the page-level chunking as a reasonable baseline and build something that \"just works\". If you are looking to iteratively improve chunking further, consider running the two LlamaCloud indexes you've defined over a more structured dataset and evaluating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ef8e0-1280-4647-b2b8-21819fbc0bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
