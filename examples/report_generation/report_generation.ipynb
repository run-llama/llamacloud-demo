{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Report Generation\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/report_generation/report_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this notebook we show you how to perform financial report generation with LlamaCloud consisting of text and tables, given an existing bank of reports.\n",
    "\n",
    "LlamaCloud provides advanced retrieval endpoints allowing you to fetch context from complex financial reports consisting of text, tables, and sometimes images/diagrams.\n",
    "\n",
    "We build an agentic workflow on top of LlamaCloud consisting of researcher and writer steps in order to generate the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "|\n",
    "Install core packages, download 10k files from Apple and Tesla.\n",
    "\n",
    "You will need to upload these documents to LlamaCloud. For best results, we recommend: \n",
    "- Setting Parse settings to \"Accurate\" mode, \"Premium\" mode, or \"3rd Party multimodal\" \n",
    "- Setting the \"Segmentation Configuration\" to \"Page\" and the \"Chunking Configuration\" to None. This will give you page-level chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-embeddings-openai\n",
    "!pip install llama-index-question-gen-openai\n",
    "!pip install llama-index-postprocessor-flag-embedding-reranker\n",
    "!pip install git+https://github.com/FlagOpen/FlagEmbedding.git\n",
    "!pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "# download Apple \n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf\" -O data/apple_2023.pdf\n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2022/q4/_10-K-2022-(As-Filed).pdf\" -O data/apple_2022.pdf\n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2021/q4/_10-K-2021-(As-Filed).pdf\" -O data/apple_2021.pdf\n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2020/ar/_10-K-2020-(As-Filed).pdf\" -O data/apple_2020.pdf\n",
    "!wget \"https://www.dropbox.com/scl/fi/i6vk884ggtq382mu3whfz/apple_2019_10k.pdf?rlkey=eudxh3muxh7kop43ov4bgaj5i&dl=1\" -O data/apple_2019.pdf\n",
    "\n",
    "# download Tesla\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf\" -O data/tesla_2023.pdf\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000095017023001409/tsla-20221231-gen.pdf\" -O data/tesla_2022.pdf\n",
    "!wget \"https://www.dropbox.com/scl/fi/ptk83fmye7lqr7pz9r6dm/tesla_2021_10k.pdf?rlkey=24kxixeajbw9nru1sd6tg3bye&dl=1\" -O data/tesla_2021.pdf\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459021004599/tsla-10k_20201231-gen.pdf\" -O data/tesla_2020.pdf\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459020004475/tsla-10k_20191231-gen_0.pdf\" -O data/tesla_2019.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some OpenAI and LlamaParse details. The OpenAI LLM is used for response synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI API for embeddings/llms\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup embedding/LLM model\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Documents into LlamaCloud\n",
    "\n",
    "The first order of business is to download the 5 Apple and Tesla 10Ks and upload them into LlamaCloud.\n",
    "\n",
    "You can easily do this by creating a pipeline and uploading docs via the \"Files\" mode.\n",
    "\n",
    "After this is done, proceed to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LlamaCloud Retriever over Documents\n",
    "\n",
    "In this section we define a chunk-level LlamaCloud Retriever over these documents. The chunk-level LlamaCloud retriever is our default retriever that returns chunks via hybrid search + reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "import os\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "  name=\"apple_tesla_demo_2\",\n",
    "  project_name=\"llamacloud_demo\",\n",
    "  api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_retriever = index.as_retriever(\n",
    "    retrieval_mode=\"chunks\",\n",
    "    rerank_top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import List\n",
    "\n",
    "# function tools\n",
    "def chunk_retriever_fn(query: str) -> List[NodeWithScore]:\n",
    "    \"\"\"Retrieves a small set of relevant document chunks from the corpus.\n",
    "\n",
    "    ONLY use for research questions that want to look up specific facts from the knowledge corpus,\n",
    "    and don't need entire documents.\n",
    "\n",
    "    \"\"\"\n",
    "    return chunk_retriever.retrieve(query)\n",
    "\n",
    "chunk_retriever_tool = FunctionTool.from_defaults(fn=chunk_retriever_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Report Generation Workflow\n",
    "\n",
    "Now that we've defined the retrievers, we're ready to build the report generation workflow.\n",
    "\n",
    "The workflow contains roughly the following steps:\n",
    "\n",
    "1. **Research Gathering**: Perform a function calling loop where the agent tries to reason about what tool to call (chunk-level or document-level retrieval) in order to gather more information. All information is shared to a dictionary that is propagated throughout each step. The tools return an indication of the type of information returned to the agent. After the agent feels like it's gathered enough information, move on to the next phase.\n",
    "2. **Report Generation**: Generate a research report given the pooled research. For now, try to stuff as much information into the context window through the summary index.\n",
    "\n",
    "This implementation is inspired by our [Function Calling Agent](https://docs.llamaindex.ai/en/stable/examples/workflow/function_calling_agent/) workflow implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "class TextBlock(BaseModel):\n",
    "    \"\"\"Text block.\"\"\"\n",
    "\n",
    "    text: str = Field(..., description=\"The text for this block.\")\n",
    "\n",
    "\n",
    "class TableBlock(BaseModel):\n",
    "    \"\"\"Table block.\"\"\"\n",
    "\n",
    "    caption: str = Field(..., description=\"Caption of the table.\")\n",
    "    col_names: List[str] = Field(..., description=\"Names of the columns.\")\n",
    "    rows: List[List] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"List of rows. Each row is a data entry tuple, \"\n",
    "            \"where each element of the tuple corresponds positionally to the column name.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def to_df(self) -> pd.DataFrame:\n",
    "        \"\"\"To dataframe.\"\"\"\n",
    "        df = pd.DataFrame(self.rows, columns=self.col_names)\n",
    "        df.style.set_caption(self.caption)\n",
    "        return df\n",
    "\n",
    "\n",
    "class ReportOutput(BaseModel):\n",
    "    \"\"\"Data model for a report.\n",
    "\n",
    "    Can contain a mix of text and table blocks. Use table blocks to present any quantitative metrics and comparisons.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    blocks: List[TextBlock | TableBlock] = Field(\n",
    "        ..., description=\"A list of text and table blocks.\"\n",
    "    )\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Render as formatted text within a jupyter notebook.\"\"\"\n",
    "        for b in self.blocks:\n",
    "            if isinstance(b, TextBlock):\n",
    "                display(Markdown(b.text))\n",
    "            else:\n",
    "                display(b.to_df())\n",
    "\n",
    "\n",
    "report_gen_llm = OpenAI(\n",
    "    model=\"gpt-4o\", \n",
    "    # system_prompt=report_gen_system_prompt, \n",
    "    max_tokens=2048,\n",
    ")\n",
    "report_gen_sllm = report_gen_llm.as_structured_llm(output_cls=ReportOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_gen_sllm.metadata.context_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Workflow\n",
    "\n",
    "from typing import Any, List\n",
    "from operator import itemgetter\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.llms.structured_llm import StructuredLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.tools import ToolSelection\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, Context, step\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import TreeSummarize, CompactAndRefine\n",
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: List[ChatMessage]\n",
    "\n",
    "\n",
    "class ChunkRetrievalEvent(Event):\n",
    "    tool_call: ToolSelection\n",
    "    \n",
    "\n",
    "class ReportGenerationEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "report_gen_system_prompt = \"\"\"\\\n",
    "You are a report generation assistant tasked with producing a well-formatted context given parsed context.\n",
    "You will be given context from one or more reports that take the form of parsed text + tables\n",
    "You are responsible for producing a report with interleaving text and tables - in the format of interleaving text and \"table\" blocks.\n",
    "You MUST output your response as a tool call in order to adhere to the required output format. Do NOT give back normal text.\n",
    "\n",
    "Here is an example of a toy valid tool call - note the text and table block:\n",
    "```\n",
    "{\n",
    "    \"blocks\": [\n",
    "        {\n",
    "            \"text\": \"A report on cities\"\n",
    "        },\n",
    "        {\n",
    "            \"caption\": \"Comparison of CityA vs. CityB\",\n",
    "            \"col_names\": [\n",
    "              \"\",\n",
    "              \"Population\",\n",
    "              \"Country\",\n",
    "            ],\n",
    "            \"rows\": [\n",
    "              [\n",
    "                \"CityA\",\n",
    "                \"1,000,000\",\n",
    "                \"USA\"\n",
    "              ],\n",
    "              [\n",
    "                \"CityB\",\n",
    "                \"2,000,000\",\n",
    "                \"Mexico\"\n",
    "              ]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "report_gen_user_prompt = \"\"\"\\\n",
    "Here is a list of stored context that you can use to generate the report.\n",
    "\n",
    "-----------------------------\n",
    "{context_str}\n",
    "-----------------------------\n",
    "\n",
    "Here is the user task: {query_str}\n",
    "\n",
    "Generate the report below.\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_REPORT_GEN_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", report_gen_system_prompt),\n",
    "    (\"user\", report_gen_user_prompt),\n",
    "])\n",
    "\n",
    "class ReportGenerationAgent(Workflow):\n",
    "    \"\"\"Report generation agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_retriever_tool: BaseTool,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        report_gen_sllm: StructuredLLM | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.chunk_retriever_tool = chunk_retriever_tool\n",
    "\n",
    "        self.llm = llm or OpenAI()\n",
    "        self.summarizer = CompactAndRefine(llm=self.llm)\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        self.report_gen_sllm = report_gen_sllm or self.llm.as_structured_llm(\n",
    "            ReportOutput, system_prompt=report_gen_system_prompt\n",
    "        )\n",
    "        self.report_gen_prompt = DEFAULT_REPORT_GEN_PROMPT\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.sources = []\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def prepare_chat_history(self, ctx: Context, ev: StartEvent) -> InputEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        ctx.data[\"stored_chunks\"] = []\n",
    "        ctx.data[\"query\"] = ev.input\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ChunkRetrievalEvent | ReportGenerationEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        response = await self.llm.achat_with_tools(\n",
    "            [self.chunk_retriever_tool],\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "        self.memory.put(response.message)\n",
    "\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "        for tool_call in tool_calls:\n",
    "            print(f\"Tool call: {tool_call}\")\n",
    "        if not tool_calls:\n",
    "            # all the content should be stored in the context, so just pass along input\n",
    "            return ReportGenerationEvent(input=ev.input)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.tool_name == self.chunk_retriever_tool.metadata.name:\n",
    "                return ChunkRetrievalEvent(tool_call=tool_call)\n",
    "            else:\n",
    "                return StopEvent(result={\"response\": \"Invalid tool.\"})\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def handle_retrieval(\n",
    "        self, ctx: Context, ev: ChunkRetrievalEvent\n",
    "    ) -> InputEvent:\n",
    "        \"\"\"Handle retrieval.\n",
    "\n",
    "        Store retrieved chunks, and go back to agent reasoning loop.\n",
    "\n",
    "        \"\"\"\n",
    "        query = ev.tool_call.tool_kwargs[\"query\"]\n",
    "        if isinstance(ev, ChunkRetrievalEvent):\n",
    "            retrieved_chunks = self.chunk_retriever_tool(query).raw_output\n",
    "        else:\n",
    "            retrieved_chunks = self.doc_retriever_tool(query).raw_output\n",
    "        ctx.data[\"stored_chunks\"].extend(retrieved_chunks)\n",
    "\n",
    "        # synthesize an answer given the query to return to the LLM.\n",
    "        response = self.summarizer.synthesize(query, nodes=retrieved_chunks)\n",
    "        self.memory.put(\n",
    "            ChatMessage(\n",
    "                role=\"tool\",\n",
    "                content=str(response),\n",
    "                additional_kwargs={\n",
    "                    \"tool_call_id\": ev.tool_call.tool_id,\n",
    "                    \"name\": ev.tool_call.tool_name,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # send input event back with updated chat history\n",
    "        return InputEvent(input=self.memory.get())\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def generate_report(\n",
    "        self, ctx: Context, ev: ReportGenerationEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Generate report.\"\"\"\n",
    "\n",
    "        messages = self.report_gen_prompt.format_messages(\n",
    "            query_str=ctx.data[\"query\"], \n",
    "            context_str=\"\\n\\n\".join([n.get_content(metadata_mode=\"all\") for n in ctx.data[\"stored_chunks\"]])\n",
    "        )\n",
    "        response = self.report_gen_sllm.chat(messages).raw\n",
    "        return StopEvent(result={\"response\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = ReportGenerationAgent(\n",
    "    chunk_retriever_tool,\n",
    "    llm=llm,\n",
    "    report_gen_sllm=report_gen_sllm,\n",
    "    verbose=True,\n",
    "    timeout=120.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Tool call: tool_id='call_MWY7STXZYCAVUr4XhlutAAdd' tool_name='chunk_retriever_fn' tool_kwargs={'query': 'Tesla 2021 financial statements assets liabilities'}\n",
      "Step handle_llm_input produced event ChunkRetrievalEvent\n",
      "Running step handle_retrieval\n",
      "Step handle_retrieval produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Tool call: tool_id='call_Fx7TOUiP9kyFCU4kEmkPyl4F' tool_name='chunk_retriever_fn' tool_kwargs={'query': 'Apple 2021 financial statements assets liabilities'}\n",
      "Step handle_llm_input produced event ChunkRetrievalEvent\n",
      "Running step handle_retrieval\n",
      "Step handle_retrieval produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ReportGenerationEvent\n",
      "Running step generate_report\n",
      "Step generate_report produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "ret = await agent.run(\n",
    "    input=\"Tell me about the top-level assets and liabilities for Tesla in 2021, and compare it against those of Apple in 2021. Which company is doing better?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This report provides a comparative analysis of the top-level assets and liabilities for Tesla and Apple in the year 2021."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "First, we present the consolidated balance sheets for Tesla and Apple for the year 2021."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>December 31, 2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total current assets</td>\n",
       "      <td>$ 27,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total non-current assets</td>\n",
       "      <td>$ 35,031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total assets</td>\n",
       "      <td>$ 62,131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total current liabilities</td>\n",
       "      <td>$ 19,705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total non-current liabilities</td>\n",
       "      <td>$ 10,843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total liabilities</td>\n",
       "      <td>$ 30,548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Total equity</td>\n",
       "      <td>$ 31,583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 December 31, 2021\n",
       "0           Total current assets          $ 27,100\n",
       "1       Total non-current assets          $ 35,031\n",
       "2                   Total assets          $ 62,131\n",
       "3      Total current liabilities          $ 19,705\n",
       "4  Total non-current liabilities          $ 10,843\n",
       "5              Total liabilities          $ 30,548\n",
       "6                   Total equity          $ 31,583"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>September 25, 2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total current assets</td>\n",
       "      <td>$ 134,836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total non-current assets</td>\n",
       "      <td>$ 216,166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total assets</td>\n",
       "      <td>$ 351,002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total current liabilities</td>\n",
       "      <td>$ 125,481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total non-current liabilities</td>\n",
       "      <td>$ 162,431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total liabilities</td>\n",
       "      <td>$ 287,912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Total equity</td>\n",
       "      <td>$ 63,090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 September 25, 2021\n",
       "0           Total current assets          $ 134,836\n",
       "1       Total non-current assets          $ 216,166\n",
       "2                   Total assets          $ 351,002\n",
       "3      Total current liabilities          $ 125,481\n",
       "4  Total non-current liabilities          $ 162,431\n",
       "5              Total liabilities          $ 287,912\n",
       "6                   Total equity           $ 63,090"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "From the balance sheets, we can observe the following key points:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1. **Total Assets**: Apple has significantly higher total assets ($351,002 million) compared to Tesla ($62,131 million)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2. **Total Liabilities**: Apple's total liabilities ($287,912 million) are also much higher than Tesla's ($30,548 million)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3. **Total Equity**: Apple has a higher total equity ($63,090 million) compared to Tesla ($31,583 million)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "In conclusion, Apple has a stronger financial position in terms of total assets, liabilities, and equity compared to Tesla in the year 2021."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ret[\"response\"].render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ChunkRetrievalEvent\n",
      "Running step handle_retrieval\n",
      "Step handle_retrieval produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ChunkRetrievalEvent\n",
      "Running step handle_retrieval\n",
      "Step handle_retrieval produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ChunkRetrievalEvent\n",
      "Running step handle_retrieval\n",
      "Step handle_retrieval produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ReportGenerationEvent\n",
      "Running step generate_report\n",
      "Step generate_report produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "ret = await agent.run(\n",
    "    input=\"Tell me about the gross margin breakdown of Apple 2020-2022.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Gross Margin Breakdown of Apple (2020-2022)\n",
       "\n",
       "This report provides a detailed breakdown of Apple's gross margin for the years 2020, 2021, and 2022. The gross margin is divided into Products and Services categories, with both the gross margin in dollars and the gross margin percentage provided for each category."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2022</th>\n",
       "      <th>2021</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Products</td>\n",
       "      <td>$ 114,728</td>\n",
       "      <td>$ 105,126</td>\n",
       "      <td>$ 69,461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Services</td>\n",
       "      <td>$ 56,054</td>\n",
       "      <td>$ 47,710</td>\n",
       "      <td>$ 35,495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total gross margin</td>\n",
       "      <td>$ 170,782</td>\n",
       "      <td>$ 152,836</td>\n",
       "      <td>$ 104,956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            2022       2021       2020\n",
       "0            Products  $ 114,728  $ 105,126   $ 69,461\n",
       "1            Services   $ 56,054   $ 47,710   $ 35,495\n",
       "2  Total gross margin  $ 170,782  $ 152,836  $ 104,956"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2022</th>\n",
       "      <th>2021</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Products</td>\n",
       "      <td>36.3%</td>\n",
       "      <td>35.3%</td>\n",
       "      <td>31.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Services</td>\n",
       "      <td>71.7%</td>\n",
       "      <td>69.7%</td>\n",
       "      <td>66.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total gross margin percentage</td>\n",
       "      <td>43.3%</td>\n",
       "      <td>41.8%</td>\n",
       "      <td>38.2%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   2022   2021   2020\n",
       "0                       Products  36.3%  35.3%  31.5%\n",
       "1                       Services  71.7%  69.7%  66.0%\n",
       "2  Total gross margin percentage  43.3%  41.8%  38.2%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ret[\"response\"].render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamacloud-demo",
   "language": "python",
   "name": "llamacloud-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
