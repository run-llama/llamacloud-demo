{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1348d3-4c0e-450f-8faf-19503f61b7b2",
   "metadata": {},
   "source": [
    "# LlamaCloud Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57082f55-66e0-44e1-8072-2450405c21d1",
   "metadata": {},
   "source": [
    "## Step 0: Setup environment config for LlamaCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83a35ec-8e6c-475c-827c-20f46c4a3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013c59e6-34b2-4685-b8c3-10b9e9afc3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472b5e3-c203-410d-82a3-b19c7c0ed61b",
   "metadata": {},
   "source": [
    "## Step 1: Parse pdf with LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f713c84-2774-45c8-b030-a572273724db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83d26e5-05ea-4bac-a06c-987e06993f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6414cbff-8b05-4599-bce6-5ce764600a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 1d2c0056-8a84-4036-8b5e-fb7081905a38\n"
     ]
    }
   ],
   "source": [
    "file_extractor = {\".pdf\": parser}\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=['data_resnet/resnet.pdf'], \n",
    "    file_extractor=file_extractor\n",
    ")\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b8955-b6f7-43c2-8ba4-33a07fca0e2c",
   "metadata": {},
   "source": [
    "## Step 2: Build cloud index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1896de-c850-44d2-8e79-cb75a04d669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d879e25-3ffc-4b90-9858-f9f3f6c83851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find your index at https://cloud.llamaindex.ai/project/82137f10-5033-48f4-9a01-7ffcda87cd81/deploy/968be726-0f47-428a-b839-1279e11c5df1\n"
     ]
    }
   ],
   "source": [
    "index = LlamaCloudIndex.from_documents(\n",
    "    name='resnet_0226',\n",
    "    documents=docs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375bb94-3386-4a21-9f6e-a31c7ff55e5b",
   "metadata": {},
   "source": [
    "## Step 3: Use your retrieval endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5cb21-8f98-490d-9468-cb30e12c3d83",
   "metadata": {},
   "source": [
    "If you have a reference to the index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df510141-13be-4285-9fab-1906c56dc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(rerank_top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63457127-5a86-4b78-81e9-848a07c37362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 3.24 ms, total: 17.9 ms\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nodes = retriever.retrieve('How are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3de876-fad7-41e4-b965-fb93b3909fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 3460eb38-8a3a-41f8-bc9d-763a1c90d9a6\n",
      "Text: (1e4)| | |0|0|1|2|3|4|5|6|  Figure 6. Training on CIFAR-10.\n",
      "Dashed lines denote training error, and bold lines denote testing\n",
      "error. Left: plain networks. The error of plain-110 is higher than 60%\n",
      "and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202\n",
      "layers.  | |plain-20|plain-56|ResNet-20| |---|---|---|---| |std|\n",
      "|ResNet-56|ResNe...\n",
      "Score:  0.885\n",
      "\n",
      "Node ID: 79e524c8-2759-49f7-bbeb-a0f399e9dba2\n",
      "Text: 2). Identity shortcut connections add neither extra parameter\n",
      "nor computational complexity. The entire network can still be trained\n",
      "end-to-end by SGD with backpropagation, and can be easily implemented\n",
      "using common libraries (e.g., Caffe) without modifying the solvers.\n",
      "We present comprehensive experiments on ImageNet to show the\n",
      "degradation pro...\n",
      "Score:  0.868\n",
      "\n",
      "Node ID: 708ee92e-7047-4aba-93cd-1bb48883491e\n",
      "Text: Detection results on the PASCAL VOC 2007 test set. The baseline\n",
      "is the Faster R-CNN system. The system “baseline+++” include box\n",
      "refinement, context, and multi-scale testing in Table 9.  |system|net|\n",
      "data|mAP|areo|bike|bird|boat|bottle|bus|car|cat|chair|cow|table|dog|ho\n",
      "rse|mbike|person|plant|sheep|sofa|train|tv|\n",
      "|---|---|---|---|---|---|---|---|-...\n",
      "Score:  0.853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5001d-fcdf-4888-8940-0e7db5372df4",
   "metadata": {},
   "source": [
    "Alternatively, you can directly create a retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4c651b-a1d0-40b0-91ce-a7a96fd701fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2796361d-90cd-423c-9726-f638a7aefbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = LlamaCloudRetriever(\n",
    "    name='resnet',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828c11dd-8b40-4262-a4c3-f0345b81a522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.63 ms, sys: 1.99 ms, total: 9.62 ms\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nodes = retriever.retrieve('Who are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58688348-fa6f-4daf-8d95-20604c6f5a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: bc315636-30ec-4bd5-8833-3129d83622bb\n",
      "Text: ## Deep Residual Learning for Image Recognition  Kaiming He\n",
      "Xiangyu Zhang Shaoqing Ren Jian Sun Microsoft Research\n",
      "arXiv:1512.03385v1 [cs.CV] 10 Dec 2015 {kahe, v-xiangz, v-shren,\n",
      "jiansun}@microsoft.com 20 20  ### Abstract  Deeper neural networks are\n",
      "more difficult to train. We present a residual learning framework to\n",
      "ease the training of netwo...\n",
      "Score:  0.944\n",
      "\n",
      "Node ID: 8d41f633-d8d8-4247-ac81-bed3f6b6dc2f\n",
      "Text: Residual learning: a building block.  are comparably good or\n",
      "better than the constructed solution (or unable to do so in feasible\n",
      "time).  In this paper, we address the degradation problem by\n",
      "introducing a deep residual learning framework. Instead of hoping each\n",
      "few stacked layers directly fit a desired underlying mapping, we\n",
      "explicitly let these...\n",
      "Score:  0.935\n",
      "\n",
      "Node ID: fa1e64d4-1552-4b06-93a8-60e5cbee5b71\n",
      "Text: This result won the 1st place in the ImageNet detection task in\n",
      "ILSVRC 2015, surpassing the second place by 8.5 points (absolute).  ##\n",
      "ImageNet Localization  The ImageNet Localization (LOC) task [36]\n",
      "requires to classify and localize the objects. Following [40, 41], we\n",
      "assume that the image-level classifiers are first adopted for\n",
      "predicting the ...\n",
      "Score:  0.909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
